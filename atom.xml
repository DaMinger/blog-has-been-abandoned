<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[DaMinger's blog]]></title>
  <subtitle><![CDATA[信春哥，系统稳，闭眼上线不回滚]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="https://github.com/DaMinger/DaMinger.github.io.git/"/>
  <updated>2014-05-19T03:47:12.646Z</updated>
  <id>https://github.com/DaMinger/DaMinger.github.io.git/</id>
  
  <author>
    <name><![CDATA[DaMinger]]></name>
    <email><![CDATA[564400632@qq.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[12-15周学习计划]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/14/%E6%9D%82%E8%B0%88/plan/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/14/杂谈/plan/</id>
    <published>2014-05-14T11:55:11.000Z</published>
    <updated>2014-05-18T13:04:34.000Z</updated>
    <content type="html"><![CDATA[<p>离大三结束还剩50天吧，最近思来想后，这几周干什么！？最终还是决定把基础知识抓一抓，准备写几个专题，也是对自己的查缺补漏，使自己的基本功更扎实。</p>
<ul>
<li>设计模式  阅读《大话设计模式》 对某些设计模式进行java和python的改写</li>
<li>Shell基础与实战  参考《Shell脚本学习指南》</li>
<li>Python学习总结  写写案例，总结常用模块</li>
<li>基本的数据结构与算法     用C或者JAVA实现 </li>
<li>Oracle     理论与实验并行，原先在OneNote上的笔记 整理整理</li>
</ul>
<p>学习任务很多，给自己一个鼓励，努力坚持！</p>
]]></content>
    
    
      <category term="学习计划" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"/>
    
      <category term="杂谈" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Sqoop的安装及数据在Mysql与HDFS之间的导入导出]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/12/Hadoop/sqoop/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/12/Hadoop/sqoop/</id>
    <published>2014-05-12T07:16:57.000Z</published>
    <updated>2014-05-12T07:19:00.000Z</updated>
    <content type="html"><![CDATA[<h4 id="解压Sqoop并cp相应的hadoop以及mysql的jar包">解压Sqoop并cp相应的hadoop以及mysql的jar包</h4>
<pre><code>[grid@hadoop01 ~]$ tar -xzvf sqoop-<span class="number">1.4</span><span class="number">.4</span><span class="preprocessor">.bin</span>__hadoop-<span class="number">1.0</span><span class="number">.0</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span> 

[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ <span class="keyword">cp</span> hadoop-core-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span> ~/sqoop-<span class="number">1.4</span><span class="number">.4</span><span class="preprocessor">.bin</span>__hadoop-<span class="number">1.0</span><span class="number">.0</span>/lib/

[grid@hadoop01 lib]$ <span class="keyword">cp</span> ~/mysql-connector-java-<span class="number">5.1</span><span class="number">.25</span>-bin<span class="preprocessor">.jar</span>   ~/sqoop-<span class="number">1.4</span><span class="number">.4</span><span class="preprocessor">.bin</span>__hadoop-<span class="number">1.0</span><span class="number">.0</span>/lib/
[grid@hadoop01 ~]$ <span class="keyword">cp</span> mysql-connector-java-<span class="number">5.1</span><span class="number">.25</span>-bin<span class="preprocessor">.jar</span>  hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib
</code></pre><h4 id="编辑环境变量">编辑环境变量</h4>
<pre><code>export SQOOP_HOME=/home/<span class="keyword">grid</span>/sqoop-<span class="number">1.4</span><span class="number">.4</span>.bin__hadoop-<span class="number">1.0</span><span class="number">.0</span>
export PATH=<span class="variable">$PATH</span>:<span class="variable">$SQOOP_HOME</span>/bin:<span class="variable">$SQOOP_HOME</span>
[root<span class="variable">@hadoop01</span> ~]# <span class="keyword">source</span> /etc/profile

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> conf]# cp sqoop-<span class="keyword">env</span>-template.sh  sqoop-<span class="keyword">env</span>.sh

#Set path to where bin/hadoop is available
 export HADOOP_COMMON_HOME=/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>

#Set path to where hadoop-<span class="variable">*-</span>core.jar is available
 export HADOOP_MAPRED_HOME=/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>
</code></pre><h4 id="建立测试表">建立测试表</h4>
<pre><code><span class="operator"><span class="keyword">create</span> <span class="keyword">database</span> sqoop;</span>
<span class="operator"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'sqoop'</span>@<span class="string">'192.168.255.151'</span> identified <span class="keyword">by</span> <span class="string">'sqoop'</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span>
flush privileges;

use sqoop;
<span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> tb1 <span class="keyword">as</span> <span class="keyword">select</span> table_schema,table_name,table_type <span class="keyword">from</span> information_schema.TABLES;</span>
<span class="operator"><span class="keyword">show</span> tables;</span>
</code></pre><h4 id="用Sqoop把该表里的数据拷贝到HDFS上">用Sqoop把该表里的数据拷贝到HDFS上</h4>
<h5 id="测试连接">测试连接</h5>
<pre><code>sqoop <span class="type">list</span>-databases <span class="comment">--connect jdbc:mysql://192.168.255.151:3306/ --username sqoop --password sqoop</span>
sqoop <span class="type">list</span>-tables <span class="comment">--connect jdbc:mysql://192.168.255.151:3306/sqoop --username sqoop --password sqoop</span>

[grid@hadoop01 ~]$ sqoop <span class="type">list</span>-databases <span class="comment">--connect jdbc:mysql://192.168.255.151:3306/ --username sqoop password sqoop</span>
Warning: /usr/lib/hcatalog <span class="keyword">does</span> <span class="keyword">not</span> exist! HCatalog jobs will fail.
Please <span class="keyword">set</span> $HCAT_HOME <span class="keyword">to</span> <span class="keyword">the</span> root <span class="keyword">of</span> your HCatalog installation.
Warning: $HADOOP_HOME <span class="keyword">is</span> deprecated.

<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">43</span>:<span class="number">12</span> WARN tool.BaseSqoopTool: Setting your password <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> command-line <span class="keyword">is</span> insecure. Consider using -P instead.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">43</span>:<span class="number">12</span> INFO manager.MySQLManager: Preparing <span class="keyword">to</span> use a MySQL streaming resultset.
information_schema
hive
mysql
performance_schema
sqoop
test
</code></pre><h4 id="将数据拷贝到HDFS上">将数据拷贝到HDFS上</h4>
<pre><code>sqoop import --connect jdbc:mysql:<span class="comment">//192.168.255.151:3306/sqoop --username sqoop --password sqoop --table tb1 -m 1</span>
这个时候报错了
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">45</span>:<span class="number">52</span> INFO mapred.JobClient: Task Id : attempt_201405101008_0008_m_000000_0, Status : FAILED
java.lang.RuntimeException: java.lang.RuntimeException: java.sql.SQLException: Access denied <span class="keyword">for</span> user <span class="string">'sqoop'</span>@<span class="string">'hadoop03.myhadoop.com'</span> (<span class="keyword">using</span> password: YES)
        at org.apache.sqoop.mapreduce.db.DBInputFormat.setConf(DBInputFormat.java:<span class="number">167</span>)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:<span class="number">62</span>)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:<span class="number">117</span>)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:<span class="number">722</span>)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:<span class="number">364</span>)
        at org.apache.hadoop.mapred.Child$<span class="number">4</span>.run(Child.java:<span class="number">255</span>)
        at java.security.AccessController.doPrivileged(Native <span class="function"><span class="keyword">Method</span>)</span>
</code></pre><h5 id="解决办法，其他节点也要设置权限">解决办法，其他节点也要设置权限</h5>
<pre><code><span class="operator"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'sqoop'</span>@<span class="string">'192.168.255.152'</span> identified <span class="keyword">by</span> <span class="string">'sqoop'</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span>
<span class="operator"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'sqoop'</span>@<span class="string">'192.168.255.153'</span> identified <span class="keyword">by</span> <span class="string">'sqoop'</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span>
flush privileges;
<span class="operator"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> CONCAT(<span class="string">'User: '''</span>,<span class="keyword">user</span>,<span class="string">'''@'''</span>,host,<span class="string">''';'</span>) <span class="keyword">AS</span> query <span class="keyword">FROM</span> mysql.<span class="keyword">user</span>;</span>
</code></pre><h4 id="重新导入">重新导入</h4>
<pre><code>[grid@hadoop01 ~]$ sqoop import --connect jdbc:mysql://<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span>:<span class="number">3306</span>/sqoop --username sqoop --password sqoop --table tb1 -m <span class="number">1</span>
<span class="label">Warning:</span> /usr/lib/hcatalog does not exist! HCatalog jobs will fail.
Please <span class="keyword">set</span> $HCAT_HOME to the root of your HCatalog installation.
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">30</span> WARN tool<span class="preprocessor">.BaseSqoopTool</span>: Setting your password on the command-line is insecure. Consider using -P instead.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">30</span> INFO manager<span class="preprocessor">.MySQLManager</span>: Preparing to use a MySQL streaming resultset.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">30</span> INFO tool<span class="preprocessor">.CodeGenTool</span>: Beginning code generation
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">31</span> INFO manager<span class="preprocessor">.SqlManager</span>: Executing SQL statement: SELECT t.* FROM `tb1` AS t LIMIT <span class="number">1</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">31</span> INFO manager<span class="preprocessor">.SqlManager</span>: Executing SQL statement: SELECT t.* FROM `tb1` AS t LIMIT <span class="number">1</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">31</span> INFO orm<span class="preprocessor">.CompilationManager</span>: HADOOP_MAPRED_HOME is /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>
<span class="label">Note:</span> /tmp/sqoop-grid/compile/<span class="number">4</span>a0739c3b0538ec549c03cafb8dace32/tb1<span class="preprocessor">.java</span> uses <span class="keyword">or</span> overrides a deprecated API.
<span class="label">Note:</span> Recompile with -Xlint:deprecation for details.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">38</span> INFO orm<span class="preprocessor">.CompilationManager</span>: Writing jar file: /tmp/sqoop-grid/compile/<span class="number">4</span>a0739c3b0538ec549c03cafb8dace32/tb1<span class="preprocessor">.jar</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">38</span> WARN manager<span class="preprocessor">.MySQLManager</span>: It looks like you are importing from mysql.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">38</span> WARN manager<span class="preprocessor">.MySQLManager</span>: This transfer can be faster! Use the --direct
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">38</span> WARN manager<span class="preprocessor">.MySQLManager</span>: option to exercise a MySQL-specific fast path.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">38</span> INFO manager<span class="preprocessor">.MySQLManager</span>: Setting zero DATETIME behavior to convertToNull (mysql)
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">38</span> INFO mapreduce<span class="preprocessor">.ImportJobBase</span>: Beginning import of tb1
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">43</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201405101008_0009
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">44</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">59</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201405101008_0009
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">18</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">16169</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">4249</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">87</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">70026</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_WRITTEN=<span class="number">4249</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">112</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">73797632</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">1540</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">15728640</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">1564905472</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">112</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">87</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapreduce<span class="preprocessor">.ImportJobBase</span>: Transferred <span class="number">4.1494</span> KB <span class="keyword">in</span> <span class="number">24.7602</span> seconds (<span class="number">171.6062</span> bytes/<span class="keyword">sec</span>)
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span>:<span class="number">03</span> INFO mapreduce<span class="preprocessor">.ImportJobBase</span>: Retrieved <span class="number">112</span> records.
</code></pre><h4 id="查看导入的文件">查看导入的文件</h4>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ hadoop fs -<span class="keyword">ls</span> tb1
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">51</span> /user/<span class="keyword">grid</span>/tb1/_SUCCESS
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span> /user/<span class="keyword">grid</span>/tb1/_logs
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup       <span class="number">4249</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">50</span> /user/<span class="keyword">grid</span>/tb1/part-m-<span class="number">00000</span>

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ hadoop fs -cat tb1/part-m-<span class="number">00000</span>
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

information_schema,CHARACTER_SETS,SYSTEM VIEW
information_schema,COLLATIONS,SYSTEM VIEW
information_schema,COLLATION_CHARACTER_SET_APPLICABILITY,SYSTEM VIEW
information_schema,COLUMNS,SYSTEM VIEW
information_schema,COLUMN_PRIVILEGES,SYSTEM VIEW
information_schema,ENGINES,SYSTEM VIEW
information_schema,EVENTS,SYSTEM VIEW
information_schema,FILES,SYSTEM VIEW
information_schema,GLOBAL_STATUS,SYSTEM VIEW
information_schema,GLOBAL_VARIABLES,SYSTEM VIEW
information_schema,KEY_COLUMN_USAGE,SYSTEM VIEW
information_schema,PARAMETERS,SYSTEM VIEW
information_schema,PARTITIONS,SYSTEM VIEW
information_schema,PLUGINS,SYSTEM VIEW
information_schema,PROCESSLIST,SYSTEM VIEW
information_schema,PROFILING,SYSTEM VIEW
information_schema,REFERENTIAL_CONSTRAINTS,SYSTEM VIEW
information_schema,ROUTINES,SYSTEM VIEW
information_schema,SCHEMATA,SYSTEM VIEW
information_schema,SCHEMA_PRIVILEGES,SYSTEM VIEW
information_schema,SESSION_STATUS,SYSTEM VIEW
information_schema,SESSION_VARIABLES,SYSTEM VIEW
information_schema,STATISTICS,SYSTEM VIEW
information_schema,TABLES,SYSTEM VIEW
information_schema,TABLESPACES,SYSTEM VIEW
information_schema,TABLE_CONSTRAINTS,SYSTEM VIEW
information_schema,TABLE_PRIVILEGES,SYSTEM VIEW
information_schema,TRIGGERS,SYSTEM VIEW

OK
</code></pre><h4 id="把HDFS的数据导回去mysql">把HDFS的数据导回去mysql</h4>
<pre><code>mysql&gt; use sqoop
Database changed
mysql&gt; show tables<span class="comment">;</span>
+-----------------+
| Tables_in_sqoop |
+-----------------+
| tb1             |
+-----------------+
<span class="number">1</span> row <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> <span class="keyword">sec</span>)

mysql&gt; truncate tb1<span class="comment">;</span>
Query OK, <span class="number">0</span> rows affected (<span class="number">0.01</span> <span class="keyword">sec</span>)

sqoop export  --connect jdbc:mysql://<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span>:<span class="number">3306</span>/sqoop --username sqoop --password sqoop --table tb1 --export-dir hdfs://<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span>:<span class="number">9000</span>/user/grid/tb1/

[grid@hadoop01 ~]$ sqoop export  --connect jdbc:mysql://<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span>:<span class="number">3306</span>/sqoop --username sqoop --password sqoop --table tb1 --export-dir hdfs://<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span>:<span class="number">9000</span>/user/grid/tb1/
<span class="label">Warning:</span> /usr/lib/hcatalog does not exist! HCatalog jobs will fail.
Please <span class="keyword">set</span> $HCAT_HOME to the root of your HCatalog installation.
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">19</span> WARN tool<span class="preprocessor">.BaseSqoopTool</span>: Setting your password on the command-line is insecure. Consider using -P instead.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">20</span> INFO manager<span class="preprocessor">.MySQLManager</span>: Preparing to use a MySQL streaming resultset.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">20</span> INFO tool<span class="preprocessor">.CodeGenTool</span>: Beginning code generation
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">21</span> INFO manager<span class="preprocessor">.SqlManager</span>: Executing SQL statement: SELECT t.* FROM `tb1` AS t LIMIT <span class="number">1</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">21</span> INFO manager<span class="preprocessor">.SqlManager</span>: Executing SQL statement: SELECT t.* FROM `tb1` AS t LIMIT <span class="number">1</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">21</span> INFO orm<span class="preprocessor">.CompilationManager</span>: HADOOP_MAPRED_HOME is /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>
<span class="label">Note:</span> /tmp/sqoop-grid/compile/<span class="number">605</span>c350fea684271f2c51e2f616406c7/tb1<span class="preprocessor">.java</span> uses <span class="keyword">or</span> overrides a deprecated API.
<span class="label">Note:</span> Recompile with -Xlint:deprecation for details.
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">27</span> INFO orm<span class="preprocessor">.CompilationManager</span>: Writing jar file: /tmp/sqoop-grid/compile/<span class="number">605</span>c350fea684271f2c51e2f616406c7/tb1<span class="preprocessor">.jar</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">27</span> INFO mapreduce<span class="preprocessor">.ExportJobBase</span>: Beginning export of tb1
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">32</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">1</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">32</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">1</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">32</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">32</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">33</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201405101008_0010
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">34</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">06</span>:<span class="number">58</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">50</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">00</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201405101008_0010
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">19</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">71976</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Rack-local map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">4</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Data-local map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">11629</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">279312</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">112</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">301490176</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">0</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">5450</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">62914560</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">6251151360</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">112</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">611</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapreduce<span class="preprocessor">.ExportJobBase</span>: Transferred <span class="number">11.3564</span> KB <span class="keyword">in</span> <span class="number">34.5615</span> seconds (<span class="number">336.4727</span> bytes/<span class="keyword">sec</span>)
<span class="number">14</span>/<span class="number">05</span>/<span class="number">10</span> <span class="number">12</span>:<span class="number">07</span>:<span class="number">04</span> INFO mapreduce<span class="preprocessor">.ExportJobBase</span>: Exported <span class="number">112</span> records.
</code></pre><h4 id="验证">验证</h4>
<pre><code>mysql&gt; use sqoop;
Database changed
<span class="header">mysql&gt; show tables;
+-----------------+</span>
<span class="header">| Tables_in_sqoop |
+-----------------+</span>
<span class="header">| tb1             |
+-----------------+</span>
1 row in set (0.00 sec)
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Sqoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Sqoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBase shell的使用]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/12/Hadoop/hbase_shell/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/12/Hadoop/hbase_shell/</id>
    <published>2014-05-12T07:16:49.000Z</published>
    <updated>2014-05-12T07:18:28.000Z</updated>
    <content type="html"><![CDATA[<h4 id="HBase_Shell的一些基本操作命令">HBase Shell的一些基本操作命令</h4>
<p><table><tr><td>名称</td><td>命令表达式</td></tr><tr><td>创建表</td><td>create ‘表名称’, ‘列名称1’,’列名称2’,’列名称N’</td></tr><tr><td>添加记录</td><td>put ‘表名称’, ‘行名称’, ‘列名称:’, ‘值</td></tr><tr><td>查看记录</td><td>get ‘表名称’, ‘行名称’</td></tr><tr><td>查看表中的记录总数</td><td>count  ‘表名称’</td></tr></p>
<p><tr><td>删除记录</td><td>delete  ‘表名’ ,’行名称’ , ‘列名称’</td></tr><tr><td>删除一张表</td><td>先要屏蔽该表，才能对该表进行删除，第一步 disable ‘表名称’ 第二步  drop ‘表名称’</td></tr><tr><td>查看所有记录</td><td>scan “表名称”</td></tr><tr><td>查看某个表某个列中所有数据</td><td>scan “表名称” , [‘列名称:’]</td></tr><tr><td>更新记录</td><td>就是重写一遍进行覆盖</td></tr></table></p>
<h4 id="HBase_例子说明">HBase 例子说明</h4>
<p>我们用一个学生成绩表作为例子,对HBase的基本操作和基本概念进行讲解:</p>
<pre><code>下面是学生的成绩表:
<span class="tag">name</span>     <span class="tag">grad</span>    <span class="tag">course</span><span class="pseudo">:math</span>    <span class="tag">course</span><span class="pseudo">:art</span>
<span class="tag">Tom</span>      1       87             97

<span class="tag">Jerry</span>    2       100            80
</code></pre><p>这里grad对于表来说是一个列,course对于表来说是一个列族,这个列族由两个列组成:math和art,当然我们可以根据我们的需要在course中建立更多的列族,如computer,physics等相应的列添加入course列族.</p>
<p>有了上面的想法和需求,我们就可以在HBase中建立相应的数据表啦!</p>
<h5 id="建立一个表格_scores_具有两个列族grade和courese">建立一个表格 scores 具有两个列族grade和courese</h5>
<pre><code>hbase(main):<span class="number">002</span>:<span class="number">0</span>&gt; <span class="built_in">create</span> <span class="string">'scores'</span>, <span class="string">'grade'</span>, <span class="string">'course'</span>

<span class="number">0</span> row(s) <span class="operator">in</span> <span class="number">4.1610</span> <span class="built_in">seconds</span>
</code></pre><h5 id="查看当先HBase中具有哪些表">查看当先HBase中具有哪些表</h5>
<pre><code><span class="tag">hbase</span>(<span class="tag">main</span>)<span class="pseudo">:003</span><span class="pseudo">:0</span>&gt; <span class="tag">list</span>

<span class="tag">scores</span>

1 <span class="tag">row</span>(<span class="tag">s</span>) <span class="tag">in</span> 0<span class="class">.0210</span> <span class="tag">seconds</span>
</code></pre><h5 id="查看表的构造">查看表的构造</h5>
<pre><code>hbase(main):<span class="number">004</span>:<span class="number">0</span>&gt; describe <span class="string">'scores'</span>

{NAME<span class="function"> =&gt;</span> <span class="string">'scores'</span>, IS_ROOT<span class="function"> =&gt;</span> <span class="string">'false'</span>, IS_META<span class="function"> =&gt;</span> <span class="string">'false'</span>, 
FAMILIES<span class="function"> =&gt;</span> [{NAME<span class="function"> =&gt;</span> <span class="string">'course'</span>, BLOOMFILTER<span class="function"> =&gt;</span> <span class="string">'false'</span>, IN_MEMORY<span class="function"> =&gt;</span> <span class="string">'false'</span>,LENGTH<span class="function"> =&gt;</span> <span class="string">'2147483647'</span>, BLOCKCACHE<span class="function"> =&gt;</span> <span class="string">'false'</span>, VERSIONS<span class="function"> =&gt;</span> <span class="string">'3'</span>, TTL<span class="function"> =&gt;</span> <span class="string">'-1'</span>, COMPRESSION<span class="function"> =&gt;</span> <span class="string">'NONE'</span>}, 
{NAME<span class="function"> =&gt;</span> <span class="string">'grade'</span>, BLOOMFILTER<span class="function"> =&gt;</span> <span class="string">'false'</span>, IN_MEMORY<span class="function"> =&gt;</span> <span class="string">'false'</span>, LENGTH<span class="function"> =&gt;</span> <span class="string">'2147483647'</span>,BLOCKCACHE<span class="function"> =&gt;</span> <span class="string">'false'</span>, VERSIONS<span class="function"> =&gt;</span> <span class="string">'3'</span>, TTL<span class="function"> =&gt;</span> <span class="string">'-1'</span>, COMPRESSION<span class="function"> =&gt;</span> <span class="string">'NONE'</span>}]}

<span class="number">1</span> row(s) <span class="keyword">in</span> <span class="number">0.0130</span> seconds
</code></pre><h5 id="加入一行数据,行名称为_Tom_列族grade的列名为””_值位1">加入一行数据,行名称为 Tom 列族grade的列名为”” 值位1</h5>
<pre><code>hbase(main):<span class="number">005</span>:<span class="number">0</span>&gt; <span class="built_in">put</span> <span class="string">'scores'</span>, <span class="string">'Tom'</span>, <span class="string">'grade:'</span>, <span class="string">'1'</span>

<span class="number">0</span> row(s) <span class="operator">in</span> <span class="number">0.0070</span> <span class="built_in">seconds</span>
</code></pre><h5 id="给Tom这一行的数据的列族添加一列_<math,87>">给Tom这一行的数据的列族添加一列 <math,87></h5>
<pre><code>hbase(main):<span class="number">006</span>:<span class="number">0</span>&gt; <span class="built_in">put</span> <span class="string">'scores'</span>, <span class="string">'Tom'</span>, <span class="string">'course:math'</span>, <span class="string">'87'</span>

<span class="number">0</span> row(s) <span class="operator">in</span> <span class="number">0.0040</span> <span class="built_in">seconds</span>
</code></pre><h5 id="给Tom这一行的数据的列族添加一列_<art,97>">给Tom这一行的数据的列族添加一列 <art,97></h5>
<pre><code>hbase(main):<span class="number">007</span>:<span class="number">0</span>&gt; <span class="built_in">put</span> <span class="string">'scores'</span>, <span class="string">'Tom'</span>, <span class="string">'course:art'</span>, <span class="string">'97'</span>

<span class="number">0</span> row(s) <span class="operator">in</span> <span class="number">0.0030</span> <span class="built_in">seconds</span>
</code></pre><h5 id="加入一行数据,行名称为_Jerry_列族grad的列名为””_值位2">加入一行数据,行名称为 Jerry 列族grad的列名为”” 值位2</h5>
<pre><code>hbase(main):<span class="number">008</span>:<span class="number">0</span>&gt; <span class="built_in">put</span> <span class="string">'scores'</span>, <span class="string">'Jerry'</span>, <span class="string">'grade:'</span>, <span class="string">'2'</span>

<span class="number">0</span> row(s) <span class="operator">in</span> <span class="number">0.0040</span> <span class="built_in">seconds</span>
</code></pre><h5 id="给Jerry这一行的数据的列族添加一列_<math,100>">给Jerry这一行的数据的列族添加一列 <math,100></h5>
<pre><code>hbase(main):<span class="number">009</span>:<span class="number">0</span>&gt; <span class="built_in">put</span> <span class="string">'scores'</span>, <span class="string">'Jerry'</span>, <span class="string">'course:math'</span>, <span class="string">'100'</span>

<span class="number">0</span> row(s) <span class="operator">in</span> <span class="number">0.0030</span> <span class="built_in">seconds</span>
</code></pre><h5 id="给Jerry这一行的数据的列族添加一列_<art,80>">给Jerry这一行的数据的列族添加一列 <art,80></h5>
<pre><code>hbase(main):<span class="number">010</span>:<span class="number">0</span>&gt; <span class="built_in">put</span> <span class="string">'scores'</span>, <span class="string">'Jerry'</span>, <span class="string">'course:art'</span>, <span class="string">'80'</span>

<span class="number">0</span> row(s) <span class="operator">in</span> <span class="number">0.0050</span> <span class="built_in">seconds</span>
</code></pre><h5 id="查看scores表中Tom的相关数据">查看scores表中Tom的相关数据</h5>
<pre><code>hbase(main):<span class="number">011</span>:<span class="number">0</span>&gt; <span class="built_in">get</span> <span class="string">'scores'</span>, <span class="string">'Tom'</span>

COLUMN                       CELL

course:art                  timestamp=<span class="number">1224726394286</span>, <span class="built_in">value</span>=<span class="number">97</span>

course:math                 timestamp=<span class="number">1224726377027</span>, <span class="built_in">value</span>=<span class="number">87</span>

grade:                      timestamp=<span class="number">1224726360727</span>, <span class="built_in">value</span>=<span class="number">1</span>

<span class="number">3</span> row(s) <span class="operator">in</span> <span class="number">0.0070</span> <span class="built_in">seconds</span>
</code></pre><h5 id="查看scores表中所有数据">查看scores表中所有数据</h5>
<pre><code>hbase(main):<span class="number">012</span>:<span class="number">0</span>&gt; scan <span class="string">'scores'</span>

ROW                          COLUMN+CELL

Tom                         column=course:art, timestamp=<span class="number">1224726394286</span>, <span class="built_in">value</span>=<span class="number">97</span>

Tom                         column=course:math, timestamp=<span class="number">1224726377027</span>, <span class="built_in">value</span>=<span class="number">87</span>

Tom                         column=grade:, timestamp=<span class="number">1224726360727</span>, <span class="built_in">value</span>=<span class="number">1</span>

Jerry                        column=course:art, timestamp=<span class="number">1224726424967</span>, <span class="built_in">value</span>=<span class="number">80</span>

Jerry                        column=course:math, timestamp=<span class="number">1224726416145</span>, <span class="built_in">value</span>=<span class="number">100</span>

Jerry                        column=grade:, timestamp=<span class="number">1224726404965</span>, <span class="built_in">value</span>=<span class="number">2</span>

<span class="number">6</span> row(s) <span class="operator">in</span> <span class="number">0.0410</span> <span class="built_in">seconds</span>
</code></pre><h5 id="查看scores表中所有数据courses列族的所有数据">查看scores表中所有数据courses列族的所有数据</h5>
<pre><code>hbase(main):<span class="number">013</span>:<span class="number">0</span>&gt; scan <span class="string">'scores'</span>, [<span class="string">'course:'</span>]

ROW                          COLUMN+CELL

Tom                         column=course:art, timestamp=<span class="number">1224726394286</span>, <span class="built_in">value</span>=<span class="number">97</span>

Tom                         column=course:math, timestamp=<span class="number">1224726377027</span>, <span class="built_in">value</span>=<span class="number">87</span>

Jerry                        column=course:art, timestamp=<span class="number">1224726424967</span>, <span class="built_in">value</span>=<span class="number">80</span>

Jerry                        column=course:math, timestamp=<span class="number">1224726416145</span>, <span class="built_in">value</span>=<span class="number">100</span>

<span class="number">4</span> row(s) <span class="operator">in</span> <span class="number">0.0200</span> <span class="built_in">seconds</span>
</code></pre><p>上面就是HBase的基本shell操作的一个例子,可以看出,hbase的shell还是比较简单易用的,从中也可以看出HBase shell缺少很多传统sql中的一些类似于like等相关操作,当然,HBase作为BigTable的一个开源实现,而BigTable是作为 google业务的支持模型,很多sql语句中的一些东西可能还真的不需要.</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/HBase/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBse的安装]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/12/Hadoop/hbase_install/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/12/Hadoop/hbase_install/</id>
    <published>2014-05-12T07:16:29.000Z</published>
    <updated>2014-05-12T07:17:46.000Z</updated>
    <content type="html"><![CDATA[<p>在安装hbase时要求与hadoop的版本相关联，那么如何找出合适的hbase版本呢，其实有一个简单但是有点费事的做法，就是可以下载hbase后解压查看lib目录中的hadoop-core-x.x.x.jar文件的版本号，就可以找到合适的hadoop版本号了。</p>
<p>我下载了三个版本，终于找到了hadoop1.2.1是 跟Hbase-0.98.0版本匹配。同学们可以少走弯路了。</p>
<p>我的机器配置</p>
<pre><code><span class="tag">Hadoop</span><span class="pseudo">:1</span><span class="class">.2</span><span class="class">.1</span>
<span class="tag">HBase</span><span class="pseudo">:0</span><span class="class">.98</span><span class="class">.1</span>
三个节点：<span class="tag">hadoop01</span>，<span class="tag">hadoop02</span>,<span class="tag">hadoop03</span>
</code></pre><h4 id="解压HBase">解压HBase</h4>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>tar -xzvf hbase-<span class="number">0</span>.<span class="number">98.1</span>-hadoop1-bin.tar.gz 
[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>mv hbase-<span class="number">0</span>.<span class="number">98.1</span>-hadoop1 hbase
</code></pre><h4 id="修改hbase_conf_目录下的配置文件">修改hbase conf 目录下的配置文件</h4>
<h5 id="hbase-env-sh：">hbase-env.sh：</h5>
<pre><code>[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>vi hbase-env.sh 

<span class="comment"># The java implementation to use.  Java 1.6 required.</span>
  export <span class="constant">JAVA_HOME</span>=<span class="regexp">/usr/jdk</span>1.<span class="number">7.0_51</span>

<span class="comment"># Extra Java CLASSPATH elements.  Optional.</span>
  export <span class="constant">HBASE_CLASSPATH</span>=<span class="regexp">/home/grid</span><span class="regexp">/hadoop-1.2.1/conf</span>

<span class="comment"># Tell HBase whether it should manage it's own instance of Zookeeper or not.</span>
  export <span class="constant">HBASE_MANAGES_ZK</span>=<span class="keyword">true</span>
</code></pre><h5 id="hbase-site-xml：">hbase-site.xml：</h5>
<pre><code><span class="tag">&lt;<span class="title">configuration</span>&gt;</span>
        <span class="tag">&lt;<span class="title">property</span>&gt;</span>
                <span class="tag">&lt;<span class="title">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
                <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://hadoop01.myhadoop.com:9000/hbase<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
        <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">property</span>&gt;</span>
                <span class="tag">&lt;<span class="title">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
                <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
        <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">property</span>&gt;</span>
                <span class="tag">&lt;<span class="title">name</span>&gt;</span>hbase.tmp.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
                <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/grid/hbase/tmp<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
        <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">property</span>&gt;</span>
                <span class="tag">&lt;<span class="title">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
                <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop01,hadoop02,hadoop03<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
        <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">property</span>&gt;</span>
                <span class="tag">&lt;<span class="title">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
                <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/grid/hbase/tmp/zookeeper<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
        <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;/<span class="title">configuration</span>&gt;</span>
</code></pre><h4 id="更改regionservers">更改regionservers</h4>
<pre><code>[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>vi regionservers
hadoop02
hadoop03 
</code></pre><h4 id="创建hbase专用文件夹：">创建hbase专用文件夹：</h4>
<h5 id="创建文件系统目录">创建文件系统目录</h5>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> conf]$ hadoop fs -mkdir hdfs:<span class="comment">//hadoop01.myhadoop.com:9000/hbase</span>
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> conf]$ hadoop fs -<span class="keyword">ls</span> hdfs:<span class="comment">//hadoop01.myhadoop.com:9000/</span>
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

Found <span class="number">5</span> items
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">09</span> <span class="number">15</span>:<span class="number">04</span> /hbase
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">21</span>:<span class="number">09</span> /home
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">22</span> <span class="number">19</span>:<span class="number">07</span> /out4
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">17</span>:<span class="number">17</span> /tmp
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">21</span>:<span class="number">22</span> /user
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> conf]$
</code></pre><h4 id="复制Master上的hbase文件夹到另外2节点：">复制Master上的hbase文件夹到另外2节点：</h4>
<pre><code> scp -r hbase grid<span class="variable">@hadoop02</span><span class="symbol">:~</span>
 scp -r hbase grid<span class="variable">@hadoop03</span><span class="symbol">:~</span>
</code></pre><h4 id="修改各节点_的/etc/profile：">修改各节点 的/etc/profile：</h4>
<pre><code><span class="keyword">export</span> HBASE_HOME=/home/grid/hbase
<span class="keyword">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HBASE_HOME</span>/bin
使其生效：
<span class="built_in">source</span> /etc/profile
</code></pre><h4 id="启动hbase并进入shell，然后退出">启动hbase并进入shell，然后退出</h4>
<pre><code>start-hbase.sh
hbase shell
create ‘test’,’id’
disable ‘test’
drop ‘test’
<span class="keyword">exit</span>
stop-hbase.sh



[grid@hadoop01 ~]$ start-hbase.sh 
hadoop03: starting zookeeper, logging <span class="keyword">to</span> /home/grid/hbase/bin/../logs/hbase-grid-zookeeper-hadoop03.myhadoop.com.<span class="keyword">out</span>
hadoop02: starting zookeeper, logging <span class="keyword">to</span> /home/grid/hbase/bin/../logs/hbase-grid-zookeeper-hadoop02.myhadoop.com.<span class="keyword">out</span>
The authenticity <span class="keyword">of</span> host <span class="attribute">'hadoop01</span> (<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span>)' can<span class="attribute">'t</span> be established.
RSA key fingerprint <span class="keyword">is</span> <span class="number">99</span>:<span class="number">8</span>c:<span class="number">8</span>f:<span class="number">64</span>:<span class="number">6</span>a:<span class="number">9</span>e:<span class="number">17</span>:<span class="number">4</span>b:cf:cc:b1:<span class="number">4</span>b:<span class="number">8</span>d:<span class="number">13</span>:eb:e9.
Are you sure you want <span class="keyword">to</span> continue connecting (yes/no)? yes
hadoop01: Warning: Permanently added <span class="attribute">'hadoop01</span>' (RSA) <span class="keyword">to</span> the list <span class="keyword">of</span> known hosts.
hadoop01: starting zookeeper, logging <span class="keyword">to</span> /home/grid/hbase/bin/../logs/hbase-grid-zookeeper-hadoop01.myhadoop.com.<span class="keyword">out</span>
starting master, logging <span class="keyword">to</span> /home/grid/hbase/logs/hbase-grid-master-hadoop01.myhadoop.com.<span class="keyword">out</span>
hadoop03: starting regionserver, logging <span class="keyword">to</span> /home/grid/hbase/bin/../logs/hbase-grid-regionserver-hadoop03.myhadoop.com.<span class="keyword">out</span>
hadoop02: starting regionserver, logging <span class="keyword">to</span> /home/grid/hbase/bin/../logs/hbase-grid-regionserver-hadoop02.myhadoop.com.<span class="keyword">out</span>
[grid@hadoop01 ~]$ jps
<span class="number">4675</span> HQuorumPeer
<span class="number">3796</span> SecondaryNameNode
<span class="number">4726</span> HMaster
<span class="number">3862</span> JobTracker
<span class="number">3653</span> NameNode
<span class="number">4873</span> Jps
[grid@hadoop01 ~]$ 
[grid@hadoop01 ~]$ hbase shell
HBase Shell; enter <span class="attribute">'help</span>&lt;<span class="keyword">RETURN</span>&gt;' <span class="keyword">for</span> list <span class="keyword">of</span> supported commands.
<span class="keyword">Type</span> <span class="string">"exit&lt;RETURN&gt;"</span> <span class="keyword">to</span> leave the HBase Shell
Version <span class="number">0.98</span><span class="number">.1</span>-hadoop1, r1583035, Sat Mar <span class="number">29</span> <span class="number">16</span>:<span class="number">51</span>:<span class="number">51</span> PDT <span class="number">2014</span>

hbase(main):<span class="number">001</span>:<span class="number">0</span>&gt; create <span class="attribute">'test</span>',<span class="attribute">'id</span>'
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding <span class="keyword">in</span> [jar:<span class="keyword">file</span>:/home/grid/hbase/lib/slf4j-log4j12-<span class="number">1.6</span><span class="number">.4</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding <span class="keyword">in</span> [jar:<span class="keyword">file</span>:/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/slf4j-log4j12-<span class="number">1.4</span><span class="number">.3</span>.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings <span class="keyword">for</span> an explanation.
<span class="number">0</span> row(s) <span class="keyword">in</span> <span class="number">7.1400</span> seconds

=&gt; Hbase::Table - test

hbase(main):<span class="number">003</span>:<span class="number">0</span>&gt; disable <span class="attribute">'test</span>'
<span class="number">0</span> row(s) <span class="keyword">in</span> <span class="number">1.5220</span> seconds

hbase(main):<span class="number">004</span>:<span class="number">0</span>&gt; drop <span class="attribute">'test</span>'
<span class="number">0</span> row(s) <span class="keyword">in</span> <span class="number">0.3550</span> seconds

hbase(main):<span class="number">005</span>:<span class="number">0</span>&gt; <span class="keyword">exit</span>
[grid@hadoop01 bin]$ stop-hbase.sh 
stopping hbase....................
hadoop02: stopping zookeeper.
hadoop03: stopping zookeeper.
hadoop01: stopping zookeeper.
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/HBase/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[炼数成金论坛上关于HBase三个小问题的讨论]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/11/Hadoop/hbase_problem/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/11/Hadoop/hbase_problem/</id>
    <published>2014-05-11T02:23:49.000Z</published>
    <updated>2014-05-11T02:25:28.000Z</updated>
    <content type="html"><![CDATA[<p><div align=left><a href="http://blog.csdn.net/michael_zhu_2004/article/details/8330777" target="_blank">参考1</a>和<a href="http://www.dataguru.cn/blog-303-529.html" target="_blank">参考2</a></div></p>
<h5 id="我们常说HBase是“数据即日志”的数据库，它是怎样修改和删除数据的？和Oracle这类传统的RDBMS有什么区别？">我们常说HBase是“数据即日志”的数据库，它是怎样修改和删除数据的？和Oracle这类传统的RDBMS有什么区别？</h5>
<p>数据即日志，主要是指HBase中数据如日志般存储和更新，即几乎没有插入，修改，删除的操作，只是不停增加数据。这一特性主要是由于HBase数据文件存储在HDFS系统中，而该系统由于统一数据分布在不同的节点中，对其中数据的删除和修改都是非常困难的。故HBase在修改数据时就对该数据增加一个更新的数据版本（以时间戳），HBase的修改操作也同样增加一条命令删除数据的记录，等到对storefile进行合并时，去掉这些数据。</p>
<p>Oracle没有数据的版本概念，在修改和删除数据时不会增加新的数据记录，直接对老数据进行修改或删除。一旦完成修改或删除，则原始数据发生变化。</p>
<h5 id="HBase合并storefile的原因是什么？在合并的过程中会做什么操作？如果在合并过程中恰好有涉及到有关storefile的查询发生，会发生什么情况（这个问题需要自行研究）？">HBase合并storefile的原因是什么？在合并的过程中会做什么操作？如果在合并过程中恰好有涉及到有关storefile的查询发生，会发生什么情况（这个问题需要自行研究）？</h5>
<p>HBbase的新数据首先是放在MemoryStore中的，数据量超过阈值后才会被写入物理文件StoreFile。因此，这些物理文件StoreFile初次的容量都不大（内存的映射）。HBase会定时合并这些较小的StoreFile，形成较大的StoreFile更有利于HDFS物理文件的读操作。而且，合并时会删除其中过期的旧版本数据和被删除的数据。</p>
<p>如果在合并过程中恰好有涉及到有关storefile的查询发生的话，HBase先是把小storefile加载到内存中，用户可以在内存中检索相关数据，其实内存中做存在一个独立镜像备份专门提供查询，当合并完成后内存空间中的镜像备份才会被撤销。</p>
<h5 id="HBase具有怎样的一致性水平？">HBase具有怎样的一致性水平？</h5>
<p>CAP理论是由EricBrewer教授提出的，在设计和部署分布式应用的时候，存在三个核心的系统需求，这个三个需求之间存在一定的特殊关系。三个需求如下：</p>
<pre><code><span class="keyword">C</span>: Consistency 一致性
A: Availability 可用性
P:<span class="keyword">Partition</span> <span class="keyword">Tolerance</span>分区容错性
</code></pre><p>CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。 </p>
<p>C: Consistency 一致性</p>
<p>一致性又称为原子性或者事务性。表示一个事务的操作是不可分割的，要不然这个事务完成，要不然这个事务不完成，不会出现这个事务完成了一半这样的情况。这种事务的原子性使得数据具有一致性。我们通常情况下在数据库中存在的脏数据就属于数据没有具有一致性的表现。而在分布式系统中，经常出现的一个数据不具有一致性的情况是读写数据时缺乏一致性。比如两个节点数据冗余，第一个节点有一个写操作，数据更新以后没有有效的使得第二个节点更新数据，在读取第二个节点的时候就会出现不一致的问题出现。传统的ACID数据库是很少存在一致性问题的，因为数据的单点原因，数据的存取又具有良好的事务性，不会出现读写的不一致。</p>
<p>HBase架构在HDFS系统之上，是分布式的NoSQL列式数据库，它是一个CP型数据库。每一行数据仅在一个Region中，没有多余副本可读，保证了数据的一致性</p>
<h4 id="这个答案也不错，故一并记录下：">这个答案也不错，故一并记录下：</h4>
<h5 id="我们常说HBase是“数据即日志”的数据库，它是怎样修改和删除数据的？和Oracle这类传统的RDBMS有什么区别？-1">我们常说HBase是“数据即日志”的数据库，它是怎样修改和删除数据的？和Oracle这类传统的RDBMS有什么区别？</h5>
<p>答：首先Hbase中的一个“元素”是由行键、列族名、限定符、时间戳唯一标识的并且行键作为数据行在表里的唯一标识，我们只有通过行键来访问列族别无他法。</p>
<p>修改数据：我们先找到要修改的行键把新的数据记录追加到对应的列族中并打上一个新时间戳代表最新版本。</p>
<p>删除数据：插入带有删除标记的行进入，相当于把整个行键所在的行删了。</p>
<p>小结：hbase中所有修改和删除都是用insert方式来完成的，这是由底层HDFS文件系统特性决定的，HDFS中的文件只能一次性写入不能修改可以删除在写回。因此hbase是天生面向时间查询的数据库。例如 查询最近一段时间一个人发布的博客、发布签名、发布照片so on。</p>
<p>hbase特点</p>
<pre><code>(<span class="number">1</span>)适合大量插入同时key-value查询，例如可以输入一个key查询一个value，还可以输入一组key查询一组value。
(<span class="number">2</span>)瓶颈是硬盘的传输速度，因为有大量的插入操作和读出操作，使用SSD  SCSI  IDE不同的硬盘效率是不同的。
(<span class="number">3</span>)适合数据分析。
(<span class="number">4</span>)列式数据库会把相同列的数据都放在一块即列为单位存储。当我们查询某一列的时候只需要调出相应的块即可，这样还可以减少很多<span class="keyword">I</span>/<span class="keyword">O</span>。
(<span class="number">5</span>)如果数据元素间的相似性很高的话可以进行大幅度的压缩，相似度越高压缩比越大，甚至可以压缩到原来十几分之一、上百分之一。即节约了空间又减少了<span class="keyword">I</span>/<span class="keyword">O</span>，从而提高性能。
(<span class="number">6</span>)hbase只有主键索引，它使用的是LSM（<span class="keyword">Log</span> Structure Merge）索引，因为hbase所有的修改都是使用追加方式完成的，从数据流上看按照顺序方式写入与日志写入的方式相同，我们又可以认为数据和日志一体化，这又节约了很多空间。
</code></pre><p>oracle特点</p>
<pre><code>(<span class="number">1</span>)适合小事务短时间片密集型OLTP系统，例如在线交易系统。
(<span class="number">2</span>)瓶颈是硬盘的寻道时间（磁头移动时间），因为oracle随机写随机修改块，首先要找到块这个过程就是寻道时间，而寻道时间又由硬盘转速决定的，<span class="number">5400</span> <span class="number">7200</span> <span class="number">15000</span>转/秒 不同的转速效率也是不同的。
(<span class="number">3</span>)适合做SQL统计。
(<span class="number">4</span>)行式数据库会按照数据行顺序集中存放即行为单位存储。当我们查询某一列的时候必须把表里所有的行读完才能抽取我们所要的行，这样很不划算，还要付出很大的<span class="keyword">I</span>/<span class="keyword">O</span>资源。
(<span class="number">5</span>)那么从结构上讲oracle的压缩性能就要略逊一筹。
(<span class="number">6</span>)oracle常用的是B+树索引，比较大小来查找记录，小的走左边大的走右边，如果列中的相似度较高的话性能较差。
</code></pre><h5 id="HBase合并storefile的原因是什么？在合并的过程中会做什么操作？如果在合并过程中恰好有涉及到有关storefile的查询发生，会发生什么情况！">HBase合并storefile的原因是什么？在合并的过程中会做什么操作？如果在合并过程中恰好有涉及到有关storefile的查询发生，会发生什么情况！</h5>
<p>答：首先我们介绍一下Hbase数据存储的物理结构</p>
<pre><code>一个物理节点只能跑一个HRegionserver
一个HRegionServer可以包括很多个Region实例，可以是不同表Region
一个Region包含一个hlog和多个store（一个store就是一个列族，因为同列族元素在物理上存放在同一个地方，不同列族在物理上是分离的）
一个store包含一个memstore和多个storefile
</code></pre><p>当我们在处理数据的时候，首先把数据加载到memstore，数据越来越多直到memstore占满，再写入硬盘storefile中，每次写入形成一个单独storefile，当storefile达到一定的数量后，就会开始把小storefile合并成大storefile，因为hadoop不擅长处理小文件，文件越大性能越好。</p>
<p>在合并的过程中会抛弃删除标识的行和版本过旧的行（hbase版本抛弃方式（1）我们可以预先定义版本的个数，超过这个值就抛弃（2）还可以预先定义版本的时间长短，超过这个时间就抛弃），合并完后形成更大的storefile，当达到数量再次合并，直到storefile容量超过一定阀值后会把当前的Region进行分裂为2个并由Hmaster（hbase数据库主控节点）分配到不同的HRegionServer服务器处理实现负载均衡。</p>
<p>如果在合并过程中恰好有涉及到有关storefile的查询发生的话，我们先是把小storefile加载到内存中进行合并此时如有用户访问可以在内存中检索相关数据返回给用户，我们可以想象在内存中做一个独立镜像备份专门提供被查询需求，另一个主体在另一块内存空间里进行合并，当合并完成后释放备份的内存空间，返回到原来的状态。</p>
<h5 id="Hbase具有怎么样的一致性水平">Hbase具有怎么样的一致性水平</h5>
<p>答：hbase是最终一致性的系统，因为hbase是架构在hadoop之上的数据库，“错误是常态”是hadoop座右铭，在cap理论中hbase为了满足可用性和分区容错性牺牲了一部分的数据一致性。</p>
<p>举例：我们要进行电信的指标汇总，并且把汇总结果冗余三份分布在3个datanode中，我们可以设置阀值只要有2份结果保存了我们就可以继续做下面的操作，在一定时间范围内允许第3份结果稍后一致性同步，这就是最终一致性。所以说hbase是适用于AP理论的系统，最终一致性也满足分布式集群的特点。</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/HBase/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBase体系结构及原理]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/11/Hadoop/hbase/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/11/Hadoop/hbase/</id>
    <published>2014-05-11T02:20:31.000Z</published>
    <updated>2014-05-11T03:12:52.000Z</updated>
    <content type="html"><![CDATA[<h4 id="HBase简介">HBase简介</h4>
<p>HBase是Apache Hadoop的数据库，能够对大型数据提供随机、实时的读写访问。HBase的目标是存储并处理大型的数据。HBase是一个开源的，分布式的，多版本的，面向列的存储模型。它存储的是松散型数据。</p>
<p>HBase特性：高可靠性、高效性、面向列、可伸缩、可在廉价PC Server搭建大规模结构化存储集群</p>
<p>HBase是Google BigTable的开源实现，其相互对应如下：</p>
<table>    <tr><td></td><td>Google</td><td>HBase</td></tr><tr><td>文件存储系统</td><td>GFS</td><td>HDFS</td></tr><tr><td>海量数据处理</td><td>MapReduce</td><td>MapReduce</td></tr><tr><td>协同服务管理</td><td>Chubby</td><td>Zookeeper</td></tr><br></table>

<h4 id="HBase关系图：">HBase关系图：</h4>
<p><img src="/img/Hadoop/HBase/1.jpg" alt="HBase关系图"><br>HBase位于结构化存储层，围绕HBase，各部件对HBase的支持情况：</p>
<pre><code>Hadoop部件　　　　　　　　　　　　作用
HDFS　　　　　　　　　　　　　　高可靠的底层存储支持
MapReduce        　　　　　　　 高性能的计算能力
Zookeeper        　　　　　　　 稳定服务和failover机制
Pig&amp;Hive　　　　　　　　　　　　高层语言支持，便于数据统计
Sqoop　　　　　　　　　　　　　 提供RDBMS数据导入，便于传统数据库向HBase迁移
</code></pre><p>访问HBase的接口</p>
<pre><code>方式　　　　　　　　　　　　特点　　　　　　　　　　　　　　         场合
Native Java API　　　　　　最常规和高效        　　　　　　　　　　Hadoop MapReduce Job并行处理HBase表数据
HBase Shell　　　　　　　  最简单接口        　　　　　　　　　　　HBase管理使用
Thrift Gateway　　　　　　 利用Thrift序列化支持多种语言        　　异构系统在线访问HBase表数据
<span class="keyword">Rest</span> Gateway　　　　　　   解除语言限制        　　　　　　　　　　<span class="keyword">Rest</span>风格Http API访问
Pig　　　　　　　　　　　　Pig Latin六十编程语言处理数据        　 数据统计
Hive　　　　　　　　　　　 简单，SqlLike
</code></pre><h4 id="HBase_数据模型">HBase 数据模型</h4>
<p><img src="/img/Hadoop/HBase/2.jpg" alt="HBase数据模型"><br>组成部件说明：<br>Row Key：　　 　　Table主键 行键 Table中记录按照Row Key排序<br>Timestamp：   　　每次对数据操作对应的时间戳，也即数据的version number<br>Column Family： 　列簇，一个table在水平方向有一个或者多个列簇，列簇可由任意多个Column组成，列簇支持动态扩展，无须预定义数量及类型，二进制存储，用户需自行进行类型转换</p>
<h4 id="Table&amp;Region">Table&amp;Region</h4>
<p><img src="/img/Hadoop/HBase/3.jpg" alt="图三"><br>Table随着记录增多不断变大，会自动分裂成多份Splits，成为Regions<br>一个region由[startkey，endkey)表示<br>不同region会被Master分配给相应的RegionServer进行管理</p>
<h4 id="两张特殊表：-ROOT-_&amp;_-META-">两张特殊表：-ROOT- &amp; .META.</h4>
<p><img src="/img/Hadoop/HBase/4.jpg" alt="图四"><br>.META.        记录用户表的Region信息，同时，.META.也可以有多个region<br>-ROOT-         记录.META.表的Region信息，但是，-ROOT-只有一个region<br>Zookeeper中记录了-ROOT-表的location<br>客户端访问数据的流程：<br>Client -&gt; Zookeeper -&gt; -ROOT- -&gt; .META. -&gt; 用户数据表<br>多次网络操作，不过client端有cache缓存</p>
<h4 id="HBase_系统架构图">HBase 系统架构图</h4>
<p><img src="/img/Hadoop/HBase/5.jpg" alt="HBase系统架构图"><br>组成部件说明</p>
<h6 id="Client：">Client：</h6>
<p>使用HBase RPC机制与HMaster和HRegionServer进行通信</p>
<p>Client与HMaster进行通信进行管理类操作</p>
<p>Client与HRegionServer进行数据读写类操作</p>
<h6 id="Zookeeper：">Zookeeper：</h6>
<p>Zookeeper Quorum存储-ROOT-表地址、HMaster地址</p>
<p>HRegionServer把自己以Ephedral方式注册到Zookeeper中，HMaster随时感知各个HRegionServer的健康状况</p>
<p>Zookeeper避免HMaster单点问题</p>
<h6 id="HMaster：">HMaster：</h6>
<p>HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master在运行</p>
<p>主要负责Table和Region的管理工作：</p>
<p>管理用户对表的增删改查操作</p>
<p>管理HRegionServer的负载均衡，调整Region分布</p>
<p>Region Split后，负责新Region的分布</p>
<p>在HRegionServer停机后，负责失效HRegionServer上Region迁移</p>
<h6 id="HRegionServer：">HRegionServer：</h6>
<p>HBase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统中读写数据<br><img src="/img/Hadoop/HBase/6.jpg" alt="图6"><br>HRegionServer管理一些列HRegion对象；</p>
<p>每个HRegion对应Table中一个Region，HRegion由多个HStore组成；</p>
<p>每个HStore对应Table中一个Column Family的存储；</p>
<p>Column Family就是一个集中的存储单元，故将具有相同IO特性的Column放在一个Column Family会更高效</p>
<h6 id="HStore：">HStore：</h6>
<p>HBase存储的核心。由MemStore和StoreFile组成。</p>
<p>MemStore是Sorted Memory Buffer。用户写入数据的流程：<br><img src="/img/Hadoop/HBase/7.jpg" alt="图7"></p>
<p>Client写入 -&gt; 存入MemStore，一直到MemStore满 -&gt; Flush成一个StoreFile，直至增长到一定阈值 -&gt; 出发Compact合并操作 -&gt; 多个StoreFile合并成一个StoreFile，同时进行版本合并和数据删除 -&gt; 当StoreFiles Compact后，逐步形成越来越大的StoreFile -&gt; 单个StoreFile大小超过一定阈值后，触发Split操作，把当前Region Split成2个Region，Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer上，使得原先1个Region的压力得以分流到2个Region上</p>
<p>由此过程可知，HBase只是增加数据，有所得更新和删除操作，都是在Compact阶段做的，所以，用户写操作只需要进入到内存即可立即返回，从而保证I/O高性能。</p>
<h6 id="HLog">HLog</h6>
<p>引入HLog原因：</p>
<p>在分布式系统环境中，无法避免系统出错或者宕机，一旦HRegionServer以外退出，MemStore中的内存数据就会丢失，引入HLog就是防止这种情况</p>
<p>工作机制：</p>
<p>每个HRegionServer中都会有一个HLog对象，HLog是一个实现Write Ahead Log的类，每次用户操作写入Memstore的同时，也会写一份数据到HLog文件，HLog文件定期会滚动出新，并删除旧的文件(已持久化到StoreFile中的数据)。当HRegionServer意外终止后，HMaster会通过Zookeeper感知，HMaster首先处理遗留的HLog文件，将不同region的log数据拆分，分别放到相应region目录下，然后再将失效的region重新分配，领取到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p>
<h6 id="HBase存储格式">HBase存储格式</h6>
<p>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，格式主要有两种：</p>
<p>HFile HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile</p>
<p>HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File</p>
<h6 id="HFile">HFile</h6>
<p><img src="/img/Hadoop/HBase/8.jpg" alt="图8"><br>图片解释：<br>HFile文件不定长，长度固定的块只有两个：Trailer和FileInfo</p>
<p>Trailer中指针指向其他数据块的起始点</p>
<p>File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等</p>
<p>Data Index和Meta Index块记录了每个Data块和Meta块的起始点</p>
<p>Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制</p>
<p>每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询；每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏</p>
<p>HFile里面的每个KeyValue对就是一个简单的byte数组。这个byte数组里面包含了很多项，并且有固定的结构。<br><img src="/img/Hadoop/HBase/9.jpg" alt="图9"></p>
<p>KeyLength和ValueLength：两个固定的长度，分别代表Key和Value的长度</p>
<p>Key部分：Row Length是固定长度的数值，表示RowKey的长度，Row 就是RowKey</p>
<p>Column Family Length是固定长度的数值，表示Family的长度</p>
<p>接着就是Column Family，再接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）</p>
<p>Value部分没有这么复杂的结构，就是纯粹的二进制数据</p>
<h5 id="HLog_File">HLog File</h5>
<p><img src="/img/Hadoop/HBase/10.jpg" alt="图10"></p>
<p>HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。</p>
<p>HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/HBase/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Mahout利用朴素贝叶斯分类处理20个新闻组案例]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/08/Hadoop/mahout_2/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/08/Hadoop/mahout_2/</id>
    <published>2014-05-08T00:40:51.000Z</published>
    <updated>2014-05-08T01:09:10.000Z</updated>
    <content type="html"><![CDATA[<h3 id="朴素贝叶斯分类算法">朴素贝叶斯分类算法</h3>
<p><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html" target="_blank">朴素贝叶斯分类算法</a></p>
<h3 id="下载20个新闻组数据">下载20个新闻组数据</h3>
<p><a href="http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz" target="_blank">20个新闻组</a></p>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> data]$ <span class="keyword">ls</span>
<span class="number">20</span>news-bydate.tar.gz  <span class="number">20</span>news-bydate-test  <span class="number">20</span>news-bydate-train
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> data]$ <span class="keyword">pwd</span>
/home/<span class="keyword">grid</span>/data
</code></pre><h3 id="建立训练集">建立训练集</h3>
<pre><code>bin/mahout org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.classifier</span><span class="preprocessor">.bayes</span><span class="preprocessor">.PrepareTwentyNewsgroups</span> \ 
-p /home/grid/data/<span class="number">20</span>news-bydate-train \   指定数据源
-o /home/grid/data/bayes-train-input \     指定生成目录
-a org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.vectorizer</span><span class="preprocessor">.DefaultAnalyzer</span> \ 
-c UTF-<span class="number">8</span>

[grid@hadoop01 mahout-distribution-<span class="number">0.6</span>]$ bin/mahout org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.classifier</span><span class="preprocessor">.bayes</span><span class="preprocessor">.PrepareTwentyNewsgroups</span> -p /home/grid/data/<span class="number">20</span>news-bydate-train -o /home/grid/data/bayes-train-input -a org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.vectorizer</span><span class="preprocessor">.DefaultAnalyzer</span> -c UTF-<span class="number">8</span>
MAHOUT_LOCAL is not <span class="keyword">set</span><span class="comment">; adding HADOOP_CONF_DIR to classpath.</span>
Running on hadoop, using HADOOP_HOME=/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>
HADOOP_CONF_DIR=/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/conf
MAHOUT-JOB: /home/grid/mahout-distribution-<span class="number">0.6</span>/mahout-examples-<span class="number">0.6</span>-job<span class="preprocessor">.jar</span>
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">17</span>:<span class="number">57</span>:<span class="number">30</span> WARN driver<span class="preprocessor">.MahoutDriver</span>: No org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.classifier</span><span class="preprocessor">.bayes</span><span class="preprocessor">.PrepareTwentyNewsgroups</span><span class="preprocessor">.props</span> found on classpath, will use command-line arguments only
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">17</span>:<span class="number">57</span>:<span class="number">37</span> INFO driver<span class="preprocessor">.MahoutDriver</span>: Program took <span class="number">7635</span> ms (Minutes: <span class="number">0.12726666666666667</span>)
[grid@hadoop01 data]$ ls
<span class="number">20</span>news-bydate<span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>  <span class="number">20</span>news-bydate-test  <span class="number">20</span>news-bydate-train  bayes-train-input
[grid@hadoop01 data]$ cd bayes-train-input/
[grid@hadoop01 bayes-train-input]$ ls
alt<span class="preprocessor">.atheism</span><span class="preprocessor">.txt</span>               comp<span class="preprocessor">.windows</span><span class="preprocessor">.x</span><span class="preprocessor">.txt</span>      rec<span class="preprocessor">.sport</span><span class="preprocessor">.hockey</span><span class="preprocessor">.txt</span>  soc<span class="preprocessor">.religion</span><span class="preprocessor">.christian</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.graphics</span><span class="preprocessor">.txt</span>             misc<span class="preprocessor">.forsale</span><span class="preprocessor">.txt</span>        sci<span class="preprocessor">.crypt</span><span class="preprocessor">.txt</span>         talk<span class="preprocessor">.politics</span><span class="preprocessor">.guns</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.os</span><span class="preprocessor">.ms</span>-windows<span class="preprocessor">.misc</span><span class="preprocessor">.txt</span>   rec<span class="preprocessor">.autos</span><span class="preprocessor">.txt</span>           sci<span class="preprocessor">.electronics</span><span class="preprocessor">.txt</span>   talk<span class="preprocessor">.politics</span><span class="preprocessor">.mideast</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.sys</span><span class="preprocessor">.ibm</span><span class="preprocessor">.pc</span><span class="preprocessor">.hardware</span><span class="preprocessor">.txt</span>  rec<span class="preprocessor">.motorcycles</span><span class="preprocessor">.txt</span>     sci<span class="preprocessor">.med</span><span class="preprocessor">.txt</span>           talk<span class="preprocessor">.politics</span><span class="preprocessor">.misc</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.sys</span><span class="preprocessor">.mac</span><span class="preprocessor">.hardware</span><span class="preprocessor">.txt</span>     rec<span class="preprocessor">.sport</span><span class="preprocessor">.baseball</span><span class="preprocessor">.txt</span>  sci<span class="preprocessor">.space</span><span class="preprocessor">.txt</span>         talk<span class="preprocessor">.religion</span><span class="preprocessor">.misc</span><span class="preprocessor">.txt</span>
</code></pre><h3 id="建立测试集">建立测试集</h3>
<pre><code>bin/mahout org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.classifier</span><span class="preprocessor">.bayes</span><span class="preprocessor">.PrepareTwentyNewsgroups</span> \
-p /home/grid/data/<span class="number">20</span>news-bydate-test \     指定数据源
-o /home/grid/data/bayes-test-input \       指定生成目录
-a org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.vectorizer</span><span class="preprocessor">.DefaultAnalyzer</span> \
-c UTF-<span class="number">8</span> \

[grid@hadoop01 mahout-distribution-<span class="number">0.6</span>]$ bin/mahout org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.classifier</span><span class="preprocessor">.bayes</span><span class="preprocessor">.PrepareTwentyNewsgroups</span> -p /home/grid/data/<span class="number">20</span>news-bydate-test -o /home/grid/data/bayes-test-input -a org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.vectorizer</span><span class="preprocessor">.DefaultAnalyzer</span> -c UTF-<span class="number">8</span>
MAHOUT_LOCAL is not <span class="keyword">set</span><span class="comment">; adding HADOOP_CONF_DIR to classpath.</span>
Running on hadoop, using HADOOP_HOME=/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>
HADOOP_CONF_DIR=/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/conf
MAHOUT-JOB: /home/grid/mahout-distribution-<span class="number">0.6</span>/mahout-examples-<span class="number">0.6</span>-job<span class="preprocessor">.jar</span>
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">17</span>:<span class="number">59</span>:<span class="number">38</span> WARN driver<span class="preprocessor">.MahoutDriver</span>: No org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.classifier</span><span class="preprocessor">.bayes</span><span class="preprocessor">.PrepareTwentyNewsgroups</span><span class="preprocessor">.props</span> found on classpath, will use command-line arguments only
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">00</span>:<span class="number">00</span> INFO driver<span class="preprocessor">.MahoutDriver</span>: Program took <span class="number">21875</span> ms (Minutes: <span class="number">0.3645833333333333</span>)

[grid@hadoop01 data]$ ls
<span class="number">20</span>news-bydate<span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>  <span class="number">20</span>news-bydate-test  <span class="number">20</span>news-bydate-train  bayes-test-input  bayes-train-input
[grid@hadoop01 data]$ cd bayes-test-input/
[grid@hadoop01 bayes-test-input]$ ls
alt<span class="preprocessor">.atheism</span><span class="preprocessor">.txt</span>               comp<span class="preprocessor">.windows</span><span class="preprocessor">.x</span><span class="preprocessor">.txt</span>      rec<span class="preprocessor">.sport</span><span class="preprocessor">.hockey</span><span class="preprocessor">.txt</span>  soc<span class="preprocessor">.religion</span><span class="preprocessor">.christian</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.graphics</span><span class="preprocessor">.txt</span>             misc<span class="preprocessor">.forsale</span><span class="preprocessor">.txt</span>        sci<span class="preprocessor">.crypt</span><span class="preprocessor">.txt</span>         talk<span class="preprocessor">.politics</span><span class="preprocessor">.guns</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.os</span><span class="preprocessor">.ms</span>-windows<span class="preprocessor">.misc</span><span class="preprocessor">.txt</span>   rec<span class="preprocessor">.autos</span><span class="preprocessor">.txt</span>           sci<span class="preprocessor">.electronics</span><span class="preprocessor">.txt</span>   talk<span class="preprocessor">.politics</span><span class="preprocessor">.mideast</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.sys</span><span class="preprocessor">.ibm</span><span class="preprocessor">.pc</span><span class="preprocessor">.hardware</span><span class="preprocessor">.txt</span>  rec<span class="preprocessor">.motorcycles</span><span class="preprocessor">.txt</span>     sci<span class="preprocessor">.med</span><span class="preprocessor">.txt</span>           talk<span class="preprocessor">.politics</span><span class="preprocessor">.misc</span><span class="preprocessor">.txt</span>
comp<span class="preprocessor">.sys</span><span class="preprocessor">.mac</span><span class="preprocessor">.hardware</span><span class="preprocessor">.txt</span>     rec<span class="preprocessor">.sport</span><span class="preprocessor">.baseball</span><span class="preprocessor">.txt</span>  sci<span class="preprocessor">.space</span><span class="preprocessor">.txt</span>         talk<span class="preprocessor">.religion</span><span class="preprocessor">.misc</span><span class="preprocessor">.txt</span>
[grid@hadoop01 bayes-test-input]$ 
</code></pre><h3 id="上传生成数据到HDFS">上传生成数据到HDFS</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ hadoop fs -mkdir ./<span class="number">20</span>news
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ hadoop fs -put ./data/bayes-train-input ./<span class="number">20</span>news
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ hadoop fs -put ./data/bayes-test-input ./<span class="number">20</span>news    
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ hadoop fs -<span class="keyword">ls</span> ./<span class="number">20</span>news
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

Found <span class="number">2</span> items
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">05</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/bayes-test-input
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">05</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/bayes-train-input
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ 
</code></pre><h3 id="训练贝叶斯分类器">训练贝叶斯分类器</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> mahout-distribution-<span class="number">0.6</span>]<span class="variable">$bin</span>/mahout trainclassifier -i /user/<span class="keyword">grid</span>/<span class="number">20</span>news/bayes-train-input -o /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel -type cbayes -ng <span class="number">3</span> -<span class="keyword">source</span> hdfs
等着吧，跑个几十分钟
最后的显示信息
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO mapred.JobClient:     Combine output records=<span class="number">4916</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO mapred.JobClient:     Physical <span class="keyword">memory</span> (bytes) <span class="keyword">snapshot</span>=<span class="number">1415671808</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO mapred.JobClient:     Reduce output records=<span class="number">20</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO mapred.JobClient:     Virtual <span class="keyword">memory</span> (bytes) <span class="keyword">snapshot</span>=<span class="number">5063999488</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO mapred.JobClient:     Map output records=<span class="number">66413901</span>
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO common.HadoopUtil: Deleting /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-docCount
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO common.HadoopUtil: Deleting /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-termDocCount
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO common.HadoopUtil: Deleting /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-featureCount
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO common.HadoopUtil: Deleting /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-wordFreq
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO common.HadoopUtil: Deleting /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-tfIdf/trainer-vocabCount
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span>:<span class="number">53</span> INFO driver.MahoutDriver: Program took <span class="number">2584721</span> ms (Minutes: <span class="number">43.07868333333333</span>)
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bayes-test-input]$ hadoop fs -<span class="keyword">ls</span> ./<span class="number">20</span>news
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

Found <span class="number">4</span> items
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">05</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/bayes-test-input
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">55</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/bayes-test-input-output
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">05</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/bayes-train-input
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bayes-test-input]$ hadoop fs -<span class="keyword">ls</span> ./<span class="number">20</span>news/newsmodel 
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

Found <span class="number">5</span> items
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">30</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/_SUCCESS
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">10</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/_logs
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-tfIdf
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-thetaNormalizer
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">43</span> /user/<span class="keyword">grid</span>/<span class="number">20</span>news/newsmodel/trainer-weights
</code></pre><h3 id="测试贝叶斯分类器">测试贝叶斯分类器</h3>
<pre><code>[grid@hadoop01 data]bin/mahout testclassifier -m /user/grid/<span class="number">20</span>news/newsmodel -d /user/grid/<span class="number">20</span>news/bayes-test-input -<span class="keyword">type</span> cbayes -ng <span class="number">3</span> -source hdfs -<span class="function"><span class="keyword">method</span> <span class="title">mapreduce</span></span>
</code></pre><h4 id="这里报错了，查找原因是JVM分配内存不足">这里报错了，查找原因是JVM分配内存不足</h4>
<pre><code>Caused by: java<span class="preprocessor">.lang</span><span class="preprocessor">.OutOfMemoryError</span>: Java heap space
        at java<span class="preprocessor">.nio</span><span class="preprocessor">.HeapCharBuffer</span>.&lt;init&gt;(HeapCharBuffer<span class="preprocessor">.java</span>:<span class="number">57</span>)
        at java<span class="preprocessor">.nio</span><span class="preprocessor">.CharBuffer</span><span class="preprocessor">.allocate</span>(CharBuffer<span class="preprocessor">.java</span>:<span class="number">331</span>)
        at java<span class="preprocessor">.nio</span><span class="preprocessor">.charset</span><span class="preprocessor">.CharsetDecoder</span><span class="preprocessor">.decode</span>(CharsetDecoder<span class="preprocessor">.java</span>:<span class="number">777</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="preprocessor">.decode</span>(Text<span class="preprocessor">.java</span>:<span class="number">350</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="preprocessor">.decode</span>(Text<span class="preprocessor">.java</span>:<span class="number">327</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="preprocessor">.toString</span>(Text<span class="preprocessor">.java</span>:<span class="number">254</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.common</span><span class="preprocessor">.StringTuple</span><span class="preprocessor">.readFields</span>(StringTuple<span class="preprocessor">.java</span>:<span class="number">143</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.SequenceFile</span>$Reader<span class="preprocessor">.next</span>(SequenceFile<span class="preprocessor">.java</span>:<span class="number">1898</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.SequenceFile</span>$Reader<span class="preprocessor">.next</span>(SequenceFile<span class="preprocessor">.java</span>:<span class="number">1938</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.common</span><span class="preprocessor">.iterator</span><span class="preprocessor">.sequencefile</span><span class="preprocessor">.SequenceFileIterator</span><span class="preprocessor">.computeNext</span>(SequenceFileIterator<span class="preprocessor">.java</span>:<span class="number">95</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.common</span><span class="preprocessor">.iterator</span><span class="preprocessor">.sequencefile</span><span class="preprocessor">.SequenceFileIterator</span><span class="preprocessor">.computeNext</span>(SequenceFileIterator<span class="preprocessor">.java</span>:<span class="number">38</span>)
        at <span class="keyword">com</span><span class="preprocessor">.google</span><span class="preprocessor">.common</span><span class="preprocessor">.collect</span><span class="preprocessor">.AbstractIterator</span><span class="preprocessor">.tryToComputeNext</span>(AbstractIterator<span class="preprocessor">.java</span>:<span class="number">141</span>)
        at <span class="keyword">com</span><span class="preprocessor">.google</span><span class="preprocessor">.common</span><span class="preprocessor">.collect</span><span class="preprocessor">.AbstractIterator</span><span class="preprocessor">.hasNext</span>(AbstractIterator<span class="preprocessor">.java</span>:<span class="number">136</span>)
        at <span class="keyword">com</span><span class="preprocessor">.google</span><span class="preprocessor">.common</span><span class="preprocessor">.collect</span><span class="preprocessor">.Iterators</span>$5<span class="preprocessor">.hasNext</span>(Iterators<span class="preprocessor">.java</span>:<span class="number">525</span>)
        at <span class="keyword">com</span><span class="preprocessor">.google</span><span class="preprocessor">.common</span><span class="preprocessor">.collect</span><span class="preprocessor">.ForwardingIterator</span><span class="preprocessor">.hasNext</span>(ForwardingIterator<span class="preprocessor">.java</span>:<span class="number">43</span>)
        at org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.classifier</span><span class="preprocessor">.bayes</span><span class="preprocessor">.SequenceFileModelReader</span><span class="preprocessor">.loadFeatureWeights</span>(SequenceFileModelReader<span class="preprocessor">.java</span>:<span class="number">72</span>)
每个节点修改mapred-site<span class="preprocessor">.xml</span>文件
    [grid@hadoop02 conf]$ cat mapred-site<span class="preprocessor">.xml</span> 
    &lt;?xml version=<span class="string">"1.0"</span>?&gt;
    &lt;?xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span>?&gt;

    &lt;!-- Put site-specific property overrides <span class="keyword">in</span> this file. --&gt;

    &lt;configuration&gt;
            &lt;property&gt;
                    &lt;name&gt;mapred<span class="preprocessor">.job</span><span class="preprocessor">.tracker</span>&lt;/name&gt;
                            &lt;value&gt;hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9001</span>&lt;/value&gt;
            &lt;/property&gt;
            &lt;property&gt;
                     &lt;name&gt;mapred<span class="preprocessor">.child</span><span class="preprocessor">.java</span><span class="preprocessor">.opts</span>&lt;/name&gt;
                            &lt;value&gt;-Xmx1000m&lt;/value&gt;
            &lt;/property&gt;
    &lt;/configuration&gt;
</code></pre><h3 id="运行成功后">运行成功后</h3>
<pre><code>[grid@hadoop01 data]$ hadoop fs -ls ./<span class="number">20</span>news/
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

Found <span class="number">4</span> items
drwxr-xr-<span class="built_in">x</span>   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">05</span> /user/grid/<span class="number">20</span>news/bayes-test-input
drwxr-xr-<span class="built_in">x</span>   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">19</span>:<span class="number">28</span> /user/grid/<span class="number">20</span>news/bayes-test-input-output
drwxr-xr-<span class="built_in">x</span>   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">05</span> /user/grid/<span class="number">20</span>news/bayes-train-input
drwxr-xr-<span class="built_in">x</span>   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">18</span>:<span class="number">53</span> /user/grid/<span class="number">20</span>news/newsmodel

运行成功后最后的屏幕信息
Summary 
------------------------------------------------------- 
Correctly Classified Instances          :      <span class="number">18369</span>   <span class="number">97.5621</span>% 
Incorrectly Classified Instances        :        <span class="number">459</span>    <span class="number">2.4379</span>% 
Total Classified Instances              :      <span class="number">18828</span> 

======================================================= 
Confusion Matrix 
------------------------------------------------------- 
a    b    c    d    e    f    g    h    i    j    k    l    m    n    o    p    q    r    s    t    &lt;--Classified as 
<span class="number">994</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">994</span>   a     = rec<span class="preprocessor">.motorcycles</span> 
<span class="number">0</span>    <span class="number">976</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">1</span>     |  <span class="number">980</span>   b     = comp<span class="preprocessor">.windows</span><span class="preprocessor">.x</span> 
<span class="number">7</span>    <span class="number">0</span>    <span class="number">929</span>  <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">940</span>   c     = talk<span class="preprocessor">.politics</span><span class="preprocessor">.mideast</span>
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">905</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">3</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>     |  <span class="number">910</span>   d     = talk<span class="preprocessor">.politics</span><span class="preprocessor">.guns</span> 
<span class="number">4</span>    <span class="number">1</span>    <span class="number">4</span>    <span class="number">27</span>   <span class="number">388</span>  <span class="number">1</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">5</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">2</span>    <span class="number">2</span>    <span class="number">149</span>  <span class="number">7</span>    <span class="number">2</span>    <span class="number">33</span>   <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">628</span>   e     = talk<span class="preprocessor">.religion</span><span class="preprocessor">.misc</span>
<span class="number">3</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">985</span>  <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">990</span>   f     = rec<span class="preprocessor">.autos</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">993</span>  <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">994</span>   g     = rec<span class="preprocessor">.sport</span><span class="preprocessor">.baseball</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">998</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">999</span>   h     = rec<span class="preprocessor">.sport</span><span class="preprocessor">.hockey</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">956</span>  <span class="number">0</span>    <span class="number">2</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">1</span>     |  <span class="number">961</span>   i     = comp<span class="preprocessor">.sys</span><span class="preprocessor">.mac</span><span class="preprocessor">.hardware</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">981</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">5</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">987</span>   j     = sci<span class="preprocessor">.space</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">978</span>  <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">1</span>     |  <span class="number">982</span>   k     = comp<span class="preprocessor">.sys</span><span class="preprocessor">.ibm</span><span class="preprocessor">.pc</span><span class="preprocessor">.hardware</span> 
<span class="number">1</span>    <span class="number">0</span>    <span class="number">3</span>    <span class="number">36</span>   <span class="number">0</span>    <span class="number">1</span>    <span class="number">2</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">5</span>    <span class="number">0</span>    <span class="number">697</span>  <span class="number">4</span>    <span class="number">0</span>    <span class="number">3</span>    <span class="number">3</span>    <span class="number">19</span>   <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">775</span>   l     = talk<span class="preprocessor">.politics</span><span class="preprocessor">.misc</span> 
<span class="number">0</span>    <span class="number">2</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">0</span>    <span class="number">966</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">1</span>     |  <span class="number">973</span>   m     = comp<span class="preprocessor">.graphics</span> 
<span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">6</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">971</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">3</span>    <span class="number">0</span>     |  <span class="number">981</span>   n     = sci<span class="preprocessor">.electronics</span> 
<span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">992</span>  <span class="number">1</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">1</span>     |  <span class="number">997</span>   o     = soc<span class="preprocessor">.religion</span><span class="preprocessor">.christian</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">988</span>  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>     |  <span class="number">990</span>   p     = sci<span class="preprocessor">.med</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">985</span>  <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>     |  <span class="number">991</span>   q     = sci<span class="preprocessor">.crypt</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">19</span>   <span class="number">0</span>    <span class="number">1</span>    <span class="number">775</span>  <span class="number">0</span>    <span class="number">0</span>     |  <span class="number">799</span>   r     = alt<span class="preprocessor">.atheism</span> 
<span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">3</span>    <span class="number">1</span>    <span class="number">2</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">3</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">5</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">957</span>  <span class="number">0</span>     |  <span class="number">972</span>   s     = misc<span class="preprocessor">.forsale</span> 
<span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">8</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">6</span>    <span class="number">0</span>    <span class="number">6</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">10</span>   <span class="number">955</span>   |  <span class="number">985</span>   t     = comp<span class="preprocessor">.os</span><span class="preprocessor">.ms</span>-windows<span class="preprocessor">.misc</span> 
</code></pre><p>这个混合矩阵的意思说明： 上述a到t分别是代表了有20类别，这就是我们之前给的20个输入文件，列中的数据说明每个类别中被分配到的字节个数，classified说明应该被分配到的总数<br>    994  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   |  994  a     = rec.motorcycles<br>意思为rec.motorcycles 本来是属于 a，有381篇文档被划为了a类，这个是正确的数据，其它的分别表示划到 b~t类中的数目。我们可以看到其正确率为 994/994=1,可见其正确率还是很高的了。</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Mahout" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Mahout/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Mahout的初步体验]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/08/Hadoop/mahout_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/08/Hadoop/mahout_1/</id>
    <published>2014-05-08T00:19:47.000Z</published>
    <updated>2014-05-08T00:37:40.000Z</updated>
    <content type="html"><![CDATA[<h3 id="下载解压Mahout">下载解压Mahout</h3>
<pre><code>[grid@hadoop01 ~]$ ls
access_log<span class="preprocessor">.txt</span>                 Documents                       Music                  Public
apache-ant-<span class="number">1.9</span><span class="number">.3</span>               Downloads                       Pictures               score<span class="preprocessor">.txt</span>
apache-ant-<span class="number">1.9</span><span class="number">.3</span>-bin<span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>    hadoop-<span class="number">1.2</span><span class="number">.1</span>                    pig-<span class="number">0.12</span><span class="number">.1</span>             share
apache-hive-<span class="number">0.13</span><span class="number">.0</span>-bin         hadoop-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>             pig-<span class="number">0.12</span><span class="number">.1</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>      Templates
apache-hive-<span class="number">0.13</span><span class="number">.0</span>-bin<span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>  input                           pig_1398238654807<span class="preprocessor">.log</span>  Videos
Desktop                        mahout-distribution-<span class="number">0.6</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>  pig_1398239586082<span class="preprocessor">.log</span>  workspace
[grid@hadoop01 ~]$ tar -xzvf mahout-distribution-<span class="number">0.6</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
[grid@hadoop01 ~]$ ls
access_log<span class="preprocessor">.txt</span>                 Downloads                       Pictures               share
apache-ant-<span class="number">1.9</span><span class="number">.3</span>               hadoop-<span class="number">1.2</span><span class="number">.1</span>                    pig-<span class="number">0.12</span><span class="number">.1</span>             Templates
apache-ant-<span class="number">1.9</span><span class="number">.3</span>-bin<span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>    hadoop-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>             pig-<span class="number">0.12</span><span class="number">.1</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>      Videos
apache-hive-<span class="number">0.13</span><span class="number">.0</span>-bin         input                           pig_1398238654807<span class="preprocessor">.log</span>  workspace
apache-hive-<span class="number">0.13</span><span class="number">.0</span>-bin<span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>  mahout-distribution-<span class="number">0.6</span>         pig_1398239586082<span class="preprocessor">.log</span>
Desktop                        mahout-distribution-<span class="number">0.6</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>  Public
Documents                      Music                           score<span class="preprocessor">.txt</span>
</code></pre><h3 id="配置环境变量">配置环境变量</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]<span class="variable">$vi</span> .bash_profile 
PATH=<span class="variable">$PATH</span>:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>:/bin:/home/<span class="keyword">grid</span>/pig-<span class="number">0.12</span><span class="number">.1</span>/bin:<span class="variable">$HOME</span>/bin
JAVA_HOME=/usr/jdk1<span class="number">.7</span><span class="number">.0</span>_51
PIG_CLASSPATH=/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/conf/
MAHOUT_HOME=/home/<span class="keyword">grid</span>/mahout-distribution-<span class="number">0.6</span>
MAHOUT_CONF_DIR=/home/<span class="keyword">grid</span>/mahout-distribution-<span class="number">0.6</span>/conf
PATH=<span class="variable">$PATH</span>:<span class="variable">$MAHOUT_HOME</span>/conf:<span class="variable">$MAHOUT_HOME</span>/bin
export MAHOUT_HOME
export MAHOUT_CONF_DIR
export PIG_CLASSPATH
export PATH
export JAVA_HOME
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ <span class="keyword">source</span>  .bash_profile  
</code></pre><h3 id="启动hadoop">启动hadoop</h3>
<h3 id="检验Mohout是否安装成功">检验Mohout是否安装成功</h3>
<pre><code>[grid<span class="variable">@hadoop01</span> mahout-distribution-<span class="number">0</span>.<span class="number">6</span>]<span class="variable">$ </span>bin/mahout 
看到一些算法就<span class="constant">OK</span>了
</code></pre><h3 id="利用Mohout运行一个简单的k-means算法">利用Mohout运行一个简单的k-means算法</h3>
<p><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html" target="_blank">关于Kmeans的详解</a></p>
<h4 id="下载数据">下载数据</h4>
<p>下载一个文件synthetic_control.data，<a href="http://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data" target="_blank">下载地址</a>,并把这个文件放在$MAHOUT_HOME目录下</p>
<h4 id="上传数据">上传数据</h4>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> mahout-distribution-<span class="number">0.6</span>]$ hadoop fs -mkdir testdata
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> mahout-distribution-<span class="number">0.6</span>]$ hadoop fs -put /home/<span class="keyword">grid</span>/mahout-distribution-<span class="number">0.6</span>/synthetic_control.data  testdata
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> mahout-distribution-<span class="number">0.6</span>]$ hadoop fs -<span class="keyword">ls</span>  ./testdata
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

Found <span class="number">1</span> items
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup     <span class="number">288374</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">16</span>:<span class="number">49</span> /user/<span class="keyword">grid</span>/testdata/synthetic_control.data
</code></pre><h4 id="运行k-means算法">运行k-means算法</h4>
<pre><code>[grid@hadoop01 mahout-distribution-<span class="number">0.6</span>]$ mahout org<span class="preprocessor">.apache</span><span class="preprocessor">.mahout</span><span class="preprocessor">.clustering</span><span class="preprocessor">.syntheticcontrol</span><span class="preprocessor">.kmeans</span><span class="preprocessor">.Job</span>
等待一会
最后屏幕刷出结果（部分）
<span class="number">1.0</span> : [distance=<span class="number">28.040983886281218</span>]: [<span class="number">30.461</span>, <span class="number">29.172</span>, <span class="number">32.510</span>, <span class="number">29.727</span>, <span class="number">23.834</span>, <span class="number">26.091</span>, <span class="number">23.735</span>, <span class="number">23.111</span>, <span class="number">31.726</span>, <span class="number">22.930</span>, <span class="number">31.051</span>, <span class="number">23.321</span>, <span class="number">27.188</span>, <span class="number">18.231</span>, <span class="number">22.951</span>, <span class="number">21.944</span>, <span class="number">25.388</span>, <span class="number">26.809</span>, <span class="number">20.733</span>, <span class="number">16.576</span>, <span class="number">23.558</span>, <span class="number">20.733</span>, <span class="number">18.346</span>, <span class="number">24.153</span>, <span class="number">16.649</span>, <span class="number">16.234</span>, <span class="number">17.985</span>, <span class="number">11.772</span>, <span class="number">19.873</span>, <span class="number">19.622</span>, <span class="number">19.365</span>, <span class="number">14.608</span>, <span class="number">11.265</span>, <span class="number">20.604</span>, <span class="number">11.180</span>, <span class="number">18.643</span>, <span class="number">14.246</span>, <span class="number">10.830</span>, <span class="number">7.235</span>, <span class="number">10.194</span>, <span class="number">16.731</span>, <span class="number">11.470</span>, <span class="number">7.311</span>, <span class="number">10.611</span>, <span class="number">6.924</span>, <span class="number">3.440</span>, <span class="number">9.465</span>, <span class="number">4.764</span>, <span class="number">2.838</span>, <span class="number">8.807</span>, <span class="number">1.960</span>, <span class="number">2.864</span>, <span class="number">6.728</span>, <span class="number">0.369</span>, <span class="number">1.374</span>, -<span class="number">0.167</span>, <span class="number">2.125</span>, <span class="number">8.306</span>, <span class="number">4.908</span>, -<span class="number">0.432</span>]
        <span class="number">1.0</span> : [distance=<span class="number">26.945992695091952</span>]: [<span class="number">30.817</span>, <span class="number">28.079</span>, <span class="number">24.628</span>, <span class="number">23.933</span>, <span class="number">28.660</span>, <span class="number">25.704</span>, <span class="number">27.501</span>, <span class="number">23.513</span>, <span class="number">30.377</span>, <span class="number">27.595</span>, <span class="number">22.938</span>, <span class="number">26.684</span>, <span class="number">25.208</span>, <span class="number">26.834</span>, <span class="number">22.931</span>, <span class="number">17.732</span>, <span class="number">17.544</span>, <span class="number">24.167</span>, <span class="number">25.602</span>, <span class="number">19.269</span>, <span class="number">14.978</span>, <span class="number">17.223</span>, <span class="number">18.962</span>, <span class="number">22.281</span>, <span class="number">17.035</span>, <span class="number">23.789</span>, <span class="number">14.878</span>, <span class="number">18.113</span>, <span class="number">10.981</span>, <span class="number">11.661</span>, <span class="number">14.331</span>, <span class="number">19.942</span>, <span class="number">11.175</span>, <span class="number">10.714</span>, <span class="number">15.675</span>, <span class="number">15.468</span>, <span class="number">16.010</span>, <span class="number">14.972</span>, <span class="number">15.101</span>, <span class="number">15.131</span>, <span class="number">15.154</span>, <span class="number">10.492</span>, <span class="number">14.754</span>, <span class="number">5.222</span>, <span class="number">5.393</span>, <span class="number">13.606</span>, <span class="number">11.775</span>, <span class="number">6.307</span>, <span class="number">3.370</span>, <span class="number">10.107</span>, <span class="number">7.779</span>, <span class="number">10.209</span>, <span class="number">1.493</span>, <span class="number">4.822</span>, <span class="number">0.019</span>, <span class="number">8.019</span>, -<span class="number">0.279</span>, -<span class="number">0.049</span>, <span class="number">5.757</span>, <span class="number">2.718</span>]
        <span class="number">1.0</span> : [distance=<span class="number">25.05284328269594</span>]: [<span class="number">31.347</span>, <span class="number">28.245</span>, <span class="number">34.275</span>, <span class="number">29.885</span>, <span class="number">30.573</span>, <span class="number">32.373</span>, <span class="number">24.031</span>, <span class="number">24.057</span>, <span class="number">24.099</span>, <span class="number">23.777</span>, <span class="number">28.993</span>, <span class="number">29.853</span>, <span class="number">26.485</span>, <span class="number">29.245</span>, <span class="number">28.145</span>, <span class="number">22.528</span>, <span class="number">20.390</span>, <span class="number">20.570</span>, <span class="number">27.921</span>, <span class="number">18.786</span>, <span class="number">22.144</span>, <span class="number">20.163</span>, <span class="number">17.616</span>, <span class="number">19.541</span>, <span class="number">20.342</span>, <span class="number">22.061</span>, <span class="number">21.358</span>, <span class="number">23.951</span>, <span class="number">13.447</span>, <span class="number">12.974</span>, <span class="number">18.406</span>, <span class="number">17.349</span>, <span class="number">17.425</span>, <span class="number">11.041</span>, <span class="number">14.912</span>, <span class="number">10.147</span>, <span class="number">16.731</span>, <span class="number">9.845</span>, <span class="number">14.840</span>, <span class="number">18.283</span>, <span class="number">18.426</span>, <span class="number">10.059</span>, <span class="number">16.760</span>, <span class="number">14.187</span>, <span class="number">14.301</span>, <span class="number">14.277</span>, <span class="number">12.823</span>, <span class="number">15.574</span>, <span class="number">10.789</span>, <span class="number">10.957</span>, <span class="number">8.361</span>, <span class="number">4.116</span>, <span class="number">3.732</span>, <span class="number">3.508</span>, <span class="number">2.288</span>, <span class="number">9.768</span>, <span class="number">9.661</span>, <span class="number">2.183</span>, <span class="number">6.933</span>, <span class="number">4.670</span>]
        <span class="number">1.0</span> : [distance=<span class="number">25.150872291523466</span>]: [<span class="number">26.273</span>, <span class="number">31.229</span>, <span class="number">29.741</span>, <span class="number">34.208</span>, <span class="number">33.329</span>, <span class="number">33.610</span>, <span class="number">31.072</span>, <span class="number">22.530</span>, <span class="number">28.587</span>, <span class="number">21.130</span>, <span class="number">23.557</span>, <span class="number">28.078</span>, <span class="number">27.546</span>, <span class="number">25.825</span>, <span class="number">18.454</span>, <span class="number">25.903</span>, <span class="number">24.448</span>, <span class="number">24.003</span>, <span class="number">23.199</span>, <span class="number">22.158</span>, <span class="number">17.711</span>, <span class="number">23.922</span>, <span class="number">20.550</span>, <span class="number">15.913</span>, <span class="number">17.699</span>, <span class="number">13.883</span>, <span class="number">17.494</span>, <span class="number">16.360</span>, <span class="number">20.679</span>, <span class="number">11.790</span>, <span class="number">18.424</span>, <span class="number">10.493</span>, <span class="number">11.001</span>, <span class="number">17.994</span>, <span class="number">11.673</span>, <span class="number">11.014</span>, <span class="number">11.437</span>, <span class="number">16.197</span>, <span class="number">16.435</span>, <span class="number">7.331</span>, <span class="number">15.089</span>, <span class="number">16.779</span>, <span class="number">14.449</span>, <span class="number">9.551</span>, <span class="number">11.331</span>, <span class="number">10.564</span>, <span class="number">5.992</span>, <span class="number">8.369</span>, <span class="number">11.402</span>, <span class="number">7.865</span>, <span class="number">2.526</span>, <span class="number">4.632</span>, <span class="number">9.335</span>, <span class="number">6.772</span>, <span class="number">3.018</span>, <span class="number">3.675</span>, <span class="number">0.455</span>, <span class="number">5.362</span>, <span class="number">6.945</span>, <span class="number">7.901</span>]
        <span class="number">1.0</span> : [distance=<span class="number">29.28744984720542</span>]: [<span class="number">26.148</span>, <span class="number">30.828</span>, <span class="number">27.122</span>, <span class="number">31.797</span>, <span class="number">26.812</span>, <span class="number">24.681</span>, <span class="number">31.379</span>, <span class="number">22.047</span>, <span class="number">22.034</span>, <span class="number">24.293</span>, <span class="number">30.875</span>, <span class="number">22.493</span>, <span class="number">30.889</span>, <span class="number">19.167</span>, <span class="number">19.199</span>, <span class="number">27.696</span>, <span class="number">17.370</span>, <span class="number">27.648</span>, <span class="number">23.842</span>, <span class="number">26.493</span>, <span class="number">23.635</span>, <span class="number">23.577</span>, <span class="number">20.884</span>, <span class="number">18.786</span>, <span class="number">18.898</span>, <span class="number">18.091</span>, <span class="number">22.021</span>, <span class="number">20.674</span>, <span class="number">23.890</span>, <span class="number">12.646</span>, <span class="number">18.448</span>, <span class="number">17.732</span>, <span class="number">17.897</span>, <span class="number">14.679</span>, <span class="number">13.598</span>, <span class="number">12.689</span>, <span class="number">19.832</span>, <span class="number">12.489</span>, <span class="number">9.745</span>, <span class="number">18.990</span>, <span class="number">18.820</span>, <span class="number">16.517</span>, <span class="number">12.024</span>, <span class="number">14.131</span>, <span class="number">13.394</span>, <span class="number">15.473</span>, <span class="number">11.140</span>, <span class="number">5.094</span>, <span class="number">15.265</span>, <span class="number">14.651</span>, <span class="number">8.299</span>, <span class="number">3.163</span>, <span class="number">12.039</span>, <span class="number">4.893</span>, <span class="number">7.552</span>, <span class="number">12.315</span>, <span class="number">9.581</span>, <span class="number">5.462</span>, <span class="number">2.984</span>, <span class="number">8.981</span>]
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">17</span>:<span class="number">10</span>:<span class="number">13</span> INFO clustering<span class="preprocessor">.ClusterDumper</span>: Wrote <span class="number">6</span> clusters
<span class="number">14</span>/<span class="number">05</span>/<span class="number">07</span> <span class="number">17</span>:<span class="number">10</span>:<span class="number">13</span> INFO driver<span class="preprocessor">.MahoutDriver</span>: Program took <span class="number">456695</span> ms (Minutes: <span class="number">7.611583333333333</span>)
</code></pre><h4 id="看到一系列clusters，就算成功">看到一系列clusters，就算成功</h4>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ hadoop fs -<span class="keyword">ls</span> ./output
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

Found <span class="number">13</span> items
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">10</span> /user/<span class="keyword">grid</span>/output/clusteredPoints
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">03</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">0</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">03</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">1</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">09</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">10</span>-final
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">04</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">2</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">05</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">3</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">05</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">4</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">06</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">5</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">07</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">6</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">07</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">7</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">08</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">8</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">08</span> /user/<span class="keyword">grid</span>/output/clusters-<span class="number">9</span>
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">07</span> <span class="number">17</span>:<span class="number">03</span> /user/<span class="keyword">grid</span>/output/data
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ 
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Mahout" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Mahout/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Linux下进程与线程]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/05/04/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/process/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/05/04/操作系统/process/</id>
    <published>2014-05-04T12:12:50.000Z</published>
    <updated>2014-05-04T12:49:57.000Z</updated>
    <content type="html"><![CDATA[<h3 id="进程">进程</h3>
<p>进程是程序执行时的一个实例，即它是程序已经执行到课中程度的数据结构的汇集。从内核的观点看，进程的目的就是担当分配系统资源（CPU时间、内存等）的基本单位。</p>
<h3 id="线程">线程</h3>
<p>线程是进程的一个执行流，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。一个进程由几个线程组成（拥有很多相对独立的执行流的用户程序共享应用程序的大部分数据结构），线程与同属一个进程的其他的线程共享进程所拥有的全部资源。</p>
<p>进程——资源分配的最小单位，线程——程序执行的最小单位</p>
<h4 id="例如：">例如：</h4>
<p>假设用户启动了一个窗口中的数据库应用程序，操作系统就将对数据库的调用表示为一个进程。假设用户要从数据库中产生一份工资单报表，并传到一个文件中，这是一个子任务；在产生工资单报表的过程中，用户又可以输人数据库查询请求，这又是一个子任务。这样，操作系统则把每一个请求――工资单报表和新输人的数据查询表示为数据库进程中的独立的线程。</p>
<p>线程可以在处理器上独立调度执行，这样，在多处理器环境下就允许几个线程各自在单独处理器上进行。操作系统提供线程就是为了方便而有效地实现这种并发性。</p>
<h3 id="引入线程的好处">引入线程的好处</h3>
<p>易于调度。</p>
<p>提高并发性。通过线程可方便有效地实现并发性。进程可创建多个线程来执行同一程序的不同部分。</p>
<p>开销少。创建线程比创建进程要快，所需开销很少。</p>
<p>利于充分发挥多处理器的功能。通过创建多线程进程，每个线程在一个处理器上运行，从而实现应用程序的并发性，使每个处理器都得到充分运行。</p>
<h3 id="线程的基本状态">线程的基本状态</h3>
<p>就绪：调用线程的start方法后线程进入就绪状态</p>
<p>阻塞：线程调度系统将就绪状态的线程转为运行状态</p>
<p>运行：遇到synchronized语句时，由运行状态转为阻塞</p>
<p>结束：当线程关联的代码执行完后，线程变为结束状态</p>
<h3 id="进程与线程的区别">进程与线程的区别</h3>
<p>进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。</p>
<p>总的来说就是：进程有独立的地址空间，线程没有单独的地址空间（同一进程内的线程共享进程的地址空间）。</p>
<h3 id="多线程有几种实现方法">多线程有几种实现方法</h3>
<p>多线程有两种实现方法。一种是继承Thread类，一种是实现Runnable接口。<br>同步有两种方法。一种同步方法，一种同步代码。分别是synchronized，wait与notify</p>
<h3 id="多线程同步和互斥异同">多线程同步和互斥异同</h3>
<p>线程同步是指线程之间所具有的一种制约关系，一个线程的执行依赖另一个线程的消息，当它没有得到另一个线程的消息时应等待，直到消息到达时才被唤醒。</p>
<p>线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。</p>
]]></content>
    
    
      <category term="进程与线程" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="操作系统" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive与Mysql本地安装]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/24/Hadoop/hadoop_hive&mysql/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/24/Hadoop/hadoop_hive&mysql/</id>
    <published>2014-04-24T10:49:28.000Z</published>
    <updated>2014-04-24T11:59:03.000Z</updated>
    <content type="html"><![CDATA[<h5 id="解压hive包">解压hive包</h5>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>tar -xzvf apache-hive-<span class="number">0</span>.<span class="number">13.0</span>-bin.tar.gz 
</code></pre><h5 id="配置环境变量">配置环境变量</h5>
<pre><code>[root@hadoop01 apache-hive-<span class="number">0.13</span>.<span class="number">0</span>-bin]<span class="comment"># cat /etc/profile</span>
<span class="keyword">export</span> JAVA_HOME=/usr/jdk1.<span class="number">7.0</span>_51
<span class="keyword">export</span> HADOOP_FREFIX=/home/grid/hadoop-<span class="number">1.2</span>.<span class="number">1</span>
<span class="keyword">export</span> HADOOP_COMMON_HOME=<span class="variable">${HADOOP_FREFIX}</span>
<span class="keyword">export</span> HADOOP_CONF_DIR=<span class="variable">${HADOOP_FREFIX}</span>/conf
<span class="keyword">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar
<span class="keyword">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">${HADOOP_FREFIX}</span>/bin:<span class="variable">${HADOOP_FREFIX}</span>/sbin:<span class="variable">$PATH</span>
<span class="keyword">export</span> HADOOP_HOME=/home/grid/hadoop-<span class="number">1.2</span>.<span class="number">1</span>
<span class="keyword">export</span> PATH=/home/grid/apache-ant-<span class="number">1.9</span>.<span class="number">3</span>/bin:<span class="variable">$PATH</span>
<span class="keyword">export</span> HIVE_HOME=/home/grid/apache-hive-<span class="number">0.13</span>.<span class="number">0</span>-bin
<span class="keyword">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span>
[root@hadoop01 apache-hive-<span class="number">0.13</span>.<span class="number">0</span>-bin]<span class="comment"># source  /etc/profile</span>
</code></pre><h5 id="更改配置文件">更改配置文件</h5>
<pre><code>[grid@hadoop01 conf]$ cp hive-env.sh.template  hive-env.sh
[grid@hadoop01 conf]$ vi hive-env.sh
HADOOP_HOME=/home/grid/hadoop-1.2.1
[grid@hadoop01 conf]$ cp hive-default.xml.template hive-site.xml
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/grid/apache-hive-0.13.0-bin/warehouse<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;<span class="title">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="title">description</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
  <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
  <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/grid/apache-hive-0.13.0-bin/temp<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
  <span class="tag">&lt;<span class="title">description</span>&gt;</span>Scratch space for Hive jobs<span class="tag">&lt;/<span class="title">description</span>&gt;</span>
  <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
          <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
         <span class="tag">&lt;<span class="title">value</span>&gt;</span> jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true <span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
         <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
         <span class="tag">&lt;<span class="title">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
 <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
         <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
          <span class="tag">&lt;<span class="title">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
 <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
          <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
          <span class="tag">&lt;<span class="title">value</span>&gt;</span>hive <span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>false<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;<span class="title">description</span>&gt;</span>
<span class="tag">&lt;/<span class="title">description</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
</code></pre><h6 id="这里有技巧">这里有技巧</h6>
<p>例如在一个文件里面有几千行，这样查找：</p>
<pre><code>[grid@hadoop01 conf]$ cat hive-site<span class="preprocessor">.xml</span> |grep -n hive<span class="preprocessor">.metastore</span><span class="preprocessor">.schema</span><span class="preprocessor">.verification</span> 
<span class="number">2371</span>:  &lt;name&gt;hive<span class="preprocessor">.metastore</span><span class="preprocessor">.schema</span><span class="preprocessor">.verification</span>&lt;/name&gt;
vi  <span class="keyword">set</span> nu显示行号
</code></pre><h5 id="加入连接器jar包">加入连接器jar包</h5>
<p>lib文件夹缺少mysql的连接器mysql-connector-java-5.1.25-bin.jar,下载地址<a href="http://download.csdn.net/detail/xqj198404/6338973" target="_blank">http://download.csdn.net/detail/xqj198404/6338973</a> 将其放到hive的lib目录下</p>
<h5 id="安装mysql">安装mysql</h5>
<p>查看系统是否安装了MySQL，使用命令：</p>
<pre><code><span class="preprocessor">#rpm -qa | grep mysql </span>
</code></pre><h5 id="卸载已安装的MySQL">卸载已安装的MySQL</h5>
<p>卸载mysql命令如下：</p>
<pre><code><span class="variable">#rpm</span> <span class="attribute">-e</span> <span class="subst">--</span>nodeps  mysql<span class="attribute">-libs</span><span class="subst">-</span><span class="number">5.1</span><span class="number">.61</span><span class="subst">-</span><span class="number">4.</span>el6<span class="built_in">.</span>x86_64
要将 /<span class="built_in">var</span>/lib/mysql文件夹下的所有文件都删除干净
</code></pre><h5 id="安装MySQL">安装MySQL</h5>
<p>安装顺序: 先安装服务器,然后再安装客户端。找到安装包所在位置，直接安装即可。</p>
<pre><code>[root<span class="variable">@hadoop01</span> soft]<span class="comment"># rpm -ivh MySQL-server-5.5.37-1.linux2.6.x86_64.rpm </span>
[root<span class="variable">@hadoop01</span> soft]<span class="comment"># rpm -ivh MySQL-client-5.5.37-1.linux2.6.x86_64.rpm </span>
[root<span class="variable">@hadoop01</span> soft]<span class="comment"># /etc/init.d/mysql start</span>
<span class="constant">Starting</span> <span class="constant">MySQL</span>...                                          [  <span class="constant">OK</span>  ]
</code></pre><p>修改mysql配置文件并重启MySQL,安装完成后在/usr/share/mysql目录中会有一个mysql的启动脚本mysql.server及示例配置文件等(如my-huge.cnf、my-large.cnf、my-medium.cnf)，拷贝一个示例配置文件作为mysql的配置文件：</p>
<pre><code>cp /usr/share/mysql/<span class="keyword">my</span>-medium.cnf /etc/<span class="keyword">my</span>.cnf
[root<span class="variable">@hadoop01</span> mysql]<span class="comment"># /etc/init.d/mysql restart</span>
Shutting down MySQL.                                       [  OK  ]
Starting MySQL...                                          [  OK  ]
[root<span class="variable">@hadoop01</span> mysql]<span class="comment"># </span>
</code></pre><p>登陆mysql，下面给root用户设置密码</p>
<pre><code>/usr/bin/mysqladmin <span class="attribute">-u</span> root password <span class="number">123456</span> 
mysql <span class="attribute">-u</span> root <span class="attribute">-p</span> 
Enter password: (输入修改后的密码<span class="number">123456</span>) <span class="built_in">.</span>
mysql<span class="subst">&gt;</span>
</code></pre><h5 id="创建hive元数据库及创建hive用户并授权">创建hive元数据库及创建hive用户并授权</h5>
<h6 id="创建hive元数据库">创建hive元数据库</h6>
<pre><code>mysql&gt; <span class="keyword">create</span> database hive;
</code></pre><h6 id="对hive用户并授权,注意,此处的密码必须和hive-site-xml的javax-jdo-option-ConnectionPassword的密码相一致">对hive用户并授权,注意,此处的密码必须和hive-site.xml的javax.jdo.option.ConnectionPassword的密码相一致</h6>
<pre><code>mysql<span class="subst">&gt;</span> grant <span class="literal">all</span> privileges <span class="keyword">on</span> hive<span class="built_in">.</span><span class="subst">*</span> <span class="keyword">to</span> <span class="string">'hive'</span>@<span class="string">'localhost'</span>  IDENTIFIED <span class="keyword">BY</span> <span class="string">"hive"</span>;
mysql<span class="subst">&gt;</span> GRANT <span class="literal">ALL</span>  PRIVILEGES <span class="keyword">ON</span> hive<span class="built_in">.</span><span class="subst">*</span> <span class="keyword">TO</span> <span class="string">'hive'</span>@<span class="string">'%'</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">'hive'</span>;
mysql<span class="subst">&gt;</span> GRANT <span class="literal">ALL</span>  PRIVILEGES <span class="keyword">ON</span> hive<span class="built_in">.</span><span class="subst">*</span> <span class="keyword">TO</span> <span class="string">'hive'</span>@<span class="string">'localhost'</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">'hive'</span>;
mysql<span class="subst">&gt;</span> GRANT <span class="literal">ALL</span>  PRIVILEGES <span class="keyword">ON</span> hive<span class="built_in">.</span><span class="subst">*</span> <span class="keyword">TO</span> <span class="string">'hive'</span>@<span class="string">'127.0.0.1'</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">'hive'</span>;
mysql<span class="subst">&gt;</span> GRANT <span class="literal">ALL</span>  PRIVILEGES <span class="keyword">ON</span> hived<span class="built_in">.</span><span class="subst">*</span> <span class="keyword">TO</span> <span class="string">'hive'</span>@<span class="string">'192.168.255.151'</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">'hive'</span>;
mysql<span class="subst">&gt;</span>flush privileges;
</code></pre><h5 id="查看hive用户是否已创建成功">查看hive用户是否已创建成功</h5>
<pre><code>mysql&gt; use mysql;
Database changed
<span class="header">mysql&gt;  select Host,User,Password from user;
+-----------+------+-------------------------------------------+</span>
<span class="header">| Host      | User | Password                                  |
+-----------+------+-------------------------------------------+</span>
| %         | root | <span class="strong">*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |
| master    | root |                                           |
| 127.0.0.1 | root |                                           |
| ::1       | root |                                           |
| localhost | hive | *</span>4DF1D66463C18D44E3B001A8FB1BBFBEA13E27FC |
<span class="code">+-----------+</span>------<span class="code">+-------------------------------------------+</span>
5 rows in set (0.00 sec)
</code></pre><h5 id="配置hadoop的hadoop-env-sh文件">配置hadoop的hadoop-env.sh文件</h5>
<p>增加$HADOOP_CLASSPATH,要不然启动hive时会出错</p>
<pre><code>export <span class="constant">HADOOP_CLASSPATH</span>=<span class="variable">$HADOOP_CLASSPATH</span><span class="symbol">:/home/grid/hadoop-</span><span class="number">1.2</span>.<span class="number">1</span>/myclass
</code></pre><p>Hive调试技巧,打开调试可以看到详细错误信息</p>
<pre><code>./hive -hiveconf hive<span class="preprocessor">.root</span><span class="preprocessor">.logger</span>=DEBUG,console  
</code></pre><p>Hive简单操作</p>
<pre><code>[grid@hadoop01 apache-hive-<span class="number">0.13</span><span class="number">.0</span>-bin]$ bin/hive
<span class="number">14</span>/<span class="number">04</span>/<span class="number">24</span> <span class="number">16</span>:<span class="number">24</span>:<span class="number">23</span> WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  <span class="keyword">Use</span> hive.hmshandler.retry.* instead
Logging initialized using <span class="keyword">configuration</span> <span class="keyword">in</span> jar:<span class="keyword">file</span>:/home/grid/apache-hive-<span class="number">0.13</span><span class="number">.0</span>-bin/lib/hive-common-<span class="number">0.13</span><span class="number">.0</span>.jar!/hive-log4j.properties
hive&gt; show tables;
OK
<span class="typename">Time</span> taken: <span class="number">2.062</span> seconds
hive&gt; create table hivetest(id int,name <span class="typename">string</span>);
OK
<span class="typename">Time</span> taken: <span class="number">0.962</span> seconds
hive&gt; show tables;
OK
hivetest
<span class="typename">Time</span> taken: <span class="number">0.084</span> seconds, Fetched: <span class="number">1</span> row(s)
hive&gt; <span class="keyword">select</span> count(*) from hivetest;
Total jobs = <span class="number">1</span>
Launching Job <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">1</span>
Number <span class="keyword">of</span> reduce tasks determined at compile <span class="typename">time</span>: <span class="number">1</span>
<span class="keyword">In</span> order <span class="keyword">to</span> change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
<span class="keyword">In</span> order <span class="keyword">to</span> limit the maximum number <span class="keyword">of</span> reducers:
  set hive.exec.reducers.max=&lt;number&gt;
<span class="keyword">In</span> order <span class="keyword">to</span> set a <span class="keyword">constant</span> number <span class="keyword">of</span> reducers:
  set mapred.reduce.tasks=&lt;number&gt;
Starting Job = job_201404241623_0001, Tracking URL = http://hadoop01.myhadoop.com:<span class="number">50030</span>/jobdetails.jsp?jobid=job_201404241623_0001
Kill Command = /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/libexec/../bin/hadoop job  -kill job_201404241623_0001
Hadoop job information <span class="keyword">for</span> Stage-<span class="number">1</span>: number <span class="keyword">of</span> mappers: <span class="number">0</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">24</span> <span class="number">16</span>:<span class="number">28</span>:<span class="number">53</span>,<span class="number">414</span> Stage-<span class="number">1</span> <span class="keyword">map</span> = <span class="number">0</span>%,  reduce = <span class="number">0</span>%
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">24</span> <span class="number">16</span>:<span class="number">28</span>:<span class="number">59</span>,<span class="number">466</span> Stage-<span class="number">1</span> <span class="keyword">map</span> = <span class="number">0</span>%,  reduce = <span class="number">100</span>%, Cumulative CPU <span class="number">1.27</span> sec
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">24</span> <span class="number">16</span>:<span class="number">29</span>:<span class="number">04</span>,<span class="number">511</span> Stage-<span class="number">1</span> <span class="keyword">map</span> = <span class="number">100</span>%,  reduce = <span class="number">100</span>%, Cumulative CPU <span class="number">1.27</span> sec
MapReduce Total cumulative CPU <span class="typename">time</span>: <span class="number">1</span> seconds <span class="number">270</span> msec
Ended Job = job_201404241623_0001
MapReduce Jobs Launched: 
Job <span class="number">0</span>: Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">1.27</span> sec   HDFS Read: <span class="number">0</span> HDFS Write: <span class="number">2</span> SUCCESS
Total MapReduce CPU <span class="typename">Time</span> Spent: <span class="number">1</span> seconds <span class="number">270</span> msec
OK
<span class="number">0</span>
<span class="typename">Time</span> taken: <span class="number">29.117</span> seconds, Fetched: <span class="number">1</span> row(s)
hive&gt; desc hivetest;
OK
id                      int                                         
name                    <span class="typename">string</span>                                      
<span class="typename">Time</span> taken: <span class="number">0.376</span> seconds, Fetched: <span class="number">2</span> row(s)
hive&gt; 
</code></pre><p>以hive用户进入mysql    </p>
<pre><code>mysql &gt; <span class="keyword">select</span> * <span class="keyword">from</span> tbls 查看元数据
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hive/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive 体系结构入门]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/24/Hadoop/hadoop_hive/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/24/Hadoop/hadoop_hive/</id>
    <published>2014-04-24T10:49:01.000Z</published>
    <updated>2014-04-24T11:38:28.000Z</updated>
    <content type="html"><![CDATA[<h5 id="第一部分：概念">第一部分：概念</h5>
<p>基本概念<br>•用户接口：用户访问Hive的入口<br>•元数据：Hive的用户信息与表的MetaData<br>•解释器：分析翻译HQL的组件<br>•编译器：编译HQL的组件<br>•优化器：优化HQL的组件</p>
<h5 id="第二部分：Hive架构与基本组成">第二部分：Hive架构与基本组成</h5>
<p>架构图<br><img src="/img/Hadoop/Hive/1.png" alt="图1"></p>
<p>基本组成<br>•用户接口，包括 CLI，JDBC/ODBC，WebUI<br>•元数据存储，通常是存储在关系数据库如 mysql, derby 中<br>•解释器、编译器、优化器、执行器<br>•Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算</p>
<p>各组件的基本功能<br>•用户接口主要有三个：CLI，JDBC/ODBC和 WebUI<br>•CLI，即Shell命令行<br>•JDBC/ODBC 是 Hive 的JAVA，与使用传统数据库JDBC的方式类似<br>•WebGUI是通过浏览器访问 Hive<br>•Hive 将元数据存储在数据库中，目前只支持 mysql、derby，下一版本会支持更多的数据库。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等<br>•解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行<br>•Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 星号的查询，比如 select 星号 from table 不会生成 MapRedcue 任务）<br>Metastore<br>•Metastore是系统目录(catalog)用于保存Hive中所存储的表的元数据（metadata）信息<br>•Metastore是Hive被用作传统数据库解决方案（如oracle和db2）时区别其它类似系统的一个特征<br>•Metastore包含如下的部分：<br>•Database 是表（table）的名字空间。默认的数据库（database）名为‘default’<br>•Table 表（table）的原数据包含信息有：列（list of columns）和它们的类型（types），拥有者（owner），存储空间（storage）和SerDei信息<br>•Partition 每个分区（partition）都有自己的列（columns），SerDe和存储空间（storage）。这一特征将被用来支持Hive中的模式演变（schema evolution）<br>Compiler<br>•Driver调用编译器（compiler）处理HiveQL字串，这些字串可能是一条DDL、DML或查询语句<br>•编译器将字符串转化为策略（plan）<br>•策略仅由元数据操作和HDFS操作组成，元数据操作只包含DDL语句，HDFS操作只包含LOAD语句<br>•对插入和查询而言，策略由map-reduce任务中的具有方向的非循环图（directedacyclic graph，DAG）组成</p>
<h5 id="第三部分：Hive运行模式">第三部分：Hive运行模式</h5>
<p>Hive运行模式<br>•Hive的运行模式即任务的执行环境<br>•分为本地与集群两种<br>•我们可以通过mapred.job.tracker 来指明<br>•设置方式<br>•hive &gt; SET  mapred.job.tracker=local</p>
<h5 id="第四部分：数据类型">第四部分：数据类型</h5>
<p>原始数据类型<br>•Integers<br>TINYINT - 1 byte<br>SMALLINT - 2 byte<br>INT - 4 byte<br>BIGINT - 8 byte<br>•Boolean type<br>BOOLEAN - TRUE/FALSE<br>•Floating point numbers<br>FLOAT –单精度<br>DOUBLE – 双精度<br>•String type<br>STRING - sequence of characters in a specified character set</p>
<p>复杂数据类型<br>•Structs: 例子  {c INT; d INT}<br>•Maps (key-value tuples):. 例子’group’ -&gt; gid  M[‘group’]<br>•Arrays (indexable lists):  例子[‘1’, ‘2’, ‘3’]<br>•TIMESTAMP  0.8版本新加属性</p>
<h5 id="第五部分：Hive的元数据存储">第五部分：Hive的元数据存储</h5>
<p>存储方式与模式<br>•Hive 将元数据存储在 数据库中<br>•连接到数据库模式有三种<br>•单用户模式<br><img src="/img/Hadoop/Hive/2.png" alt="图2"><br>•多用户模式<br><img src="/img/Hadoop/Hive/3.png" alt="图3"><br>•远程服务器模式<br><img src="/img/Hadoop/Hive/4.png" alt="图4"></p>
<p><p align=left>单用户模式</p>
<p>此模式连接到一个 In-memory 的数据库 Derby ，一般用于 Unit Test</p>
<p>多用户模式</p>
<p>通过网络连接到一个数据库中，是最经常使用到的模式</p>
<p>远程服务器模式</p>
<p>用于非 Java 客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用 Thrift 协议通过MetaStoreServer 访问元数据库</p>
<h5 id="第六部分：Hive的数据存储">第六部分：Hive的数据存储</h5>
<p>Hive数据存储的基本概念<br>•Hive的数据存储是建立在Hadoop HDFS之上的<br>•Hive没有专门的数据存储格式<br>•存储结构主要包括：数据库、文件、表、视图<br>•Hive默认可以直接加载文本文件，还支持sequence file 、RCFile<br>•创建表时，我们直接告诉Hive数据的列分隔符与行分隔符，Hive即可解析数据</p>
<p>Hive的数据模型-数据库<br>•类似传统数据库的DataBase<br>•在第三方数据库里实际是一张表<br>•简单示例<br>•命令行hive &gt; create database test_database;</p>
<p>Hive的数据模型-表<br>•Table 内部表<br>•Partition  分区表<br>•External Table 外部表<br>•Bucket  Table </p>
<p>内部表<br>•与数据库中的 Table 在概念上是类似<br>•每一个 Table 在 Hive 中都有一个相应的目录存储数据<br>•例如，一个表 test，它在 HDFS 中的路径为：/ warehouse /test<br>• warehouse是在 hive-site.xml 中由 ${hive.metastore.warehouse.dir} 指定的数据仓库的目录<br>•所有的 Table 数据（不包括 External Table）都保存在这个目录中。<br>•删除表时，元数据与数据都会被删除</p>
<p>内部表简单示例<br>•创建数据文件test_inner_table.txt<br>•创建表<br>•create table test_inner_table (key string)<br>•加载数据<br>•LOAD DATA LOCAL INPATH ‘filepath’ INTO TABLE test_inner_table<br>•查看数据<br>•select 星号 from test_inner_table<br>•select count(星号) from test_inner_table<br>•删除表 drop table test_inner_table</p>
<p>分区表<br>•Partition 对应于数据库中的 Partition 列的密集索引<br>•在 Hive 中，表中的一个 Partition 对应于表下的一个目录，所有的 Partition 的数据都存储在对应的目录中<br>•例如：test表中包含 date 和 position 两个 Partition，则对应于 date = 20120801, position = zh 的 HDFS 子目录为：/ warehouse /test/date=20120801/ position =zh<br>•对应于  = 20100801, position = US 的HDFS 子目录为；/ warehouse /xiaojun/date=20120801/ position =US</p>
<p>分区表简单示例<br>•创建数据文件test_partition_table.txt<br>•创建表<br>•create table test_partition_table (key string) partitioned by (dt string)<br>•加载数据<br>•LOAD DATA INPATH ‘filepath’ INTO TABLE test_partition_table partition (dt=‘2006’)<br>•查看数据<br>•select 星号 from test_partition_table<br>•select count(星号) from test_partition_table<br>•删除表 drop table test_partition_table</p>
<p>外部表<br>•指向已经在 HDFS 中存在的数据，可以创建 Partition<br>•它和 内部表 在元数据的组织上是相同的，而实际数据的存储则有较大的差异<br>•内部表 的创建过程和数据加载过程（这两个过程可以在同一个语句中完成），在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除<br>• 外部表 只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接</p>
<p>外部表简单示例<br>•创建数据文件test_external_table.txt<br>•创建表<br>•create external table test_external_table (key string)<br>•加载数据<br>•LOAD DATA INPATH ‘filepath’ INTO TABLE test_inner_table<br>•查看数据<br>•select 星号 from test_external_table<br>•select count(星号) from test_external_table<br>•删除表 drop table test_external_table</p>
<p>Bucket Table<br>•可以将表的列通过Hash算法进一步分解成不同的文件存储<br>•例如：将age列分散成20个文件，首先要对AGE进行Hash计算，对应为0的写入/warehouse/test/date=20120801/postion=zh/part-00000,对应为1的写入/warehouse/test/date=20120801/postion=zh/part-00001<br>•如果想应用很多的Map任务这样是不错的选择<br><img src="/img/Hadoop/Hive/5.png" alt="图5"></p>
<p>Bucket Table简单示例<br>•创建数据文件test_bucket_table.txt<br>•创建表<br>•create table test_bucket_table (key string)<br>     clustered by (key) into 20 buckets<br>•加载数据<br>•LOAD DATA INPATH ‘filepath’ INTO TABLE test_bucket_table<br>•查看数据<br>•select 星号 from test_bucket_table<br>•set hive.enforce.bucketing = true;</p>
<p>Hive的数据模型-视图<br>•视图与传统数据库的视图类似<br>•视图是只读的<br>•视图基于的基本表，如果改变，指增加不会影响视图的呈现；如果删除，会出现问题<br>•如果不指定视图的列，会根据select语句后的生成<br>•示例<br>•create view test_view as select * from test</p>
<h5 id="第七部分：HiveUI介绍">第七部分：HiveUI介绍</h5>
<p>启动UI<br>•配置<br>•hive-site.xml 添加<br>  <property><br>       <name>hive.hwi.war.file</name><br>       <value>lib/hive-hwi-0.8.1.war</value><br>  </property><br>•启动Hive的UI sh $HIVE_HOME/bin/hive —service hwi</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hive/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Pig的简单使用]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/23/Hadoop/hadoop_pig/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/23/Hadoop/hadoop_pig/</id>
    <published>2014-04-23T10:53:16.000Z</published>
    <updated>2014-04-23T11:38:12.000Z</updated>
    <content type="html"><![CDATA[<h5 id="安装配置pig">安装配置pig</h5>
<p>下载你对应hadoop的pig版本,解压</p>
<h5 id="本地模式">本地模式</h5>
<pre><code>配置环境变量
vi <span class="preprocessor">.bash</span>_profile
PATH=$PATH:/home/grid/pig-<span class="number">0.12</span><span class="number">.1</span>/bin:$HOME/bin
JAVA_HOME=/usr/jdk1<span class="number">.7</span><span class="number">.0</span>_51

export PATH
export JAVA_HOME
[grid@hadoop01 ~]$ pig -<span class="built_in">x</span> local
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">34</span>,<span class="number">815</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Apache Pig version <span class="number">0.12</span><span class="number">.1</span> (r1585011) compiled Apr <span class="number">05</span> <span class="number">2014</span>, <span class="number">01</span>:<span class="number">41</span>:<span class="number">34</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">34</span>,<span class="number">815</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Logging error messages to: /home/grid/pig_1398238654807<span class="preprocessor">.log</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">040</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.impl</span><span class="preprocessor">.util</span><span class="preprocessor">.Utils</span> - Default bootup file /home/grid/<span class="preprocessor">.pigbootup</span> not found
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">843</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.backend</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.executionengine</span><span class="preprocessor">.HExecutionEngine</span> - Connecting to hadoop file system at: file:///
grunt&gt; 
</code></pre><h5 id="mapreduce模式">mapreduce模式</h5>
<pre><code>配置环境变量
PATH=$PATH:/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>:/bin:/home/grid/pig-<span class="number">0.12</span><span class="number">.1</span>/bin:$HOME/bin
JAVA_HOME=/usr/jdk1<span class="number">.7</span><span class="number">.0</span>_51
PIG_CLASSPATH=/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/conf/

export PIG_CLASSPATH
export PATH
export JAVA_HOME
[grid@hadoop01 ~]$ cat /etc/hosts
<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>   localhost localhost<span class="preprocessor">.localdomain</span> localhost4 localhost4<span class="preprocessor">.localdomain</span>4
::<span class="number">1</span>         localhost localhost<span class="preprocessor">.localdomain</span> localhost6 localhost6<span class="preprocessor">.localdomain</span>6
<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span> hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">192.168</span><span class="number">.255</span><span class="number">.152</span> hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">192.168</span><span class="number">.255</span><span class="number">.153</span> hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>

[grid@hadoop01 ~]$ pig
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">06</span>,<span class="number">090</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Apache Pig version <span class="number">0.12</span><span class="number">.1</span> (r1585011) compiled Apr <span class="number">05</span> <span class="number">2014</span>, <span class="number">01</span>:<span class="number">41</span>:<span class="number">34</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">06</span>,<span class="number">090</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Logging error messages to: /home/grid/pig_1398239586082<span class="preprocessor">.log</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">06</span>,<span class="number">171</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.impl</span><span class="preprocessor">.util</span><span class="preprocessor">.Utils</span> - Default bootup file /home/grid/<span class="preprocessor">.pigbootup</span> not found
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">07</span>,<span class="number">199</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.backend</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.executionengine</span><span class="preprocessor">.HExecutionEngine</span> - Connecting to hadoop file system at: hdfs://hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">08</span>,<span class="number">712</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.backend</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.executionengine</span><span class="preprocessor">.HExecutionEngine</span> - Connecting to map-reduce job tracker at: hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9001</span>
</code></pre><h5 id="简单操作，可以help查看">简单操作，可以help查看</h5>
<pre><code>grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/demo<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;       <span class="number">29</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>  &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">out</span> &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out2        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out3        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out4        &lt;dir&gt;
grunt&gt; cd <span class="keyword">in</span>
grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;      <span class="number">39564893</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>/test1<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;   <span class="number">12</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>/test2<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;   <span class="number">13</span>
grunt&gt; cat test1<span class="preprocessor">.txt</span>
hello world
</code></pre><h5 id="做两个实验">做两个实验</h5>
<h6 id="实验一：准备数据">实验一：准备数据</h6>
<pre><code>[grid@hadoop01 ~]$ cat score<span class="preprocessor">.txt</span> 
James,Network,Tiger,<span class="number">100</span>
James,Database,Tiger,<span class="number">99</span>
James,PDE,Yao,<span class="number">95</span>
Vincent,Network,Tiger,<span class="number">95</span>
Vincent,PDE,Yao,<span class="number">98</span>
Vincent,PDE,
NocWei,PDE,Yao,<span class="number">100</span>
grunt&gt; copyFromLocal  /home/grid/score<span class="preprocessor">.txt</span> pig/score<span class="preprocessor">.txt</span>
grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/demo<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;       <span class="number">29</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>  &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">out</span> &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out2        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out3        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out4        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/pig &lt;dir&gt;
grunt&gt; cd pig
grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/pig/score<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;  <span class="number">141</span>
grunt&gt; cat score<span class="preprocessor">.txt</span>
James,Network,Tiger,<span class="number">100</span>
James,Database,Tiger,<span class="number">99</span>
James,PDE,Yao,<span class="number">95</span>
Vincent,Network,Tiger,<span class="number">95</span>
Vincent,PDE,Yao,<span class="number">98</span>
Vincent,PDE,
NocWei,PDE,Yao,<span class="number">100</span>
</code></pre><h6 id="任务目标：计算一名学生被多少老师教过(这里用到了Pig_Latin)">任务目标：计算一名学生被多少老师教过(这里用到了Pig Latin)</h6>
<p>先DISTINCT,再计数。-使用DISTINCT 能够对所有数据去重</p>
<pre><code>grunt&gt; A = LOAD 'score.txt' USING PigStorage(',') AS (student,course,teacher,score:int);
grunt&gt; DESCRIBE A;
A: {student: bytearray,course: bytearray,teacher: bytearray,score: int}
grunt&gt; B = FOREACH A GENERATE student,teacher;
grunt&gt; DESCRIBE B;
B: {student: bytearray,teacher: bytearray}
grunt&gt; C = DISTINCT B;
grunt&gt; D = GROUP C BY student
grunt&gt; D = FOREACH (GROUP C BY student ) GENERATE group AS student,COUNT(C);
grunt&gt; DUMP D;
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">076</span> [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used <span class="keyword">in</span> the script: GROUP_BY,DISTINCT
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">241</span> [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier]}
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">326</span> [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned <span class="keyword">for</span> A: $<span class="number">1</span>, $<span class="number">3</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">760</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: <span class="number">100</span> optimistic? <span class="keyword">false</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">866</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer - Choosing <span class="keyword">to</span> move algebraic foreach <span class="keyword">to</span> combiner
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">011</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: <span class="number">2</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">018</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: <span class="number">2</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">388</span> [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added <span class="keyword">to</span> the job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">453</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set <span class="keyword">to</span> <span class="keyword">default</span> <span class="number">0.3</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">463</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # <span class="keyword">of</span> required reducers.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">464</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">528</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=<span class="number">1000000000</span> maxReducers=<span class="number">999</span> totalInputFileSize=<span class="number">141</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">535</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism <span class="keyword">to</span> <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">536</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job8437033109018168730.jar
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">11</span>,<span class="number">994</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job8437033109018168730.jar created
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">071</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">106</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is <span class="keyword">false</span>, will not generate code.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">106</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process <span class="keyword">to</span> move generated code <span class="keyword">to</span> distributed cache
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">121</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] <span class="keyword">with</span> classes <span class="keyword">to</span> deserialize []
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">126</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting identity combiner <span class="keyword">class</span>.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">386</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">1</span> map-reduce job(s) waiting <span class="keyword">for</span> submission.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">892</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">0</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">591</span> [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">592</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">639</span> [JobControl] INFO  org.apache.hadoop.util.NativeCodeLoader - Loaded the native-hadoop library
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">639</span> [JobControl] WARN  org.apache.hadoop.io.compress.snappy.LoadSnappy - Snappy native library not loaded
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">645</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201404231546_0001
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases A,B
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: A[<span class="number">1</span>,<span class="number">4</span>],B[-<span class="number">1</span>,-<span class="number">1</span>] C:  R: 
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http:<span class="comment">//hadoop01.myhadoop.com:50030/jobdetails.jsp?jobid=job_201404231546_0001</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">32</span>,<span class="number">455</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">25</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">45</span>,<span class="number">009</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">50</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">152</span> [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added <span class="keyword">to</span> the job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">154</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set <span class="keyword">to</span> <span class="keyword">default</span> <span class="number">0.3</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">155</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # <span class="keyword">of</span> required reducers.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">155</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">378</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=<span class="number">1000000000</span> maxReducers=<span class="number">999</span> totalInputFileSize=<span class="number">103</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">378</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism <span class="keyword">to</span> <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">379</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job4848190506187042517.jar
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">009</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job4848190506187042517.jar created
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">047</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">052</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is <span class="keyword">false</span>, will not generate code.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">052</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process <span class="keyword">to</span> move generated code <span class="keyword">to</span> distributed cache
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">052</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] <span class="keyword">with</span> classes <span class="keyword">to</span> deserialize []
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">182</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">1</span> map-reduce job(s) waiting <span class="keyword">for</span> submission.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">663</span> [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">663</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">671</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201404231546_0002
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases <span class="number">1</span>-<span class="number">17</span>,D
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: D[<span class="number">5</span>,<span class="number">4</span>],<span class="number">1</span>-<span class="number">17</span>[<span class="number">5</span>,<span class="number">13</span>] C: D[<span class="number">5</span>,<span class="number">4</span>],<span class="number">1</span>-<span class="number">17</span>[<span class="number">5</span>,<span class="number">13</span>] R: D[<span class="number">5</span>,<span class="number">4</span>]
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http:<span class="comment">//hadoop01.myhadoop.com:50030/jobdetails.jsp?jobid=job_201404231546_0002</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">17</span>,<span class="number">750</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">75</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">27</span>,<span class="number">518</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">83</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">568</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">100</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">591</span> [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: 

HadoopVersion   PigVersion      UserId  StartedAt       FinishedAt      Features
<span class="number">1.2</span><span class="number">.1</span>   <span class="number">0.12</span><span class="number">.1</span>  grid    <span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>     <span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>     GROUP_BY,DISTINCT

Success!

Job Stats (time <span class="keyword">in</span> seconds):
JobId   Maps    Reduces MaxMapTime      MinMapTIme      AvgMapTime      MedianMapTime   MaxReduceTime   MinReduceTime        AvgReduceTime   MedianReducetime        Alias   Feature Outputs
job_201404231546_0001   <span class="number">1</span>       <span class="number">1</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      A,B DISTINCT
job_201404231546_0002   <span class="number">1</span>       <span class="number">1</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      <span class="number">1</span>-<span class="number">17</span>,D       GROUP_BY,COMBINER       hdfs:<span class="comment">//hadoop01.myhadoop.com:9000/tmp/temp1433820585/tmp1152433391,</span>

Input(s):
Successfully read <span class="number">7</span> records (<span class="number">517</span> bytes) from: <span class="string">"hdfs://hadoop01.myhadoop.com:9000/user/grid/pig/score.txt"</span>

Output(s):
Successfully stored <span class="number">3</span> records (<span class="number">41</span> bytes) <span class="keyword">in</span>: <span class="string">"hdfs://hadoop01.myhadoop.com:9000/tmp/temp1433820585/tmp1152433391"</span>

Counters:
Total records written : <span class="number">3</span>
Total bytes written : <span class="number">41</span>
Spillable Memory Manager spill count : <span class="number">0</span>
Total bags proactively spilled: <span class="number">0</span>
Total records proactively spilled: <span class="number">0</span>

Job DAG:
job_201404231546_0001   -&gt;      job_201404231546_0002,
job_201404231546_0002


<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">627</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">641</span> [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">654</span> [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">655</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
(James,<span class="number">2</span>)
(NocWei,<span class="number">1</span>)
(Vincent,<span class="number">3</span>）
</code></pre><p>最后几行展现了结果</p>
<h6 id="实验二：请使用Pig_latin语言处理access_log-txt日志，计算出每个IP的点击数">实验二：请使用Pig latin语言处理access_log.txt日志，计算出每个IP的点击数</h6>
<p>这个实验是采用了大牛刘盛的BLOG <a href="http://blog.itpub.net/26686207" target="_blank">http://blog.itpub.net/26686207</a><br>做实验的时候我没有做操作日志，下面是他的操作日志</p>
<pre><code>grunt&gt; cat <span class="filename">access_log.txt 我们来看一下文件的内容之后进行数据分析
119.146.220.12 - - [31/Jan/2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">51</span> +<span class="number">0800</span>] <span class="string">"GET /static/js/jquery-1.6.js HTTP/1.1"</span> <span class="number">404</span> <span class="number">299</span> <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">52</span> +<span class="number">0800</span>] <span class="string">"GET /static/js/floating-jf.js HTTP/1.1"</span> <span class="number">404</span> <span class="number">300</span> <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">55</span> +<span class="number">0800</span>] <span class="string">"GET /popwin_js.php?fid=53 HTTP/1.1"</span> <span class="number">404</span> <span class="number">289</span> <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">55</span> +<span class="number">0800</span>] <span class="string">"GET /static/js/smilies.js?AZH HTTP/1.1"</span> <span class="number">304</span> - <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">55</span> +<span class="number">0800</span>] <span class="string">"GET /data/cache/common_smilies_var.js?AZH HTTP/1.1"</span> <span class="number">304</span> - <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
</code></pre><p>数据算法：这是一部分dataguru上网日志，从日志内容结构看，ip地址是放在前面的，我们只要抽取出ip地址写入一张ip_text表，然后对ip列进行分组相当于分成若干个小表，每个ip集合为一个小表，再单独算出每个小表总行数即ip点击次数</p>
<p>（1）加载HDFS文件系统中access_log.txt文件内容放到pig的一个关系(表)里</p>
<pre><code>使用空格作为分隔符，只加载ip列即可。
grunt&gt; ip_text = LOAD <span class="string">'pig/access_log.txt'</span> <span class="keyword">USING</span> PigStorage(<span class="string">' '</span>) <span class="keyword">AS</span> (ip:chararray);
ip_text：代表一个关系，一个表，一个变量，这个表中存放了所有ip记录
LOAD <span class="string">'pig/access_log.txt'</span>：要加载的文件
<span class="keyword">USING</span> PigStorage(<span class="string">' '</span>)：使用空格作为分隔符
ip:chararray：表中第一列名ip，数据类型chararray字符型
</code></pre><p>（2）查看ip_text表结构与内容</p>
<pre><code>一定要仔细，例如命令结尾符不要丢掉，当我们执行一条<span class="tag">pig</span> <span class="tag">latin</span>语句时，
<span class="tag">pig</span>自动转换成<span class="tag">MapReduce</span>作业对用户来说是透明的，先创建一个<span class="tag">jar</span>包，再提交<span class="tag">MR</span> <span class="tag">job</span>，
生成<span class="tag">Hadoop</span> <span class="tag">job</span> <span class="tag">id</span>在执行，最后显示结果
<span class="tag">grunt</span>&gt; <span class="tag">DESCRIBE</span> <span class="tag">ip_text</span>; 显示表的结构，只有一列，类型为字符型
<span class="tag">ip_text</span>: <span class="rules">{<span class="rule"><span class="attribute">ip</span>:<span class="value"> chararray</span></span></span>}
<span class="tag">grunt</span>&gt; <span class="tag">DUMP</span> <span class="tag">ip_text</span>; 显示表的内容，只截取部分内容
<span class="tag">creating</span> <span class="tag">jar</span> <span class="tag">file</span> <span class="tag">Job2594979755419279957</span><span class="class">.jar</span>
1 <span class="tag">map-reduce</span> <span class="tag">job</span>(<span class="tag">s</span>) <span class="tag">waiting</span> <span class="tag">for</span> <span class="tag">submission</span>
<span class="tag">HadoopJobId</span>: <span class="tag">job_201210121146_0002</span>
(119<span class="class">.146</span><span class="class">.220</span><span class="class">.12</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.41</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.44</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.44</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.44</span>)
(221<span class="class">.194</span><span class="class">.180</span><span class="class">.166</span>)
(119<span class="class">.146</span><span class="class">.220</span><span class="class">.12</span>)
....
</code></pre><p>（3）对ip列进行分组，并查看分组后表的内容和结构，注意关键字大小写</p>
<p>把每个ip集合分成一个个小表，把分组后的结果存放在 group_ip 这个表中</p>
<pre><code>grunt<span class="subst">&gt;</span> group_ip <span class="subst">=</span> <span class="keyword">GROUP</span> ip_text <span class="keyword">BY</span> ip; 按照ip进行分组赋给group_ip表
grunt<span class="subst">&gt;</span> DESCRIBE group_ip; 查看group_ip表结构
group_ip: {<span class="keyword">group</span>: chararray,ip_text: {(ip: chararray)}}
我们一眼就看出group_ip表是一个嵌套表，第一个field是<span class="keyword">group</span>，这就是分组后的ip值
第二个field是一个嵌套的小表又叫包，是前面分组ip的整个集合
grunt<span class="subst">&gt;</span> DUMP group_ip; 又提交一个MR job运行
Pig script settings are added <span class="keyword">to</span> the job Pig脚本自动转换MR job
creating jar file Job2785495206577164389<span class="built_in">.</span>jar 创建jar包
jar file Job2785495206577164389<span class="built_in">.</span>jar created jar包创建完毕
<span class="built_in">map</span><span class="attribute">-reduce</span> job(s) waiting for submission<span class="built_in">.</span> 提交job
HadoopJobId: job_201210121146_0003 job id：job_201210121146_0003
(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="built_in">.</span>
<span class="number">180.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>
<span class="attribute">...</span><span class="built_in">.</span>
</code></pre><p>（4）统计每个小表总行数即ip点击次数</p>
<pre><code>grunt&gt; count_ip = FOREACH group_ip GENERATE <span class="keyword">group</span>,COUNT($<span class="number">1</span>) <span class="keyword">AS</span> count_ip;
FOREACH group_ip：逐行扫描group_ip表，赋给count_ip表
GENERATE <span class="keyword">group</span>：读取分组ip值
COUNT($<span class="number">1</span>) <span class="keyword">AS</span> count_ip：统计嵌套小表(包)总行数即ip点击次数，把此列取别名叫count_ip方便倒序排列，$<span class="number">1</span>统计第一列，等价于COUNT(ip_text.ip)
grunt&gt; sort_count_ip = <span class="keyword">ORDER</span> count_ip <span class="keyword">BY</span> count_ip <span class="keyword">DESC</span>; 按照count_ip列从大到小排序
# grunt&gt; sort_count_ip = <span class="keyword">ORDER</span> count_ip <span class="keyword">BY</span> count_ip <span class="keyword">ASC</span>; 从小到大排序
</code></pre><p>（5）查看sort_count_ip表结构和内容</p>
<pre><code>grunt&gt; DESCRIBE sort_count_ip; 显示表的结构，有二列
sort_count_ip: {group: chararray,count_ip: long} 第一个field是group字符型（分组ip值），第二个field是count_ip长类型（ip点击次数）
grunt&gt; DUMP sort_count_ip; 显示表的内容，只截取部分结果，先输出统计信息后显示结果
HadoopVersion PigVersion UserId StartedAt FinishedAt Features
<span class="number">0.20</span><span class="number">.2</span> <span class="number">0.9</span><span class="number">.2</span> grid <span class="number">2012</span>-<span class="number">11</span>-<span class="number">03</span> <span class="number">21</span>:<span class="number">13</span>:<span class="number">05</span> <span class="number">2012</span>-<span class="number">11</span>-<span class="number">03</span> <span class="number">21</span>:<span class="number">18</span>:<span class="number">39</span> GROUP_BY,ORDER_BY
Success!
Input(s):
Successfully read <span class="number">28134</span> records (<span class="number">7118627</span> bytes) from:
<span class="keyword">...</span>
Job DAG:
job_201210121146_0004 -&gt; job_201210121146_0005,
job_201210121146_0005 -&gt; job_201210121146_0006,
job_201210121146_0006
(<span class="number">218.20</span><span class="number">.24</span><span class="number">.203</span>,<span class="number">4597</span>)
(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>,<span class="number">4576</span>)
(<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span>,<span class="number">1850</span>)
(<span class="number">117.136</span><span class="number">.31</span><span class="number">.144</span>,<span class="number">1647</span>)
(<span class="number">121.28</span><span class="number">.95</span><span class="number">.48</span>,<span class="number">1597</span>)
<span class="keyword">...</span>
</code></pre><p>（6）把sort_count_ip表内容写入HDFS文件系统中，即固化到硬盘存入文件</p>
<pre><code>grunt<span class="subst">&gt;</span> STORE sort_count_ip <span class="keyword">INTO</span> <span class="string">'pig/sort_count_ip'</span>;
Counters:
Total <span class="keyword">records</span> written : <span class="number">476</span>
Total <span class="built_in">bytes</span> written : <span class="number">8051</span>
Spillable Memory Manager spill count : <span class="number">0</span>
Total bags proactively spilled: <span class="number">0</span>
Total <span class="keyword">records</span> proactively spilled: <span class="number">0</span>
Job DAG:
job_201210121146_0007 <span class="subst">-&gt; </span>job_201210121146_0008,
job_201210121146_0008 <span class="subst">-&gt; </span>job_201210121146_0009,
job_201210121146_0009
<span class="number">2012</span><span class="subst">-</span><span class="number">11</span><span class="subst">-</span><span class="number">03</span> <span class="number">21</span>:<span class="number">28</span>:<span class="number">41</span>,<span class="number">520</span> <span class="preprocessor">[</span>main<span class="preprocessor">]</span><span class="markup"> INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!</span>
</code></pre><p>当我们看到Success时就说明我们已经保存成功！</p>
<p>（7）查看保存在HDFS中的结果文件</p>
<pre><code>grunt&gt; <span class="built_in">cat</span> sort_count_ip
<span class="number">218.20</span><span class="number">.24</span><span class="number">.203</span> <span class="number">4597</span>
<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span> <span class="number">4576</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> <span class="number">1850</span>
<span class="number">117.136</span><span class="number">.31</span><span class="number">.144</span> <span class="number">1647</span>
<span class="number">121.28</span><span class="number">.95</span><span class="number">.48</span> <span class="number">1597</span>
<span class="number">113.109</span><span class="number">.183</span><span class="number">.126</span> <span class="number">1596</span>
<span class="number">182.48</span><span class="number">.112</span><span class="number">.2</span> <span class="number">870</span>
<span class="number">120.84</span><span class="number">.24</span><span class="number">.200</span> <span class="number">773</span>
<span class="number">61.144</span><span class="number">.125</span><span class="number">.162</span> <span class="number">750</span>
<span class="number">27.115</span><span class="number">.124</span><span class="number">.75</span> <span class="number">470</span>
<span class="number">115.236</span><span class="number">.48</span><span class="number">.226</span> <span class="number">439</span>
</code></pre><p>OK！！！</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Pig" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Pig/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[eclipse连接hadoop1.2.1集群]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/22/Hadoop/hadoop-eclispe/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/22/Hadoop/hadoop-eclispe/</id>
    <published>2014-04-22T12:43:01.000Z</published>
    <updated>2014-04-22T13:41:56.000Z</updated>
    <content type="html"><![CDATA[<h5 id="下载安装eclipse">下载安装eclipse</h5>
<p>去官网下载eclipse，我下载的是eclipse-standard-kepler-SR2-linux-gtk-x86_64.tar.gz<br>移到目录/opt下,解压tar -zxvf eclipse-standard-kepler-SR2-linux-gtk-x86_64.tar.gz 进入/opt/eclipse/目录，双击eclipse,eclipse启动了</p>
<p>为eclipse创建桌面快捷方式，在linux桌面空白处,点击鼠标右键,弹出一个对话框,选择“create Launcher”(中文是“新建启动器”),在“Create Launcher”(中文“创建快捷方式”)对话框中选择Application(中文“应用程序”)，输入名字：eclipse ，在Command(中文“命令”)找到eclipse的启动程序(比如我的是：/opt/eclipse/eclipse),双击左上侧的图标会弹出一个选择图标的对话框，找到安装eclipse的目录,在目录中找到icon.xpm，选择Open(确定),设置好后类似于下图：<br><img src="/img/Hadoop/eclipse/1.png" alt="图1"></p>
<h5 id="修改配置文件">修改配置文件</h5>
<p>hadoop-1.2.1/src/contrib/eclipse-plugin/build.xml<br>添加相应位置下内容：</p>
<pre><code><span class="tag">&lt;<span class="title">path</span> <span class="attribute">id</span>=<span class="value">"eclipse-sdk-jars"</span>&gt;</span>    <span class="tag">&lt;<span class="title">fileset</span> <span class="attribute">dir</span>=<span class="value">"${eclipse.home}/plugins/"</span>&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.ui*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.jdt*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.core*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.equinox*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.debug*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.osgi*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.swt*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.jface*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.team.cvs.ssh2*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"com.jcraft.jsch*.jar"</span>/&gt;</span>
<span class="tag">&lt;/<span class="title">fileset</span>&gt;</span>
<span class="tag">&lt;/<span class="title">path</span>&gt;</span>
//添加这些内容
<span class="tag">&lt;<span class="title">path</span> <span class="attribute">id</span>=<span class="value">"hadoop-lib-jars"</span>&gt;</span>
<span class="tag">&lt;<span class="title">fileset</span> <span class="attribute">dir</span>=<span class="value">"${hadoop.root}/"</span>&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"hadoop*.jar"</span> /&gt;</span>
<span class="tag">&lt;/<span class="title">fileset</span>&gt;</span>
<span class="tag">&lt;/<span class="title">path</span>&gt;</span>
...

<span class="tag">&lt;<span class="title">path</span> <span class="attribute">id</span>=<span class="value">"classpath"</span>&gt;</span>
<span class="tag">&lt;<span class="title">pathelement</span> <span class="attribute">location</span>=<span class="value">"${build.classes}"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">pathelement</span> <span class="attribute">location</span>=<span class="value">"${hadoop.root}/build/classes"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">pathelement</span> <span class="attribute">location</span>=<span class="value">"${hadoop.root}/hadoop-core-1.1.2.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">path</span> <span class="attribute">refid</span>=<span class="value">"eclipse-sdk-jars"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">path</span> <span class="attribute">refid</span>=<span class="value">"hadoop-lib-jars"</span> /&gt;</span>  //添加这一句
<span class="tag">&lt;/<span class="title">path</span>&gt;</span>
...
先注释掉粗体内容前两项，后再添加七项
<span class="tag">&lt;<span class="title">target</span> <span class="attribute">name</span>=<span class="value">"jar"</span> <span class="attribute">depends</span>=<span class="value">"compile"</span> <span class="attribute">unless</span>=<span class="value">"skip.contrib"</span>&gt;</span>    <span class="tag">&lt;<span class="title">mkdir</span> <span class="attribute">dir</span>=<span class="value">"${build.dir}/lib"</span>/&gt;</span>
<span class="comment">&lt;!-- &lt;copy file="${hadoop.root}/build/hadoop-core-${version}.jar" tofile="${build.dir}/lib/hadoop-core.jar" verbose="true"/&gt;
&lt;copy file="${hadoop.root}/build/ivy/lib/Hadoop/common/commons-cli-${commons-cli.version}.jar"  todir="${build.dir}/lib" verbose="true"/&gt;--&gt;</span>
//添加这七项
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/hadoop-core-${version}.jar"</span> <span class="attribute">tofile</span>=<span class="value">"${build.dir}/lib/hadoop-core.jar"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-cli-${commons-cli.version}.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-configuration-1.6.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-httpclient-3.0.1.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-lang-2.4.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/jackson-core-asl-1.8.8.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/jackson-mapper-asl-1.8.8.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>

找到src\contrib\build-contrib.xml，添加以下几行：

<span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"version"</span> <span class="attribute">value</span>=<span class="value">"1.2.1"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"ivy.version"</span> <span class="attribute">value</span>=<span class="value">"2.1.0"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"eclipse.home"</span> <span class="attribute">location</span>=<span class="value">"..."</span>/&gt;</span>
eclipse的路径请换成你主机上的eclipse存放路径。
</code></pre><h5 id="编译，并将插件拷到对应目录">编译，并将插件拷到对应目录</h5>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ ant
Buildfile: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/eclipse-plugin/build.xml

check-contrib:

init:
     [echo] contrib: eclipse-plugin

init-contrib:

ivy-download:
      [get] Getting: http:<span class="comment">//repo2.maven.org/maven2/org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar</span>
      [get] To: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivy-<span class="number">2.1</span><span class="number">.0</span>.jar
      [get] Not modified - so not downloaded

ivy-probe-antlib:

ivy-init-antlib:

ivy-init:
[ivy:configure] :: Ivy <span class="number">2.1</span><span class="number">.0</span> - <span class="number">20090925235825</span> :: http:<span class="comment">//ant.apache.org/ivy/ ::</span>
[ivy:configure] :: loading settings :: <span class="keyword">file</span> = /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivysettings.xml

ivy-resolve-common:

ivy-retrieve-common:
[ivy:cachepath] DEPRECATED: <span class="string">'ivy.conf.file'</span> is deprecated, use <span class="string">'ivy.settings.file'</span> instead
[ivy:cachepath] :: loading settings :: <span class="keyword">file</span> = /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivysettings.xml

compile:
     [echo] contrib: eclipse-plugin
    [javac] /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/eclipse-plugin/build.xml:<span class="number">68</span>: <span class="keyword">warning</span>: <span class="string">'includeantruntime'</span> was not set, defaulting to build.sysclasspath=last; set to false <span class="keyword">for</span> repeatable builds

jar:
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-core-<span class="number">1.2</span><span class="number">.1</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/hadoop-core.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-cli-<span class="number">1.2</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-cli-<span class="number">1.2</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-lang-<span class="number">2.4</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-lang-<span class="number">2.4</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-configuration-<span class="number">1.6</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-configuration-<span class="number">1.6</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-mapper-asl-<span class="number">1.8</span><span class="number">.8</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/jackson-mapper-asl-<span class="number">1.8</span><span class="number">.8</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-core-asl-<span class="number">1.8</span><span class="number">.8</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/jackson-core-asl-<span class="number">1.8</span><span class="number">.8</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-httpclient-<span class="number">3.0</span><span class="number">.1</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-httpclient-<span class="number">3.0</span><span class="number">.1</span>.jar
      [jar] Building jar: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/hadoop-eclipse-plugin-<span class="number">1.2</span><span class="number">.1</span>.jar

BUILD SUCCESSFUL
Total time: <span class="number">10</span> seconds
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ <span class="keyword">ls</span>
build.properties  build.xml  ivy  ivy.xml  META-INF  plugin.xml  resources  src
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ <span class="keyword">pwd</span>
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/eclipse-plugin
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ cd /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ <span class="keyword">ls</span>
classes  examples  hadoop-eclipse-plugin-<span class="number">1.2</span><span class="number">.1</span>.jar  lib  <span class="keyword">system</span>  test
[root<span class="variable">@hadoop01</span> plugins]# cp /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/hadoop-eclipse-plugin-<span class="number">1.2</span><span class="number">.1</span>.jar  /opt/eclipse/plugins/
</code></pre><h5 id="建立测试数据文件word-txt">建立测试数据文件word.txt</h5>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">ls</span>
FileSystemCat.class  FileSystemCat.java  noaa  <span class="keyword">python</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ touch word.txt
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ vi word.txt 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ hadoop fs -mkdir /tmp/wordcount
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ hadoop fs -copyFromLocal /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/word.txt  /tmp/wordcount/word.txt
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ cat word.txt 
java c++ <span class="keyword">python</span> c
java c++ javascript 
helloworld hadoop
mapreduce java hadoop hbase 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$
</code></pre><h5 id="在eclipse上安装hadoop插件">在eclipse上安装hadoop插件</h5>
<p>启动eclipse，配置hadoop installation directory。 如果安装插件成功，打开Window—&gt;Preferens，你会发现Hadoop Map/Reduce选项，在这个选项里你需要配置Hadoop installation directory。配置完成后退出。<br><img src="/img/Hadoop/eclipse/2.png" alt="图2"></p>
<h5 id="配置Map/Reduce_Locations。">配置Map/Reduce Locations。</h5>
<p>在Window—&gt;Show View中打开Map/Reduce Locations。 在Map/Reduce Locations中新建一个Hadoop Location。在这个View中，右键—&gt;New Hadoop Location。在弹出的对话框中你需要配置Location name，如Hadoop，还有Map/Reduce Master和DFS Master。这里面的Host、Port分别为你在mapred-site.xml、core-site.xml中配置的地址及端口。<br><img src="/img/Hadoop/eclipse/3.png" alt="图3"><br><img src="/img/Hadoop/eclipse/4.png" alt="图4"><br>配置完后退出。点击DFS Locations—&gt;Hadoop如果能显示文件夹(2)说明配置正确，如果显示”拒绝连接”，请检查你的配置。<br><img src="/img/Hadoop/eclipse/5.png" alt="图5"></p>
<h5 id="建立项目">建立项目</h5>
<p>新建项目 File—&gt;New—&gt;Other—&gt;Map/Reduce Project 项目名可以随便取，如WordCount。 复制 hadoop安装目录/src/example/org/apache/hadoop/example/WordCount.java到刚才新建的项目下面。</p>
<p>通过hadoop的命令在HDFS上创建/tmp/workcount目录，命令如下：bin/hadoop fs -mkdir /tmp/wordcount</p>
<p>通过copyFromLocal命令把本地的word.txt复制到HDFS上，命令如下：bin/hadoop fs -copyFromLocal /home/grid/word.txt  /tmp/wordcount/word.txt </p>
<h5 id="运行项目">运行项目</h5>
<p>在新建的项目Hadoop，点击WordCount.java，右键—&gt;Run As—&gt;Run Configurations  在弹出的Run Configurations对话框中，点Java Application，右键—&gt;New，这时会新建一个application名为WordCount 配置运行参数，点Arguments，在Program arguments中输入“你要传给程序的输入文件夹和你要求程序将计算结果保存的文件夹”，如：<br><img src="/img/Hadoop/eclipse/6.png" alt="图6"></p>
<p>运行<br><img src="/img/Hadoop/eclipse/7.png" alt="图7"></p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Eclipse" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Eclipse/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Map-Reduce入门]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/map_reduce/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/map_reduce/</id>
    <published>2014-04-21T13:50:52.000Z</published>
    <updated>2014-04-21T15:15:41.000Z</updated>
    <content type="html"><![CDATA[<h5 id="Map-Reduce的逻辑过程">Map-Reduce的逻辑过程</h5>
<p>假设我们需要处理一批有关天气的数据，其格式如下：按照ASCII码存储，每行一条记录,每一行字符从0开始计数，第15个到第18个字符为年,第25个到第29个字符为温度，其中第25位是符号+/-<br>    0067011990999991950051507+0000+<br>    0043011990999991950051512+0022+<br>    0043011990999991950051518-0011+<br>    0043012650999991949032412+0111+<br>    0043012650999991949032418+0078+<br>    0067011990999991937051507+0001+<br>    0043011990999991937051512-0002+<br>    0043011990999991945051518+0001+<br>    0043012650999991945032412+0002+<br>    0043012650999991945032418+0078+</p>
<p>现在需要统计出每年的最高温度。Map-Reduce主要包括两个步骤：Map和Reduce。每一步都有key-value对作为输入和输出：map阶段的key-value对的格式是由输入的格式所决定的，如果是默认的TextInputFormat，则每行作为一个记录进程处理，其中key为此行的开头相对于文件的起始位置，value就是此行的字符文本map阶段的输出的key-value对的格式必须同reduce阶段的输入key-value对的格式相对应。</p>
<p>对于上面的例子，在map过程，输入的key-value对如下：</p>
<pre><code><span class="list">(<span class="title">0</span>,<span class="number"> 0067011990999991950051507</span>+0000+)</span>
<span class="list">(<span class="title">33</span>,<span class="number"> 0043011990999991950051512</span>+0022+)</span>
<span class="list">(<span class="title">66</span>,<span class="number"> 0043011990999991950051518</span>-0011+)</span>
<span class="list">(<span class="title">99</span>,<span class="number"> 0043012650999991949032412</span>+0111+)</span>
<span class="list">(<span class="title">132</span>,<span class="number"> 0043012650999991949032418</span>+0078+)</span>
<span class="list">(<span class="title">165</span>,<span class="number"> 0067011990999991937051507</span>+0001+)</span>
<span class="list">(<span class="title">198</span>,<span class="number"> 0043011990999991937051512</span>-0002+)</span>
<span class="list">(<span class="title">231</span>,<span class="number"> 0043011990999991945051518</span>+0001+)</span>
<span class="list">(<span class="title">264</span>,<span class="number"> 0043012650999991945032412</span>+0002+)</span>
<span class="list">(<span class="title">297</span>,<span class="number"> 0043012650999991945032418</span>+0078+)</span>
</code></pre><p>在map过程中，通过对每一行字符串的解析，得到年-温度的key-value对作为输出：</p>
<pre><code><span class="list">(<span class="title">1950</span>,<span class="number"> 0</span>)</span>
<span class="list">(<span class="title">1950</span>,<span class="number"> 22</span>)</span>
<span class="list">(<span class="title">1950</span>, -11)</span>
<span class="list">(<span class="title">1949</span>,<span class="number"> 111</span>)</span>
<span class="list">(<span class="title">1949</span>,<span class="number"> 78</span>)</span>
<span class="list">(<span class="title">1937</span>,<span class="number"> 1</span>)</span>
<span class="list">(<span class="title">1937</span>, -2)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 1</span>)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 2</span>)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 78</span>)</span>
</code></pre><p>在reduce过程，将map过程中的输出，按照相同的key将value放到同一个列表中作为reduce的输入</p>
<pre><code><span class="list">(<span class="title">1950</span>, <span class="collection">[0,<span class="number"> 22</span>, –11]</span>)</span>
<span class="list">(<span class="title">1949</span>, <span class="collection">[111,<span class="number"> 78</span>]</span>)</span>
<span class="list">(<span class="title">1937</span>, <span class="collection">[1, -2]</span>)</span>
<span class="list">(<span class="title">1945</span>, <span class="collection">[1,<span class="number"> 2</span>,<span class="number"> 78</span>]</span>)</span>
</code></pre><p>在reduce过程中，在列表中选择出最大的温度，将年-最大温度的key-value作为输出：</p>
<pre><code><span class="list">(<span class="title">1950</span>,<span class="number"> 22</span>)</span>
<span class="list">(<span class="title">1949</span>,<span class="number"> 111</span>)</span>
<span class="list">(<span class="title">1937</span>,<span class="number"> 1</span>)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 78</span>)</span>
</code></pre><p>其逻辑过程可用如下图表示：<br><img src="/img/Hadoop/MapReduce/1.png" alt="图1"></p>
<h5 id="编写Map-Reduce程序">编写Map-Reduce程序</h5>
<p>编写Map-Reduce程序，一般需要实现两个函数：mapper中的map函数和reducer中的reduce函数。<br>一般遵循以下格式：</p>
<pre><code>map: (K1, V1)  -&gt;  <span class="keyword">list</span>(K2, V2)
<span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Mapper</span>&lt;<span class="title">K1</span>, <span class="title">V1</span>, <span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">extends</span> <span class="title">JobConfigurable</span>, <span class="title">Closeable</span> {</span>

  void map(K1 key, V1 value, OutputCollector&lt;K2, V2&gt; output, Reporter reporter)

  throws IOException;

}

reduce: (K2, <span class="keyword">list</span>(V))  -&gt;  <span class="keyword">list</span>(K3, V3) 
<span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Reducer</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>, <span class="title">K3</span>, <span class="title">V3</span>&gt; <span class="keyword">extends</span> <span class="title">JobConfigurable</span>, <span class="title">Closeable</span> {</span>

  void reduce(K2 key, Iterator&lt;V2&gt; values,

              OutputCollector&lt;K3, V3&gt; output, Reporter reporter)

    throws IOException;

}
</code></pre><p>对于上面的例子，则实现的mapper如下：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxTemperatureMapper</span> <span class="inheritance"><span class="keyword">extends</span></span> <span class="title">MapReduceBase</span> <span class="inheritance"><span class="keyword">implements</span></span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; {</span>

    @Override

    <span class="keyword">public</span> <span class="keyword">void</span> map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {

        String line = value.toString();

        String year = line.substring(<span class="number">15</span>, <span class="number">19</span>);

        <span class="keyword">int</span> airTemperature;

        <span class="keyword">if</span> (line.charAt(<span class="number">25</span>) == <span class="string">'+'</span>) {

            airTemperature = Integer.parseInt(line.substring(<span class="number">26</span>, <span class="number">30</span>));

        } <span class="keyword">else</span> {

            airTemperature = Integer.parseInt(line.substring(<span class="number">25</span>, <span class="number">30</span>));

        }

        output.collect(<span class="keyword">new</span> Text(year), <span class="keyword">new</span> IntWritable(airTemperature));

    }

}
</code></pre><p>实现的reducer如下：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxTemperatureReducer</span> <span class="inheritance"><span class="keyword">extends</span></span> <span class="title">MapReduceBase</span> <span class="inheritance"><span class="keyword">implements</span></span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; {</span>

    <span class="keyword">public</span> <span class="keyword">void</span> reduce(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {

        <span class="keyword">int</span> maxValue = Integer.MIN_VALUE;

        <span class="keyword">while</span> (values.hasNext()) {

            maxValue = Math.max(maxValue, values.next().get());

        }

        output.collect(key, <span class="keyword">new</span> IntWritable(maxValue));

    }

}
</code></pre><p>欲运行上面实现的Mapper和Reduce，则需要生成一个Map-Reduce得任务(Job)，其基本包括以下三部分：</p>
<pre><code>输入的数据，也即需要处理的数据
<span class="keyword">Map</span>-<span class="keyword">Reduce</span>程序，也即上面实现的Mapper和Reducer
此任务的配置项JobConf
</code></pre><p>欲配置JobConf，需要大致了解Hadoop运行job的基本原理：</p>
<pre><code>Hadoop将Job分成task进行处理，共两种task：map task和reduce task
Hadoop有两类的节点控制job的运行：JobTracker和TaskTracker
JobTracker协调整个job的运行，将task分配到不同的TaskTracker上
TaskTracker负责运行task，并将结果返回给JobTracker
Hadoop将输入数据分成固定大小的块，我们称之input split
Hadoop为每一个input split创建一个task，在此task中依次处理此split中的一个个记录(record)
Hadoop会尽量让输入数据块所在的DataNode和task所执行的DataNode(每个DataNode上都有一个TaskTracker)为同一个，
可以提高运行效率，所以input split的大小也一般是HDFS的block的大小。
Reduce task的输入一般为Map Task的输出，Reduce Task的输出为整个job的输出，保存在HDFS上。
在reduce中，相同key的所有的记录一定会到同一个TaskTracker上面运行，
然而不同的key可以在不同的TaskTracker上面运行，我们称之为partition
partition的规则为：(K2, V2) –&gt; Integer， 也即根据K2，生成一个partition的id，
具有相同id的K2则进入同一个partition，被同一个TaskTracker上被同一个Reducer进行处理。
<span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="inheritance"><span class="keyword">extends</span></span> <span class="title">JobConfigurable</span> {</span>

<span class="keyword">int</span> getPartition(K2 key, V2 value, <span class="keyword">int</span> numPartitions);

}
</code></pre><p>下图大概描述了Map-Reduce的Job运行的基本原理：<br><img src="/img/Hadoop/MapReduce/2.png" alt="图2"></p>
<p>下面我们讨论JobConf，其有很多的项可以进行配置：<br>1、setInputFormat：设置map的输入格式，默认为TextInputFormat，key为LongWritable, value为Text<br>2、setNumMapTasks：设置map任务的个数，此设置通常不起作用，map任务的个数取决于输入的数据所能分成的input split的个数<br>3、setMapperClass：设置Mapper，默认为IdentityMapper<br>4、setMapRunnerClass：设置MapRunner, map task是由MapRunner运行的，默认为MapRunnable，其功能为读取input split的一个个record，依次调用Mapper的map函数<br>5、setMapOutputKeyClass和setMapOutputValueClass：设置Mapper的输出的key-value对的格式<br>6、setOutputKeyClass和setOutputValueClass：设置Reducer的输出的key-value对的格式<br>7、setPartitionerClass和setNumReduceTasks：设置Partitioner，默认为HashPartitioner，其根据key的hash值来决定进入哪个partition，每个partition被一个reduce task处理，所以partition的个数等于reduce task的个数<br>8、setReducerClass：设置Reducer，默认为IdentityReducer<br>9、setOutputFormat：设置任务的输出格式，默认为TextOutputFormat<br>10、FileInputFormat.addInputPath：设置输入文件的路径，可以使一个文件，一个路径，一个通配符。可以被调用多次添加多个路径<br>11、FileOutputFormat.setOutputPath：设置输出文件的路径，在job运行前此路径不应该存在</p>
<p>当然不用所有的都设置，由上面的例子，可以编写Map-Reduce程序如下：</p>
<pre><code>public class MaxTemperature {

    public static void main(String[] args) throws IOException {

        if (args<span class="preprocessor">.length</span> != <span class="number">2</span>) {

            System<span class="preprocessor">.err</span><span class="preprocessor">.println</span>(<span class="string">"Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"</span>)<span class="comment">;</span>

            System<span class="preprocessor">.exit</span>(-<span class="number">1</span>)<span class="comment">;</span>

        }

        JobConf conf = new JobConf(MaxTemperature<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setJobName</span>(<span class="string">"Max temperature"</span>)<span class="comment">;</span>

        FileInputFormat<span class="preprocessor">.addInputPath</span>(conf, new Path(args[<span class="number">0</span>]))<span class="comment">;</span>

        FileOutputFormat<span class="preprocessor">.setOutputPath</span>(conf, new Path(args[<span class="number">1</span>]))<span class="comment">;</span>

        conf<span class="preprocessor">.setMapperClass</span>(MaxTemperatureMapper<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setReducerClass</span>(MaxTemperatureReducer<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setOutputKeyClass</span>(Text<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setOutputValueClass</span>(IntWritable<span class="preprocessor">.class</span>)<span class="comment">;</span>

        JobClient<span class="preprocessor">.runJob</span>(conf)<span class="comment">;</span>

    }

}
</code></pre><h5 id="Map-Reduce数据流(data_flow)">Map-Reduce数据流(data flow)</h5>
<p>Map-Reduce的处理过程主要涉及以下四个部分：<br>1、客户端Client：用于提交Map-reduce任务job<br>2、JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker<br>3、TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker<br>4、HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件<br><img src="/img/Hadoop/MapReduce/3.png" alt="图3"></p>
<h6 id="任务提交">任务提交</h6>
<p>JobClient.runJob()创建一个新的JobClient实例，调用其submitJob()函数。</p>
<p>向JobTracker请求一个新的job ID</p>
<p>检测此job的output配置</p>
<p>计算此job的input splits</p>
<p>将Job运行所需的资源拷贝到JobTracker的文件系统中的文件夹中，包括job jar文件，job.xml配置文件，input splits</p>
<p>通知JobTracker此Job已经可以运行了</p>
<p>提交任务后，runJob每隔一秒钟轮询一次job的进度，将进度返回到命令行，直到任务运行完毕。</p>
<h6 id="任务初始化">任务初始化</h6>
<p>当JobTracker收到submitJob调用的时候，将此任务放到一个队列中，job调度器将从队列中获取任务并初始化任务。</p>
<p>初始化首先创建一个对象来封装job运行的tasks, status以及progress。</p>
<p>在创建task之前，job调度器首先从共享文件系统中获得JobClient计算出的input splits。</p>
<p>其为每个input split创建一个map task。</p>
<p>每个task被分配一个ID。</p>
<h6 id="任务分配">任务分配</h6>
<p>TaskTracker周期性的向JobTracker发送heartbeat。</p>
<p>在heartbeat中，TaskTracker告知JobTracker其已经准备运行一个新的task，JobTracker将分配给其一个task。</p>
<p>在JobTracker为TaskTracker选择一个task之前，JobTracker必须首先按照优先级选择一个Job，在最高优先级的Job中选择一个task。</p>
<p>TaskTracker有固定数量的位置来运行map task或者reduce task。</p>
<p>默认的调度器对待map task优先于reduce task</p>
<p>当选择reduce task的时候，JobTracker并不在多个task之间进行选择，而是直接取下一个，因为reduce task没有数据本地化的概念。</p>
<h6 id="任务执行">任务执行</h6>
<p>TaskTracker被分配了一个task，下面便要运行此task。</p>
<p>首先，TaskTracker将此job的jar从共享文件系统中拷贝到TaskTracker的文件系统中。</p>
<p>TaskTracker从distributed cache中将job运行所需要的文件拷贝到本地磁盘。</p>
<p>其次，其为每个task创建一个本地的工作目录，将jar解压缩到文件目录中。</p>
<p>其三，其创建一个TaskRunner来运行task。</p>
<p>TaskRunner创建一个新的JVM来运行task。</p>
<p>被创建的child JVM和TaskTracker通信来报告运行进度。</p>
<h6 id="Map的过程">Map的过程</h6>
<p>MapRunnable从input split中读取一个个的record，然后依次调用Mapper的map函数，将结果输出。</p>
<p>map的输出并不是直接写入硬盘，而是将其写入缓存memory buffer。</p>
<p>当buffer中数据的到达一定的大小，一个背景线程将数据开始写入硬盘。</p>
<p>在写入硬盘之前，内存中的数据通过partitioner分成多个partition。</p>
<p>在同一个partition中，背景线程会将数据按照key在内存中排序。</p>
<p>每次从内存向硬盘flush数据，都生成一个新的spill文件。</p>
<p>当此task结束之前，所有的spill文件被合并为一个整的被partition的而且排好序的文件。</p>
<p>reducer可以通过http协议请求map的输出文件，tracker.http.threads可以设置http服务线程数。</p>
<h6 id="Reduce的过程">Reduce的过程</h6>
<p>当map task结束后，其通知TaskTracker，TaskTracker通知JobTracker。</p>
<p>对于一个job，JobTracker知道TaskTracer和map输出的对应关系。</p>
<p>reducer中一个线程周期性的向JobTracker请求map输出的位置，直到其取得了所有的map输出。</p>
<p>reduce task需要其对应的partition的所有的map输出。</p>
<p>reduce task中的copy过程即当每个map task结束的时候就开始拷贝输出，因为不同的map task完成时间不同。</p>
<p>reduce task中有多个copy线程，可以并行拷贝map输出。</p>
<p>当很多map输出拷贝到reduce task后，一个背景线程将其合并为一个大的排好序的文件。</p>
<p>当所有的map输出都拷贝到reduce task后，进入sort过程，将所有的map输出合并为大的排好序的文件。</p>
<p>最后进入reduce过程，调用reducer的reduce函数，处理排好序的输出的每个key，最后的结果写入HDFS。</p>
<p><img src="/img/Hadoop/MapReduce/4.png" alt="图4"></p>
<h6 id="任务结束">任务结束</h6>
<p>当JobTracker获得最后一个task的运行成功的报告后，将job得状态改为成功。</p>
<p>当JobClient从JobTracker轮询的时候，发现此job已经成功结束，则向用户打印消息，从runJob函数中返回。</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Map-Reduce" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Map-Reduce/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用hadoop流的方法来实现对气象数据集求最高温度的分析任务（使用python）]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_stream_python/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_stream_python/</id>
    <published>2014-04-21T13:50:46.000Z</published>
    <updated>2014-04-21T14:18:29.000Z</updated>
    <content type="html"><![CDATA[<p>使用hadoop流的方法来实现对气象数据集求最高温度的分析任务（使用python）</p>
<p>创建python脚本</p>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ <span class="keyword">ls</span>
max_temperature_map.py  max_temperature_reduce.py
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ cat max_temperature_map.py 
#!/bin/<span class="keyword">env</span> <span class="keyword">python</span>
import re
import sys
<span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:
  val = line.<span class="keyword">strip</span>()
  (year, temp) = (val[<span class="number">0</span>:<span class="number">4</span>], val[<span class="number">13</span>:<span class="number">19</span>])
  <span class="keyword">if</span> (temp != <span class="string">"+9999"</span>):
    <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (year, temp)
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ cat max_temperature_reduce.py 
#!/bin/<span class="keyword">env</span> <span class="keyword">python</span>
import sys

(last_key, max_val) = (None, -sys.maxint)
<span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:
  (key, val) = line.<span class="keyword">strip</span>().split(<span class="string">"\t"</span>)
  <span class="keyword">if</span> last_key and last_key != key:
    <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (last_key, max_val)
    (last_key, max_val) = (key, <span class="keyword">int</span>(val))
  <span class="keyword">else</span>:
    (last_key, max_val) = (key, <span class="keyword">max</span>(max_val, <span class="keyword">int</span>(val)))

<span class="keyword">if</span> last_key:
  <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (last_key, max_val)
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ 
</code></pre><p>编译运行</p>
<pre><code>[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ bin/hadoop jar contrib/streaming/hadoop-streaming-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span> -input ./<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span> -output ./out3 -mapper /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_map<span class="preprocessor">.py</span> -reducer /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_reduce<span class="preprocessor">.py</span> -file /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_map<span class="preprocessor">.py</span> -file /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_reduce<span class="preprocessor">.py</span> 
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.
<span class="label">packageJobJar:</span> [/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_map<span class="preprocessor">.py</span>, /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_reduce<span class="preprocessor">.py</span>, /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/tmp/hadoop-unjar7471324797393042067/] [] /tmp/streamjob6919467714598114562<span class="preprocessor">.jar</span> tmpDir=null
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">12</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">12</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">12</span> INFO mapred<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: getLocalDirs(): [/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/tmp/mapred/local]
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Running job: job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: To kill this job, run:
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/libexec/../bin/hadoop job  -Dmapred<span class="preprocessor">.job</span><span class="preprocessor">.tracker</span>=hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9001</span> -kill job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Tracking URL: http://hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">50030</span>/jobdetails<span class="preprocessor">.jsp</span>?jobid=job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">14</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">0</span>%  reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">26</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">100</span>%  reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">35</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">100</span>%  reduce <span class="number">33</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">40</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">100</span>%  reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">43</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Job complete: job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">43</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Output: ./out3
</code></pre><p>查看结果</p>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -<span class="keyword">ls</span> ./out3
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span> /user/<span class="keyword">grid</span>/out3/_SUCCESS
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span> /user/<span class="keyword">grid</span>/out3/_logs
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup       <span class="number">5184</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span> /user/<span class="keyword">grid</span>/out3/part-<span class="number">00000</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ 
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop_Stream" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop_Stream/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[下载气象数据集部分数据，写一个Map-Reduce作业，求每年的最低温度，部署并运行之]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_mapreduce_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_mapreduce_1/</id>
    <published>2014-04-21T13:50:41.000Z</published>
    <updated>2014-04-21T14:10:31.000Z</updated>
    <content type="html"><![CDATA[<p>下载气象数据集部分数据，写一个Map-Reduce作业，求每年的最低温度，部署并运行之，关于下载2013noaa上的天气数据，去<a href="http://f.dataguru.cn/" target="_blank">http://f.dataguru.cn/</a> 的hadoop论坛上去搜就行</p>
<pre><code>是这种格式，<span class="number">2013</span>的部分数据
<span class="number">023170</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032260</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064280</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080440</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">121200</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
<span class="number">023190</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032270</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064310</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080450</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">121240</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
<span class="number">023210</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032300</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064320</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080480</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  noaaSample<span class="preprocessor">.txt</span>
<span class="number">023250</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032380</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064325</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080530</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
<span class="number">023270</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032400</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064340</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080550</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
[grid@hadoop01 noaa]$ zcat  *<span class="preprocessor">.gz</span> &gt; noaaSample<span class="preprocessor">.txt</span>
</code></pre><p>编写Java文件 MinTemperatureMapper.java,MinTemperatureReducer.java, 和 MinTemperature.java</p>
<pre><code>MinTemperatureMapper<span class="preprocessor">.java</span>
import java<span class="preprocessor">.io</span><span class="preprocessor">.IOException</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IntWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.LongWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.Mapper</span><span class="comment">;</span>
public class MinTemperatureMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private static final int MISSING = <span class="number">9999</span><span class="comment">;</span>
    @Override
    public void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {
        String line = value<span class="preprocessor">.toString</span>()<span class="comment">;</span>
        String year = line<span class="preprocessor">.substring</span>(<span class="number">15</span>, <span class="number">19</span>)<span class="comment">;</span>
        int airTemperature<span class="comment">;</span>
        if (line<span class="preprocessor">.charAt</span>(<span class="number">87</span>) == <span class="string">'+'</span>) { 
            airTemperature = Integer<span class="preprocessor">.parseInt</span>(line<span class="preprocessor">.substring</span>(<span class="number">88</span>, <span class="number">92</span>))<span class="comment">;</span>
        } else {
            airTemperature = Integer<span class="preprocessor">.parseInt</span>(line<span class="preprocessor">.substring</span>(<span class="number">87</span>, <span class="number">92</span>))<span class="comment">;</span>
        }
        String quality = line<span class="preprocessor">.substring</span>(<span class="number">92</span>, <span class="number">93</span>)<span class="comment">;</span>
        if (airTemperature != MISSING &amp;&amp; quality<span class="preprocessor">.matches</span>(<span class="string">"[01459]"</span>)) {
            context<span class="preprocessor">.write</span>(new Text(year), new IntWritable(airTemperature))<span class="comment">;</span>
        }
    }
}

MinTemperatureReducer<span class="preprocessor">.java</span>
import java<span class="preprocessor">.io</span><span class="preprocessor">.IOException</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IntWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.Reducer</span><span class="comment">;</span>
public class MinTemperatureReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    @Override
    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)
            throws IOException, InterruptedException {
        int minValue = Integer<span class="preprocessor">.MAX</span>_VALUE<span class="comment">;</span>
        for (IntWritable value : values) {
            minValue = Math<span class="preprocessor">.min</span>(minValue, value<span class="preprocessor">.get</span>())<span class="comment">;</span>
        }
        context<span class="preprocessor">.write</span>(key, new IntWritable(minValue))<span class="comment">;</span>
    }
}

MinTemperature<span class="preprocessor">.java</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.fs</span><span class="preprocessor">.Path</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IntWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.Job</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.lib</span><span class="preprocessor">.input</span><span class="preprocessor">.FileInputFormat</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.lib</span><span class="preprocessor">.output</span><span class="preprocessor">.FileOutputFormat</span><span class="comment">;</span>
public class MinTemperature {
    public static void main(String[] args) throws Exception {
        if (args<span class="preprocessor">.length</span> != <span class="number">2</span>) {
            System<span class="preprocessor">.err</span><span class="preprocessor">.println</span>(<span class="string">"Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"</span>)<span class="comment">;</span>
            System<span class="preprocessor">.exit</span>(-<span class="number">1</span>)<span class="comment">;</span>
        }
        Job job = new Job()<span class="comment">;</span>
        job<span class="preprocessor">.setJarByClass</span>(MinTemperature<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setJobName</span>(<span class="string">"Min temperature"</span>)<span class="comment">;</span>
        FileInputFormat<span class="preprocessor">.addInputPath</span>(job, new Path(args[<span class="number">0</span>]))<span class="comment">;</span>
        FileOutputFormat<span class="preprocessor">.setOutputPath</span>(job, new Path(args[<span class="number">1</span>]))<span class="comment">;</span>
        job<span class="preprocessor">.setMapperClass</span>(MinTemperatureMapper<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setReducerClass</span>(MinTemperatureReducer<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setOutputKeyClass</span>(Text<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setOutputValueClass</span>(IntWritable<span class="preprocessor">.class</span>)<span class="comment">;</span>
        System<span class="preprocessor">.exit</span>(job<span class="preprocessor">.waitForCompletion</span>(true) ? <span class="number">0</span> : <span class="number">1</span>)<span class="comment">;</span>
    }
}
</code></pre><p>编译 打包  将jar包移到$HADOOP_HOME, 删除原class文件</p>
<pre><code>[grid@hadoop01 noaa]$ javac -classpath  /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-core-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span> *<span class="preprocessor">.java</span>
[grid@hadoop01 noaa]$ ls
MinTemperature<span class="preprocessor">.class</span>  MinTemperatureMapper<span class="preprocessor">.class</span>  MinTemperatureReducer<span class="preprocessor">.class</span>
MinTemperature<span class="preprocessor">.java</span>   MinTemperatureMapper<span class="preprocessor">.java</span>   MinTemperatureReducer<span class="preprocessor">.java</span>
[grid@hadoop01 noaa]$ hadoop  fs -put  /home/grid/Downloads/noaa/noaaSample<span class="preprocessor">.txt</span>   ./<span class="keyword">in</span>
[grid@hadoop01 noaa]$ hadoop  fs -ls ./<span class="keyword">in</span>
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> grid supergroup   <span class="number">39564893</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">43</span> /user/grid/<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span>
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">12</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span>:<span class="number">57</span> /user/grid/<span class="keyword">in</span>/test1<span class="preprocessor">.txt</span>
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">13</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span>:<span class="number">57</span> /user/grid/<span class="keyword">in</span>/test2<span class="preprocessor">.txt</span>
[grid@hadoop01 noaa]$ jar cvf ./MinTemperature<span class="preprocessor">.jar</span> *<span class="preprocessor">.class</span>
added manifest
<span class="label">adding:</span> MinTemperature<span class="preprocessor">.class</span>(<span class="keyword">in</span> = <span class="number">1418</span>) (<span class="keyword">out</span>= <span class="number">802</span>)(deflated <span class="number">43</span>%)
<span class="label">adding:</span> MinTemperatureMapper<span class="preprocessor">.class</span>(<span class="keyword">in</span> = <span class="number">1876</span>) (<span class="keyword">out</span>= <span class="number">804</span>)(deflated <span class="number">57</span>%)
<span class="label">adding:</span> MinTemperatureReducer<span class="preprocessor">.class</span>(<span class="keyword">in</span> = <span class="number">1664</span>) (<span class="keyword">out</span>= <span class="number">706</span>)(deflated <span class="number">57</span>%)
[grid@hadoop01 noaa]$ ls
MinTemperature<span class="preprocessor">.class</span>  MinTemperature<span class="preprocessor">.java</span>         MinTemperatureMapper<span class="preprocessor">.java</span>    MinTemperatureReducer<span class="preprocessor">.java</span>
MinTemperature<span class="preprocessor">.jar</span>    MinTemperatureMapper<span class="preprocessor">.class</span>  MinTemperatureReducer<span class="preprocessor">.class</span>
[grid@hadoop01 noaa]$ mv MinTemperature<span class="preprocessor">.jar</span> /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/
[grid@hadoop01 noaa]$ rm  *<span class="preprocessor">.class</span>
[grid@hadoop01 noaa]$ ls
MinTemperature<span class="preprocessor">.java</span>  MinTemperatureMapper<span class="preprocessor">.java</span>  MinTemperatureReducer<span class="preprocessor">.java</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ ls
<span class="number">7287</span>OS_Code  contrib                    Hadoop MapReduce Cookbook<span class="preprocessor">.zip</span>  lib                 NOTICE<span class="preprocessor">.txt</span>  webapps
bin          docs                       hadoop-minicluster-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>   libexec             README<span class="preprocessor">.txt</span>
build<span class="preprocessor">.xml</span>    hadoop-ant-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>       hadoop-test-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>          LICENSE<span class="preprocessor">.txt</span>         sbin
c++          hadoop-client-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>    hadoop-tools-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>         logs                share
CHANGES<span class="preprocessor">.txt</span>  hadoop-core-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>      ivy                            MinTemperature<span class="preprocessor">.jar</span>  src
conf         hadoop-examples-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>  ivy<span class="preprocessor">.xml</span>                        myclass             tmp
</code></pre><p>运行程序</p>
<pre><code>[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop jar ./MinTemperature<span class="preprocessor">.jar</span> MinTemperature ./<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span> ./out2
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> WARN mapred<span class="preprocessor">.JobClient</span>: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">30</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201404211930_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">31</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">48</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">58</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">33</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">01</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201404211930_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">29</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched reduce tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">16513</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Data-local map tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">12911</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">10</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_READ=<span class="number">1652569</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">39565019</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">3419762</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_WRITTEN=<span class="number">10</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">39564893</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output materialized bytes=<span class="number">1652569</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">154375</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce shuffle bytes=<span class="number">1652569</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">300466</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output bytes=<span class="number">1352097</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">176033792</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">7420</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine input records=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">126</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input records=<span class="number">150233</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input groups=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine output records=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">302170112</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce output records=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">1453506560</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">150233</span>
</code></pre><p>查看结果</p>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -<span class="keyword">ls</span> ./out2
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span> /user/<span class="keyword">grid</span>/out2/_SUCCESS
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span> /user/<span class="keyword">grid</span>/out2/_logs
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup         <span class="number">10</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span> /user/<span class="keyword">grid</span>/out2/part-r-<span class="number">00000</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -cat ./out2/part-r-<span class="number">00000</span>
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.
<span class="number">2013</span>    -<span class="number">604</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ 
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Map-Reduce" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Map-Reduce/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HDFS简单认识]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_hdfs/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_hdfs/</id>
    <published>2014-04-18T11:58:57.000Z</published>
    <updated>2014-04-23T11:48:29.000Z</updated>
    <content type="html"><![CDATA[<h3 id="一、HDFS的基本概念">一、HDFS的基本概念</h3>
<h4 id="1-1、数据块(block)">1.1、数据块(block)</h4>
<p>HDFS(Hadoop Distributed File System)默认的最基本的存储单位是64M的数据块。和普通文件系统相同的是，HDFS中的文件是被分成64M一块的数据块存储的。不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。</p>
<h4 id="1-2、元数据节点(Namenode)和数据节点(datanode)">1.2、元数据节点(Namenode)和数据节点(datanode)</h4>
<p>元数据节点用来管理文件系统的命名空间,其将所有的文件和文件夹的元数据保存在一个文件系统树中。这些信息也会在硬盘上保存成以下文件：命名空间镜像(namespace image)及修改日志(edit log)。其还保存了一个文件包括哪些数据块，分布在哪些数据节点上。然而这些信息并不存储在硬盘上，而是在系统启动的时候从数据节点收集而成的。</p>
<p>数据节点是文件系统中真正存储数据的地方。客户端(client)或者元数据信息(namenode)可以向数据节点请求写入或者读出数据块。其周期性的向元数据节点回报其存储的数据块信息。</p>
<p>从元数据节点(secondary namenode)从元数据节点并不是元数据节点出现问题时候的备用节点，它和元数据节点负责不同的事情。其主要功能就是周期性将元数据节点的命名空间镜像文件和修改日志合并，以防日志文件过大。这点在下面会相信叙述。合并过后的命名空间镜像文件也在从元数据节点保存了一份，以防元数据节点失败的时候，可以恢复。</p>
<h4 id="1-2-1、元数据节点文件夹结构">1.2.1、元数据节点文件夹结构</h4>
<p><img src="/img/Hadoop/HDFS/1.png" alt="图1"><br>VERSION文件是java properties文件，保存了HDFS的版本号。</p>
<pre><code>layoutVersion是一个负整数，保存了HDFS的持续化在硬盘上的数据结构的格式版本号。
namespaceID是文件系统的唯一标识符，是在文件系统初次格式化时生成的。
cTime此处为0
storageType表示此文件夹中保存的是元数据节点的数据结构。
<span class="constant">namespaceID</span>=1232737062
<span class="constant">cTime</span>=0
<span class="constant">storageType</span>=NAME_NODE
<span class="constant">layoutVersion</span>=-18
</code></pre><h4 id="1-2-2、文件系统命名空间映像文件及修改日志">1.2.2、文件系统命名空间映像文件及修改日志</h4>
<p>当文件系统客户端(client)进行写操作时，首先把它记录在修改日志中(edit log),元数据节点在内存中保存了文件系统的元数据信息。在记录了修改日志后，元数据节点则修改内存中的数据结构。每次的写操作成功之前，修改日志都会同步(sync)到文件系统。</p>
<p>fsimage文件，也即命名空间映像文件，是内存中的元数据在硬盘上的checkpoint，它是一种序列化的格式，并不能够在硬盘上直接修改。同数据的机制相似，当元数据节点失败时，则最新checkpoint的元数据信息从fsimage加载到内存中，然后逐一重新执行修改日志中的操作。从元数据节点就是用来帮助元数据节点将内存中的元数据信息checkpoint到硬盘上的。<br>checkpoint的过程如下：</p>
<pre><code>从元数据节点通知元数据节点生成新的日志文件，以后的日志都写到新的日志文件中。
从元数据节点用<span class="keyword">http</span> <span class="built_in">get</span>从元数据节点获得fsimage文件及旧的日志文件。
从元数据节点将fsimage文件加载到内存中，并执行日志文件中的操作，然后生成新的fsimage文件。
从元数据节点奖新的fsimage文件用<span class="keyword">http</span> <span class="built_in">post</span>传回元数据节点
元数据节点可以将旧的fsimage文件及旧的日志文件，换为新的fsimage文件和新的日志文件(第一步生成的)，
然后更新fstime文件，写入此次checkpoint的时间。
</code></pre><p>这样元数据节点中的fsimage文件保存了最新的checkpoint的元数据信息，日志文件也重新开始，不会变的很大了。<br><img src="/img/Hadoop/HDFS/2.png" alt="图2"></p>
<h4 id="1-2-3、从元数据节点的目录结构">1.2.3、从元数据节点的目录结构</h4>
<p><img src="/img/Hadoop/HDFS/3.png" alt="图3"></p>
<h4 id="1-2-4、数据节点的目录结构">1.2.4、数据节点的目录结构</h4>
<p><img src="/img/Hadoop/HDFS/4.png" alt="图4"><br>数据节点的VERSION文件格式如下：</p>
<pre><code><span class="setting">namespaceID=<span class="value"><span class="number">1232737062</span></span></span>
<span class="setting">storageID=<span class="value">DS-<span class="number">1640411682</span>-<span class="number">127.0</span>.<span class="number">1.1</span>-<span class="number">50010</span>-<span class="number">1254997319480</span></span></span>
<span class="setting">cTime=<span class="value"><span class="number">0</span></span></span>
<span class="setting">storageType=<span class="value">DATA_NODE</span></span>
<span class="setting">layoutVersion=<span class="value">-<span class="number">18</span></span></span>
</code></pre><p>blk_<id>保存的是HDFS的数据块，其中保存了具体的二进制数据。</p>
<p>blk_<id>.meta保存的是数据块的属性信息：版本信息，类型信息，和checksum。</p>
<p>当一个目录中的数据块到达一定数量的时候，则创建子文件夹来保存数据块及数据块属性信息。</p>
<h3 id="二、数据流(data_flow)">二、数据流(data flow)</h3>
<h4 id="2-1、读文件的过程">2.1、读文件的过程</h4>
<p>客户端(client)用FileSystem的open()函数打开文件，DistributedFileSystem用RPC调用元数据节点，得到文件的数据块信息。对于每一个数据块，元数据节点返回保存数据块的数据节点的地址。DistributedFileSystem返回FSDataInputStream给客户端，用来读取数据。客户端调用stream的read()函数开始读取数据。DFSInputStream连接保存此文件第一个数据块的最近的数据节点。Data从数据节点读到客户端(client)。<br>当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。</p>
<p>当客户端读取完毕数据的时候，调用FSDataInputStream的close函数。在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。失败的数据节点将被记录，以后不再连接。<br><img src="/img/Hadoop/HDFS/5.png" alt="图5"></p>
<h4 id="2-2、写文件的过程">2.2、写文件的过程</h4>
<p>客户端调用create()来创建文件，DistributedFileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件。元数据节点首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件。DistributedFileSystem返回DFSOutputStream，客户端用于写数据。</p>
<p>客户端开始写入数据，DFSOutputStream将数据分成块，写入data queue。Data queue由Data Streamer读取，并通知元数据节点分配数据节点，用来存储数据块(每块默认复制3块)。分配的数据节点放在一个pipeline里。Data Streamer将数据块写入pipeline中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。第二个数据节点将数据发送给第三个数据节点。DFSOutputStream为发出去的数据块保存了ack queue，等待pipeline中的数据节点告知数据已经写入成功。</p>
<p>如果数据节点在写入的过程中失败：关闭pipeline，将ack queue中的数据块放入data queue的开始。当前的数据块在已经写入的数据节点中被元数据节点赋予新的标示，则错误节点重启后能够察觉其数据块是过时的，会被删除。失败的数据节点从pipeline中移除，另外的数据块则写入pipeline中的另外两个数据节点。元数据节点则被通知此数据块是复制块数不足，将来会再创建第三份备份。</p>
<p>当客户端结束写入数据，则调用stream的close函数。此操作将所有的数据块写入pipeline中的数据节点，并等待ack queue返回成功。最后通知元数据节点写入完毕。<br><img src="/img/Hadoop/HDFS/6.png" alt="图6"></p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/HDFS/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop集群中编译并运行《权威指南》中的例3.2]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_test_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_test_1/</id>
    <published>2014-04-18T11:58:49.000Z</published>
    <updated>2014-04-18T12:15:20.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1-_install_ant">1. install ant</h3>
<h3 id="2-_download_7278OS_Code">2. download 7278OS_Code</h3>
<p><a href="http://www.packtub.com/code_download/11101" target="_blank">http://www.packtub.com/code_download/11101</a></p>
<h3 id="3-vi_/etc/profile">3.vi /etc/profile</h3>
<pre><code><span class="keyword">export</span> HADOOP_HOME=/home/grid/hadoop-<span class="number">1.2</span>.<span class="number">1</span>
</code></pre><h3 id="4-vi_/etc/profile">4.vi /etc/profile</h3>
<pre><code>export <span class="constant">PATH</span>=<span class="regexp">/home/grid</span><span class="regexp">/apache-ant-1.9.3/bin</span><span class="symbol">:</span><span class="variable">$PATH</span>
</code></pre><h3 id="5-vi_/home/grid/hadoop-1-2-1/conf/hadoop-env-sh">5.vi /home/grid/hadoop-1.2.1/conf/hadoop-env.sh</h3>
<pre><code>export <span class="constant">HADOOP_CLASSPATH</span>=<span class="regexp">/home/grid</span><span class="regexp">/hadoop-1.2.1/myclass</span>
</code></pre><h3 id="6-ant_测试">6.ant 测试</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ <span class="keyword">pwd</span>
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ <span class="keyword">ls</span>
build.xml  src
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ ant
Buildfile: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build.xml

compile:
[mkdir] Created dir: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build
[javac] Compiling <span class="number">1</span> <span class="keyword">source</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build
[jar] Building jar: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/HDFSJavaAPI.jar

BUILD SUCCESSFUL
Total time: <span class="number">7</span> seconds
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ <span class="keyword">ls</span>
build  build.xml  HDFSJavaAPI.jar  src
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ hadoop jar HDFSJavaAPI.jar  HDFSJavaAPIDemo
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

hdfs:<span class="comment">//hadoop01.myhadoop.com:9000</span>
File <span class="keyword">exists</span>.
Welcome to HDFS Java API!!!
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ 
</code></pre><h3 id="7、ant_print-cp">7、ant print-cp</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ ant <span class="keyword">print</span>-cp
Buildfile: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build.xml

<span class="keyword">print</span>-cp:
[echo] classpath= /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/asm-<span class="number">3.2</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/aspectjrt-<span class="number">1.6</span><span class="number">.11</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/aspectjtools-<span class="number">1.6</span><span class="number">.11</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-beanutils-<span class="number">1.7</span><span class="number">.0</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-beanutils-core-<span class="number">1.8</span><span class="number">.0</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-cli-<span class="number">1.2</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-codec-<span class="number">1.4</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-collections-<span class="number">3.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-configuration-<span class="number">1.6</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-daemon-<span class="number">1.0</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-digester-<span class="number">1.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-el-<span class="number">1.0</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-httpclient-<span class="number">3.0</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-io-<span class="number">2.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-lang-<span class="number">2.4</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-logging-<span class="number">1.1</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-logging-api-<span class="number">1.0</span><span class="number">.4</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-math-<span class="number">2.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-net-<span class="number">3.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/core-<span class="number">3.1</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hadoop-capacity-scheduler-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hadoop-fairscheduler-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hadoop-thriftfs-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hsqldb-<span class="number">1.8</span><span class="number">.0</span><span class="number">.10</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-core-asl-<span class="number">1.8</span><span class="number">.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-mapper-asl-<span class="number">1.8</span><span class="number">.8</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jasper-compiler-<span class="number">5.5</span><span class="number">.12</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jasper-runtime-<span class="number">5.5</span><span class="number">.12</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jdeb-<span class="number">0.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jersey-core-<span class="number">1.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jersey-json-<span class="number">1.8</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jersey-server-<span class="number">1.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jets3t-<span class="number">0.6</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jetty-<span class="number">6.1</span><span class="number">.26</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jetty-util-<span class="number">6.1</span><span class="number">.26</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jsch-<span class="number">0.1</span><span class="number">.42</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jsp-<span class="number">2.1</span>/jsp-<span class="number">2.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jsp-<span class="number">2.1</span>/jsp-api-<span class="number">2.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/junit-<span class="number">4.5</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/kfs-<span class="number">0.2</span><span class="number">.2</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/log4j-<span class="number">1.2</span><span class="number">.15</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/mockito-all-<span class="number">1.8</span><span class="number">.5</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/oro-<span class="number">2.0</span><span class="number">.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/servlet-api-<span class="number">2.5</span>-<span class="number">20081211.</span>jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/slf4j-api-<span class="number">1.4</span><span class="number">.3</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/slf4j-log4j12-<span class="number">1.4</span><span class="number">.3</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/xmlenc-<span class="number">0.52</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter1/build/lib/hadoop-cookbook-chapter1.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter10/C10Samples.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/HDFSJavaAPI.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter3/build/lib/hadoop-cookbook-chapter1.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter3/build/lib/hadoop-cookbook-chapter3.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter4/C4LogProcessor.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter5/build/lib/hadoop-cookbook.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter6/build/lib/hadoop-cookbook-chapter6.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter8/build/lib/hadoop-cookbook-chapter6.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter8/build/lib/hadoop-cookbook-chapter8.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter9/C9Samples.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/datajoin/hadoop-datajoin-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/failmon/hadoop-failmon-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/gridmix/hadoop-gridmix-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/hdfsproxy/hdfsproxy-<span class="number">2.0</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/index/hadoop-index-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/streaming/hadoop-streaming-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/vaidya/hadoop-vaidya-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-ant-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-client-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-core-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-examples-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-minicluster-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-test-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-tools-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivy-<span class="number">2.1</span><span class="number">.0</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/thriftfs/lib/hadoopthriftapi.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/thriftfs/lib/libthrift.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/ftplet-api-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/ftpserver-core-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/ftpserver-server-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/mina-core-<span class="number">2.0</span><span class="number">.0</span>-M2-<span class="number">20080407.124109</span>-<span class="number">12.</span>jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/org/apache/hadoop/mapred/test.jar

BUILD SUCCESSFUL
Total time: <span class="number">2</span> seconds
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ 
</code></pre><h3 id="8、copy_到_/home/grid/-bashrc_设置_CLASSPATH">8、copy 到 /home/grid/.bashrc 设置 CLASSPATH</h3>
<p>这一步主要为了编译的时候不指定jar包把刚才的classpath加进去</p>
<pre><code>export CLASSPATH<span class="subst">=</span><span class="variable">$CLASSPATH</span>:刚才ant<span class="attribute">-print</span><span class="attribute">-cp</span>出来的内容
</code></pre><h3 id="9、FileSystemCat-java">9、FileSystemCat.java</h3>
<pre><code>import java<span class="preprocessor">.io</span><span class="preprocessor">.InputStream</span><span class="comment">;</span>
import java<span class="preprocessor">.net</span><span class="preprocessor">.URI</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.conf</span><span class="preprocessor">.Configuration</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.fs</span><span class="preprocessor">.FileSystem</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.fs</span><span class="preprocessor">.Path</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IOUtils</span><span class="comment">;</span>

public class FileSystemCat {
        public static void main(String[] args) throws Exception {
                String uri = args[<span class="number">0</span>]<span class="comment">;</span>
                Configuration conf = new Configuration()<span class="comment">;</span>
                FileSystem fs = FileSystem<span class="preprocessor">.get</span>(URI<span class="preprocessor">.create</span>(uri), conf)<span class="comment">;</span>
                InputStream <span class="keyword">in</span> = null<span class="comment">;</span>
                try {
                        <span class="keyword">in</span> = fs<span class="preprocessor">.open</span>(new Path(uri))<span class="comment">;</span>
                        IOUtils<span class="preprocessor">.copyBytes</span>(<span class="keyword">in</span>, System<span class="preprocessor">.out</span>, <span class="number">4096</span>, false)<span class="comment">;</span>
                } finally {
                        IOUtils<span class="preprocessor">.closeStream</span>(<span class="keyword">in</span>)<span class="comment">;</span>
                }
        }
}
</code></pre><h3 id="10、编译运行">10、编译运行</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">pwd</span>
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">ls</span>
FileSystemCat.class  FileSystemCat.java
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ rm -rf FileSystemCat.class 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">ls</span>
FileSystemCat.java
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ javac FileSystemCat.java 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ hadoop FileSystemCat hdfs:<span class="comment">//hadoop01.myhadoop.com:9000/user/grid/in/test1.txt</span>
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

hello world
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HDFS fs常用命令]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_fs/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_fs/</id>
    <published>2014-04-18T11:34:54.000Z</published>
    <updated>2014-04-23T11:52:46.000Z</updated>
    <content type="html"><![CDATA[<h4 id="好记性不如烂笔头">好记性不如烂笔头</h4>
<h5 id="首先记下hadoop常用的命令：（hadoop_fs_-help列出全部）">首先记下hadoop常用的命令：（hadoop fs -help列出全部）</h5>
<h6 id="1_hadoop_fs_–fs_[local_|_<file_system_URI>]：">1 hadoop fs –fs [local | <file system URI>]：</h6>
<p>声明hadoop使用的文件系统，如果不声明的话，使用当前配置文件配置的，按如下顺序查找：hadoop jar里的hadoop-default.xml-&gt;$HADOOP_CONF_DIR下的hadoop-default.xml-&gt;$HADOOP_CONF_DIR下的hadoop-site.xml。使用local代表将本地文件系统作为hadoop的DFS。如果传递uri做参数，那么就是特定的文件系统作为DFS。</p>
<h6 id="2_hadoop_fs_–ls_<path>：">2 hadoop fs –ls <path>：</h6>
<p>等同于本地系统的ls，列出在指定目录下的文件内容，支持pattern匹配。输出格式如filename(full path)   <r n>  size.其中n代表replica的个数，size代表大小（单位bytes）。</p>
<h6 id="3_hadoop_fs_–lsr_<path>：">3 hadoop fs –lsr <path>：</h6>
<p>递归列出匹配pattern的文件信息，类似ls，只不过递归列出所有子目录信息。</p>
<h6 id="4_hadoop_fs_–du_<path>：">4 hadoop fs –du <path>：</h6>
<p>列出匹配pattern的指定的文件系统空间总量（单位bytes），等价于unix下的针对目录的du –sb <path>/*和针对文件的du –b <path> ，输出格式如name(full path)  size(in bytes)。</p>
<h6 id="5_hadoop_fs_–dus_<path>：">5 hadoop fs –dus <path>：</h6>
<p>等价于-du，输出格式也相同，只不过等价于unix的du -sb。</p>
<h6 id="6_hadoop_fs_–mv_<src>_<dst>：">6 hadoop fs –mv <src> <dst>：</h6>
<p>将制定格式的文件 move到指定的目标位置。当src为多个文件时，dst必须是个目录。</p>
<h6 id="7_hadoop_fs_–cp_<src>_<dst>：">7 hadoop fs –cp <src> <dst>：</h6>
<p>拷贝文件到目标位置，当src为多个文件时，dst必须是个目录。</p>
<h6 id="8_hadoop_fs_–rm_[-skipTrash]_<src>：">8 hadoop fs –rm [-skipTrash] <src>：</h6>
<p>删除匹配pattern的指定文件，等价于unix下的rm <src>。</p>
<h6 id="9_hadoop_fs_–rmr_[skipTrash]_<src>：">9 hadoop fs –rmr [skipTrash] <src>：</h6>
<p>递归删掉所有的文件和目录，等价于unix下的rm –rf <src>。</p>
<h6 id="10_hadoop_fs_–rmi_[skipTrash]_<src>：">10 hadoop fs –rmi [skipTrash] <src>：</h6>
<p>等价于unix的rm –rfi <src>。</p>
<h6 id="11_hadoop_fs_–put_<localsrc>_…_<dst>：">11 hadoop fs –put <localsrc> … <dst>：</h6>
<p>从本地系统拷贝文件到DFS。</p>
<h6 id="12_hadoop_fs_–copyFromLocal_<localsrc>_…_<dst>：">12 hadoop fs –copyFromLocal <localsrc> … <dst>：</h6>
<p>等价于-put。</p>
<h6 id="13_hadoop_fs_–moveFromLocal_<localsrc>_…_<dst>：">13 hadoop fs –moveFromLocal <localsrc> … <dst>：</h6>
<p>等同于-put，只不过源文件在拷贝后被删除。</p>
<h6 id="14_hadoop_fs_–get_[-ignoreCrc]_[-crc]_<src>_<localdst>：">14 hadoop fs –get [-ignoreCrc] [-crc] <src> <localdst>：</h6>
<p>从DFS拷贝文件到本地文件系统，文件匹配pattern，若是多个文件，则dst必须是目录。</p>
<h6 id="15_hadoop_fs_–getmerge_<src>_<localdst>：">15 hadoop fs –getmerge <src> <localdst>：</h6>
<p>顾名思义，从DFS拷贝多个文件、合并排序为一个文件到本地文件系统。</p>
<h6 id="16_hadoop_fs_–cat_<src>：">16 hadoop fs –cat <src>：</h6>
<p>展示文件内容。</p>
<h6 id="17_hadoop_fs_–copyToLocal_[-ignoreCrc]_[-crc]_<src>_<localdst>：">17 hadoop fs –copyToLocal [-ignoreCrc] [-crc] <src> <localdst>：</h6>
<p>等价于-get。</p>
<h6 id="18_hadoop_fs_–mkdir_<path>：">18 hadoop fs –mkdir <path>：</h6>
<p>在指定位置创建目录。</p>
<h6 id="19_hadoop_fs_–setrep_[-R]_[-w]_<rep>_<path/file>：">19 hadoop fs –setrep [-R] [-w] <rep> <path/file>：</h6>
<p>设置文件的备份级别，-R标志控制是否递归设置子目录及文件。</p>
<h6 id="20_hadoop_fs_–chmod_[-R]_<MODE[,MODE]…|OCTALMODE>_PATH…：">20 hadoop fs –chmod [-R] <MODE[,MODE]…|OCTALMODE> PATH…：</h6>
<p>修改文件的权限，-R标记递归修改。MODE为a+r,g-w,+rwx等，OCTALMODE为755这样。</p>
<h6 id="21_hadoop_fs_-chown_[-R]_[OWNER][:[GROUP]]_PATH…：">21 hadoop fs -chown [-R] [OWNER][:[GROUP]] PATH…：</h6>
<p>修改文件的所有者和组。-R表示递归。</p>
<h6 id="22_hadoop_fs_-chgrp_[-R]_GROUP_PATH…：">22 hadoop fs -chgrp [-R] GROUP PATH…：</h6>
<p>等价于-chown … :GROUP …。</p>
<h6 id="23_hadoop_fs_–count[-q]_<path>：">23 hadoop fs –count[-q] <path>：</h6>
<p>计数文件个数及所占空间的详情，输出表格的列的含义依次为：DIR_COUNT,FILE_COUNT,CONTENT_SIZE,FILE_NAME或者如果加了-q的话，还会列出QUOTA,REMAINING_QUOTA,SPACE_QUOTA,REMAINING_SPACE_QUOTA。</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/HDFS/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用wordcount程序检验hadoop集群搭建是否成功]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/16/Hadoop/hadoop_error_2/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/16/Hadoop/hadoop_error_2/</id>
    <published>2014-04-16T09:17:35.000Z</published>
    <updated>2014-04-16T09:24:09.000Z</updated>
    <content type="html"><![CDATA[<h1 id="创建个input目录，里面有test1-txt、test2-txt">创建个input目录，里面有test1.txt、test2.txt</h1>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd input/
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ <span class="keyword">ls</span>
test1.txt  test2.txt
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ cat test1.txt 
hello world
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ cat test2.txt 
hello hadoop
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ hadoop fs -<span class="keyword">ls</span>
<span class="keyword">ls</span>: Cannot access .: No such <span class="keyword">file</span> or directory.
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ cd ~
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ <span class="keyword">ls</span>
Desktop    Downloads     hadoop-<span class="number">1.2</span><span class="number">.1</span>.tar.gz  Music     Public  Templates
Documents  hadoop-<span class="number">1.2</span><span class="number">.1</span>  input                Pictures  share   Videos
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd hadoop-<span class="number">1.2</span><span class="number">.1</span>
</code></pre><h1 id="将input的内容放到自定义目录in">将input的内容放到自定义目录in</h1>
<pre><code>[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -put ../input ./<span class="keyword">in</span>
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -ls
<span class="constant">Found</span> <span class="number">1</span> items
drwxr-xr-x   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -ls ./<span class="keyword">in</span>/*
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">12</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>/test1.txt
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">13</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>/test2.txt
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -cat ./<span class="keyword">in</span>/test1.txt
hello world
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>ls
bin          docs                          hadoop-test-<span class="number">1.2</span>.<span class="number">1</span>.jar   <span class="constant">LICENSE</span>.txt  src
build.xml    hadoop-ant-<span class="number">1.2</span>.<span class="number">1</span>.jar          hadoop-tools-<span class="number">1.2</span>.<span class="number">1</span>.jar  logs         tmp
c++          hadoop-client-<span class="number">1.2</span>.<span class="number">1</span>.jar       ivy                     <span class="constant">NOTICE</span>.txt   webapps
<span class="constant">CHANGES</span>.txt  hadoop-core-<span class="number">1.2</span>.<span class="number">1</span>.jar         ivy.xml                 <span class="constant">README</span>.txt
conf         hadoop-examples-<span class="number">1.2</span>.<span class="number">1</span>.jar     lib                     sbin
contrib      hadoop-minicluster-<span class="number">1.2</span>.<span class="number">1</span>.jar  libexec                 share
</code></pre><h1 id="运行wordcount程序，放到out目录">运行wordcount程序，放到out目录</h1>
<pre><code>[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop jar hadoop-examples-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>  wordcount <span class="keyword">in</span> <span class="keyword">out</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">51</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201404161644_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">52</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">07</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">50</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">09</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">17</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">33</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">20</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201404161644_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">29</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched reduce tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">20824</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Data-local map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">12323</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_READ=<span class="number">55</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">267</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">173563</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_WRITTEN=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output materialized bytes=<span class="number">61</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce shuffle bytes=<span class="number">61</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">8</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output bytes=<span class="number">41</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">336338944</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">3230</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine input records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">242</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input groups=<span class="number">3</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine output records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">422449152</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce output records=<span class="number">3</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">2173210624</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">4</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -ls ./<span class="keyword">out</span>
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span> /user/grid/<span class="keyword">out</span>/_SUCCESS
drwxr-xr-<span class="built_in">x</span>   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span> /user/grid/<span class="keyword">out</span>/_logs
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">25</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span> /user/grid/<span class="keyword">out</span>/part-r-<span class="number">00000</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -cat ./<span class="keyword">out</span>/part-r-<span class="number">00000</span>
hadoop  <span class="number">1</span>
hello   <span class="number">2</span>
world   <span class="number">1</span>
</code></pre><h1 id="浏览器访问：">浏览器访问：</h1>
<pre><code><span class="label">http:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="number">50070</span>
<span class="label">http:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="number">50030</span>
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
</feed>
