<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[DaMinger's blog]]></title>
  <subtitle><![CDATA[信春哥，系统稳，闭眼上线不回滚]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="https://github.com/DaMinger/DaMinger.github.io.git/"/>
  <updated>2014-04-16T09:25:36.544Z</updated>
  <id>https://github.com/DaMinger/DaMinger.github.io.git/</id>
  
  <author>
    <name><![CDATA[DaMinger]]></name>
    <email><![CDATA[564400632@qq.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[利用wordcount程序检验hadoop集群搭建是否成功]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/16/Hadoop/hadoop_error_2/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/16/Hadoop/hadoop_error_2/</id>
    <published>2014-04-16T09:17:35.000Z</published>
    <updated>2014-04-16T09:24:09.000Z</updated>
    <content type="html"><![CDATA[<h1 id="创建个input目录，里面有test1-txt、test2-txt">创建个input目录，里面有test1.txt、test2.txt</h1>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd input/
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ <span class="keyword">ls</span>
test1.txt  test2.txt
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ cat test1.txt 
hello world
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ cat test2.txt 
hello hadoop
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ hadoop fs -<span class="keyword">ls</span>
<span class="keyword">ls</span>: Cannot access .: No such <span class="keyword">file</span> or directory.
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ cd ~
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ <span class="keyword">ls</span>
Desktop    Downloads     hadoop-<span class="number">1.2</span><span class="number">.1</span>.tar.gz  Music     Public  Templates
Documents  hadoop-<span class="number">1.2</span><span class="number">.1</span>  input                Pictures  share   Videos
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd hadoop-<span class="number">1.2</span><span class="number">.1</span>
</code></pre><h1 id="将input的内容放到自定义目录in">将input的内容放到自定义目录in</h1>
<pre><code>[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -put ../input ./<span class="keyword">in</span>
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -ls
<span class="constant">Found</span> <span class="number">1</span> items
drwxr-xr-x   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -ls ./<span class="keyword">in</span>/*
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">12</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>/test1.txt
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">13</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>/test2.txt
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -cat ./<span class="keyword">in</span>/test1.txt
hello world
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>ls
bin          docs                          hadoop-test-<span class="number">1.2</span>.<span class="number">1</span>.jar   <span class="constant">LICENSE</span>.txt  src
build.xml    hadoop-ant-<span class="number">1.2</span>.<span class="number">1</span>.jar          hadoop-tools-<span class="number">1.2</span>.<span class="number">1</span>.jar  logs         tmp
c++          hadoop-client-<span class="number">1.2</span>.<span class="number">1</span>.jar       ivy                     <span class="constant">NOTICE</span>.txt   webapps
<span class="constant">CHANGES</span>.txt  hadoop-core-<span class="number">1.2</span>.<span class="number">1</span>.jar         ivy.xml                 <span class="constant">README</span>.txt
conf         hadoop-examples-<span class="number">1.2</span>.<span class="number">1</span>.jar     lib                     sbin
contrib      hadoop-minicluster-<span class="number">1.2</span>.<span class="number">1</span>.jar  libexec                 share
</code></pre><h1 id="运行wordcount程序，放到out目录">运行wordcount程序，放到out目录</h1>
<pre><code>[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop jar hadoop-examples-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>  wordcount <span class="keyword">in</span> <span class="keyword">out</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">51</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201404161644_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">52</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">07</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">50</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">09</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">17</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">33</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">20</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201404161644_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">29</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched reduce tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">20824</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Data-local map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">12323</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_READ=<span class="number">55</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">267</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">173563</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_WRITTEN=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output materialized bytes=<span class="number">61</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce shuffle bytes=<span class="number">61</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">8</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output bytes=<span class="number">41</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">336338944</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">3230</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine input records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">242</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input groups=<span class="number">3</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine output records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">422449152</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce output records=<span class="number">3</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">2173210624</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">4</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -ls ./<span class="keyword">out</span>
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span> /user/grid/<span class="keyword">out</span>/_SUCCESS
drwxr-xr-<span class="built_in">x</span>   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span> /user/grid/<span class="keyword">out</span>/_logs
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">25</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span> /user/grid/<span class="keyword">out</span>/part-r-<span class="number">00000</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -cat ./<span class="keyword">out</span>/part-r-<span class="number">00000</span>
hadoop  <span class="number">1</span>
hello   <span class="number">2</span>
world   <span class="number">1</span>
</code></pre><h1 id="浏览器访问：">浏览器访问：</h1>
<pre><code><span class="label">http:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="number">50070</span>
<span class="label">http:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="number">50030</span>
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hadoop报错：could only be replicated to 0 nodes, instead of 1 解决思路]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/15/Hadoop/hadoop_error_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/15/Hadoop/hadoop_error_1/</id>
    <published>2014-04-15T03:36:28.000Z</published>
    <updated>2014-04-15T04:07:09.000Z</updated>
    <content type="html"><![CDATA[<p>有时候做一些hadoop操作时候，例如上传文件、运行程序时<br>hadoop报错：could only be replicated to 0 nodes, instead of 1</p>
<p>其思路可以这样考虑一下</p>
<h1 id="一、优先考虑看下hadoop的日志，根据日志排错。">一、优先考虑看下hadoop的日志，根据日志排错。</h1>
<h1 id="二、如果不会看日志，注意以下几点做了没：">二、如果不会看日志，注意以下几点做了没：</h1>
<pre><code><span class="number">1</span>、看看各节点的防火墙关了没。
<span class="number">2</span>、这个问题是由于没有添加节点的原因，也就是说需要先启动namenode，
再启动datanode，然后启动jobtracker和tasktracker。
<span class="variable">$hadoop</span><span class="attribute">-daemon</span><span class="built_in">.</span>sh start namenode
<span class="variable">$hadoop</span><span class="attribute">-daemon</span><span class="built_in">.</span>sh start datanode
<span class="number">3</span>、查看配置文件   conf/hdfs<span class="attribute">-site</span><span class="built_in">.</span><span class="built_in">xml</span> 观察配置的域名和IP改了么。
<span class="number">4</span>、实验环境可以这样做，生产环境别这样做：
先把服务都停掉 stop<span class="attribute">-all</span><span class="built_in">.</span>sh
格式化namenode hadoop namenode <span class="attribute">-foramt</span>
重新启动所有服务 start<span class="attribute">-all</span><span class="built_in">.</span>sh
可以进行正常操作了
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[启动eth0 网卡时报错Error: Connection activation failed: Device not managed by NetworkManager]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Linux/linux_error_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Linux/linux_error_1/</id>
    <published>2014-04-13T02:44:29.000Z</published>
    <updated>2014-04-13T12:07:14.000Z</updated>
    <content type="html"><![CDATA[<p>Centos6.x下解决办法，在虚拟机中，启动eth0 网卡时报错：Error: Connection activation failed: Device not managed by NetworkManager。<br>解决办法：<br>    chkconfig NetworkManager off<br>    chkconfig network on<br>    service NetworkManager stop<br>    service network start<br>这是因为图形界面NetworkManager管理器与network服务有冲突，关上NetworkManager管理器既可<br>附：排查网络问题<br>基本的排错步骤（从上往下）<br>ping 127.0.0.1ping的通说明tcp协议栈没有问题<br>ping 主机地址 ping的通说明网卡没有问题<br>ping 路由器默认网关 ping的通说明包可以到达路由器<br>最后 ping DNS服务器地址<br>卡在哪一步，就查哪里</p>
]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="网卡错误" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E7%BD%91%E5%8D%A1%E9%94%99%E8%AF%AF/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用NFS+DNS+Shell脚本搭建Hadoop1.2.1]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Hadoop/hadoop_install/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Hadoop/hadoop_install/</id>
    <published>2014-04-13T01:24:45.000Z</published>
    <updated>2014-04-13T12:48:42.000Z</updated>
    <content type="html"><![CDATA[<p>今天按照几个帖子把hadoop1.2.1搭建了,生产环境使用hadoop区别与测试环境的主要是：<br>1.用DNS而不是hosts文件解析主机名<br>2.使用NFS共享密钥文件，而不是逐个手工拷贝添加密钥<br>3.复制Hadoop时使用批量拷贝脚本而不是逐台复制</p>
<h1 id="地址规划与安装环境">地址规划与安装环境</h1>
<h2 id="Ip地址规划">Ip地址规划</h2>
<pre><code>Ip地址             主机名                     hadoop角色                 其他用途
<span class="number">192.168</span><span class="number">.18</span><span class="number">.60</span>     hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>     namenode和jobtracker     DNS服务，NFS服务
<span class="number">192.168</span><span class="number">.18</span><span class="number">.61</span>     hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>     datanode和tasktracker
<span class="number">192.168</span><span class="number">.18</span><span class="number">.62</span>     hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>     datanode和tasktracker
</code></pre><h2 id="安装环境">安装环境</h2>
<pre><code>虚拟机软件 VMWare 
安装的操作系统 CentOS <span class="number">6.3</span> x64
java jdk <span class="number">1.7</span>_40 x64
hadoop <span class="number">1.2</span>.<span class="number">1</span>
<span class="built_in">bind</span> <span class="number">9.8</span>.<span class="number">2</span>
</code></pre><h1 id="安装系统">安装系统</h1>
<h2 id="安装CentOS_6-3">安装CentOS 6.3</h2>
<pre><code>安装后注意关闭防火墙：
service iptables <span class="keyword">stop</span>
chkconfig iptables <span class="keyword">off</span>
关闭selinux
vi /etc/selinux/config
将SELINUX=enforcing 改成SELINUX=disabled
</code></pre><h2 id="创建用户">创建用户</h2>
<pre><code>建立hadoop运行账号
在所有的节点创建hadoop运行账号
[root<span class="property">@hadoop01</span> ~]<span class="comment"># groupadd hadoop</span>
[root<span class="property">@hadoop01</span> ~]<span class="comment"># useradd -g hadoop grid --注意此处一定要指定分组，不然可能会不能建立互信</span>
[root<span class="property">@hadoop01</span> ~]<span class="comment"># id grid</span>
uid=<span class="number">501</span>(grid)gid=<span class="number">54326</span>(hadoop) groups=<span class="number">54326</span>(hadoop)
[root<span class="property">@hadoop01</span> ~]<span class="comment"># passwd grid</span>
Changingpassword <span class="keyword">for</span> user grid.
New <span class="attribute">UNIXpassword</span>:
BAD <span class="attribute">PASSWORD</span>: itis too short
Retype <span class="keyword">new</span> <span class="attribute">UNIXpassword</span>:
<span class="attribute">passwd</span>: allauthentication tokens updated successfully.
说明：在大的hadoop集群安装环境中，这步可以在批量安装linux系统之前完成，然后再进行系统的复制。（没
有试过，据说可以ghost工具软件应该可以实现）    
</code></pre><h2 id="安装JDK">安装JDK</h2>
<pre><code>[root<span class="variable">@hadoop01</span> grid]<span class="comment"># tar -zxvf jdk-7u40-linux-x64.tar.gz</span>
[root<span class="variable">@hadoop01</span> grid]<span class="comment"># mv jdk1.7.0_40/ /usr/</span>
</code></pre><h2 id="环境变量">环境变量</h2>
<pre><code>[root@hadoop01 etc]<span class="comment"># vi /etc/profile</span>
<span class="keyword">export</span> JAVA_HOME=/usr/jdk1.<span class="number">7.0</span>_40
<span class="keyword">export</span> HADOOP_FREFIX=/home/grid/hadoop-<span class="number">1.2</span>.<span class="number">1</span>
<span class="keyword">export</span> HADOOP_COMMON_HOME=<span class="variable">${HADOOP_FREFIX}</span>
<span class="keyword">export</span> HADOOP_CONF_DIR=<span class="variable">${HADOOP_FREFIX}</span>/conf
<span class="keyword">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar
<span class="keyword">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">${HADOOP_FREFIX}</span>/bin:<span class="variable">${HADOOP_FREFIX}</span>/sbin:<span class="variable">$PATH</span>
[root@hadoop01 etc]<span class="comment"># source /etc/profile</span>
</code></pre><h2 id="安装本地yum源">安装本地yum源</h2>
<p><a href="http://daminger.github.io/2014/04/09/Linux/linux_yum/" target="_blank">参考我的另外一篇博客</a></p>
<h1 id="安装DNS">安装DNS</h1>
<h2 id="从本地源安装">从本地源安装</h2>
<pre><code>[root<span class="variable">@hadoop01</span> ~]<span class="comment"># yum install bind_libs bind bind-utils</span>
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
Setting up Install Process
No <span class="keyword">package</span> bind_libs available.
Package <span class="number">32</span>:<span class="keyword">bind</span>-utils-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 already installed <span class="keyword">and</span> latest version
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package <span class="keyword">bind</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 will be installed
--&gt; Finished Dependency Resolution
Dependencies Resolved
=========================================================================
=======
Package Arch Version Repository Size
=========================================================================
=======
Installing:
<span class="keyword">bind</span> x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 Media <span class="number">4.0</span> M
Transaction Summary
=========================================================================
=======
Install <span class="number">1</span> Package(<span class="keyword">s</span>)
Total download size: <span class="number">4.0</span> M
Installed size: <span class="number">7.2</span> M
Is this ok [<span class="regexp">y/N]: y
Downloading Packages:
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
Warning: RPMDB altered outside of yum.
Installing : 32:bind-9.8.2-0.10.rc1.el6.x86_64 1/1
/sbin</span><span class="regexp">/ldconfig: /usr</span><span class="regexp">/lib64/libhdfs</span>.so.<span class="number">0</span> is <span class="keyword">not</span> a symbolic <span class="keyword">link</span>
/sbin/ldconfig: <span class="regexp">/usr/lib</span>64/libhadoop.so.<span class="number">1</span> is <span class="keyword">not</span> a symbolic <span class="keyword">link</span>
Verifying : <span class="number">32</span>:<span class="keyword">bind</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 <span class="number">1</span>/<span class="number">1</span>
Installed:
<span class="keyword">bind</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6
Complete!
[root<span class="variable">@hadoop01</span> ~]<span class="comment"># yum install bind_libs bind bind-utils bind-chroot</span>
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
Setting up Install Process
No <span class="keyword">package</span> bind_libs available.
Package <span class="number">32</span>:<span class="keyword">bind</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 already installed <span class="keyword">and</span> latest version
Package <span class="number">32</span>:<span class="keyword">bind</span>-utils-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 already installed <span class="keyword">and</span> latest version
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package <span class="keyword">bind</span>-<span class="keyword">chroot</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 will be installed
--&gt; Finished Dependency Resolution
Dependencies Resolved
=========================================================================
=======
Package Arch Version Repository Size
=========================================================================
=======
Installing:
<span class="keyword">bind</span>-<span class="keyword">chroot</span> x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 Media <span class="number">70</span> k
Transaction Summary
=========================================================================
=======
Install <span class="number">1</span> Package(<span class="keyword">s</span>)
Total download size: <span class="number">70</span> k
Installed size: <span class="number">0</span>
Is this ok [<span class="regexp">y/N]: y
Downloading Packages:
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
Installing : 32:bind-chroot-9.8.2-0.10.rc1.el6.x86_64 1/1
Verifying : 32:bind-chroot-9.8.2-0.10.rc1.el6.x86_64 1/</span><span class="number">1</span>
Installed:
<span class="keyword">bind</span>-<span class="keyword">chroot</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6
Complete!
安装<span class="keyword">bind</span>-<span class="keyword">chroot</span>，将<span class="keyword">bind</span>的根目录限定在某一个目录之中，增加系统安全性。
[root<span class="variable">@hadoop01</span> ~]<span class="comment"># yum install bind_libs bind bind-utils</span>
检查安装情况：
[root<span class="variable">@hadoop01</span> named]<span class="comment"># rpm -qa bind</span>
<span class="keyword">bind</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64
root<span class="variable">@hadoop01</span> named]<span class="comment"># rpm -qa bind-chroot</span>
<span class="keyword">bind</span>-<span class="keyword">chroot</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64
[root<span class="variable">@hadoop01</span> named]<span class="comment"># rpm -qa bind-utils</span>
<span class="keyword">bind</span>-utils-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64
将<span class="string">"<span class="variable">$AddUnixListenSocket</span> /var/named/chroot/dev/log"</span>加入/etc/rsyslog.conf文件中，不然rsyslog守护程序将无法记载
<span class="keyword">bind</span>日志。
[root<span class="variable">@hadoop01</span> etc]<span class="comment"># vi rsyslog.conf</span>
<span class="variable">$AddUnixListenSocket</span> /var/named/<span class="keyword">chroot</span>/dev/<span class="keyword">log</span>
</code></pre><h2 id="配置/etc/named-conf">配置/etc/named.conf</h2>
<pre><code>[root<span class="variable">@hadoop01</span> etc]<span class="comment"># vi named.conf</span>
options {
listen-on port <span class="number">53</span> { any; }; 把localhost改为any
listen-on-v6 port <span class="number">53</span> { <span class="symbol">:</span><span class="symbol">:</span><span class="number">1</span>; };
directory <span class="string">"/var/named"</span>;
dump-file <span class="string">"/var/named/data/cache_dump.db"</span>;
statistics-file <span class="string">"/var/named/data/named_stats.txt"</span>;
memstatistics-file <span class="string">"/var/named/data/named_mem_stats.txt"</span>;
allow-query { any; }; 把localhost改为any
recursion yes;
dnssec-enable yes;
dnssec-validation yes;
dnssec-lookaside auto;
<span class="regexp">/* Path to ISC DLV key */</span>
bindkeys-file <span class="string">"/etc/named.iscdlv.key"</span>;
managed-keys-directory <span class="string">"/var/named/dynamic"</span>;
};
logging {
channel default_debug {
file <span class="string">"data/named.run"</span>;
severity dynamic;
};
};
<span class="regexp">//zone</span> <span class="string">"."</span> <span class="constant">IN</span> {
<span class="regexp">//</span> type hint;
<span class="regexp">//</span> file <span class="string">"named.ca"</span>;
<span class="regexp">//</span> };
<span class="keyword">include</span> <span class="string">"/etc/named.rfc1912.zones"</span>;
<span class="regexp">//include</span> <span class="string">"/etc/named.root.key"</span>; 注释掉
</code></pre><h2 id="配置named-rfc1912-zones">配置named.rfc1912.zones</h2>
<pre><code>[root@hadoop01 etc]# vi named.rfc1912.zones
修改为以下内容：
zone <span class="string">"myhadoop.com"</span> IN {
<span class="class"><span class="keyword">type</span> <span class="title">master</span>;</span>
file <span class="string">"myhadoop.com.zone"</span>;
allow-update { none; };
};

zone <span class="string">"18.168.192.in-addr.arpa"</span> IN {
<span class="class"><span class="keyword">type</span> <span class="title">master</span>;</span>
file <span class="string">"18.168.192.zone"</span>;
allow-update { none; };
};
</code></pre><h2 id="创建区域文件">创建区域文件</h2>
<pre><code>需要创建正向记录文件myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>和反向记录文件<span class="number">18.168</span><span class="number">.192</span><span class="preprocessor">.in</span>-addr<span class="preprocessor">.zon</span>，文件应
在/var/named/chroot/var/named目录下
[root@hadoop01 named]<span class="preprocessor"># vi myhadoop.com.zone</span>
$TTL <span class="number">1</span>D
@ <span class="keyword">IN</span> SOA hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. root<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. (
<span class="number">20140221</span> <span class="comment">; serial</span>
<span class="number">1</span>D <span class="comment">; refresh</span>
<span class="number">1</span>H <span class="comment">; retry</span>
<span class="number">1</span>W <span class="comment">; expire</span>
<span class="number">3</span>H ) <span class="comment">; minimum</span>
@ <span class="keyword">IN</span> NS hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>.
hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. <span class="keyword">IN</span> A <span class="number">192.168</span><span class="number">.18</span><span class="number">.60</span>
hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. <span class="keyword">IN</span> A <span class="number">192.168</span><span class="number">.18</span><span class="number">.61</span>
hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. <span class="keyword">IN</span> A <span class="number">192.168</span><span class="number">.18</span><span class="number">.62</span>
[root@hadoop01 named]<span class="preprocessor"># vi 18.168.192.zone</span>
$TTL <span class="number">86400</span>
@ <span class="keyword">IN</span> SOA hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. root<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. (
<span class="number">1997022700</span> <span class="comment">; Serial</span>
<span class="number">28800</span> <span class="comment">; Refresh</span>
<span class="number">14400</span> <span class="comment">; Retry</span>
<span class="number">3600000</span> <span class="comment">; Expire</span>
<span class="number">86400</span> ) <span class="comment">; Minimum</span>
@ <span class="keyword">IN</span> NS hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>.
<span class="number">60</span> <span class="keyword">IN</span> PTR hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">61</span> <span class="keyword">IN</span> PTR hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">62</span> <span class="keyword">IN</span> PTR hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
[root@hadoop01 named]<span class="preprocessor"># chgrp named *</span>
[root@hadoop01 named]<span class="preprocessor"># ls -l</span>
total <span class="number">8</span>
-rwxr-xr-<span class="built_in">x</span> <span class="number">1</span> root named <span class="number">532</span> Feb <span class="number">21</span> <span class="number">12</span>:<span class="number">23</span> <span class="number">18.168</span><span class="number">.192</span><span class="preprocessor">.zone</span>
-rwxr-xr-<span class="built_in">x</span> <span class="number">1</span> root named <span class="number">336</span> Feb <span class="number">21</span> <span class="number">11</span>:<span class="number">38</span> myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>
</code></pre><h2 id="修改各节点/etc/resolv-conf文件">修改各节点/etc/resolv.conf文件</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># vi /etc/resolv.conf</span>
nameserver <span class="number">192.168</span>.<span class="number">18.60</span>
</code></pre><h2 id="检查">检查</h2>
<pre><code>[root@hadoop01 named]<span class="preprocessor"># named-checkzone node.myhadoop.com /var/named/chroot/var/named/myhadoop.com.zone</span>
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">9</span>: ignoring <span class="keyword">out</span>-of-zone data (hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">10</span>: ignoring <span class="keyword">out</span>-of-zone data (hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">11</span>: ignoring <span class="keyword">out</span>-of-zone data (hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">12</span>: ignoring <span class="keyword">out</span>-of-zone data (node3<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
zone node<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="keyword">IN</span>: loaded serial <span class="number">20140221</span>
OK
[root@hadoop01 named]<span class="preprocessor"># named-checkzone 192.168.18.60 /var/named/chroot/var/named/18.168.192.zone</span>
zone <span class="number">192.168</span><span class="number">.18</span><span class="number">.60</span>/<span class="keyword">IN</span>: loaded serial <span class="number">1997022700</span>
OK
</code></pre><h2 id="启动服务">启动服务</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># service named restart</span>
检查：
[root<span class="variable">@hadoop01</span> named]<span class="comment"># nslookup hadoop01.myhadoop.com</span>
</code></pre><h2 id="配置开机自启动">配置开机自启动</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig named on</span>
</code></pre><h1 id="配置/安装NFS">配置/安装NFS</h1>
<h2 id="检查-1">检查</h2>
<pre><code>[root@hadoop01 named]<span class="preprocessor"># rpm -qa |grep nfs</span>
nfs-utils-<span class="number">1.2</span><span class="number">.3</span>-<span class="number">26.</span>el6<span class="preprocessor">.x</span>86_64
nfs4-acl-tools-<span class="number">0.3</span><span class="number">.3</span>-<span class="number">6.</span>el6<span class="preprocessor">.x</span>86_64
nfs-utils-lib-<span class="number">1.1</span><span class="number">.5</span>-<span class="number">4.</span>el6<span class="preprocessor">.x</span>86_64
说明nfs已安装，如果没有安装可通过命令：
yum install nfs-utils
</code></pre><h2 id="启动服务-1">启动服务</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># service rpcbind restart</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># service nfs restart</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># service nfslock restart</span>
</code></pre><h2 id="配置开机自启动-1">配置开机自启动</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig rpcbind on</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig nfs on</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig nfslock on</span>
</code></pre><h2 id="设置共享目录">设置共享目录</h2>
<pre><code>[root<span class="variable">@hadoop01</span> ~]<span class="comment"># vi /etc/exports</span>
增加行：
/home/grid /share *(insecure,rw,async,no_root_squash)
说明：
/home/grid/share 是<span class="constant">NFS</span>要共享的目录
* 代表所有ip地址
rw为读写，ro为只读
<span class="constant">Sync</span>为立刻写入硬盘，rsync为优先写入缓存
<span class="constant">No_root_squas</span> root用户具有根目录的完全管理访问权限（这个如果不配置会造成远程root用户只读）
重启nfs服务：
[root<span class="variable">@hadoop01</span> ~]<span class="comment"># service nfs restart</span>
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">daemon:</span> [ <span class="constant">OK</span> ]
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">mountd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">quotas:</span> [ <span class="constant">OK</span> ]
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">services:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">services:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">quotas:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">mountd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Stopping</span> <span class="constant">RPC</span> <span class="symbol">idmapd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">RPC</span> <span class="symbol">idmapd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">daemon:</span> [ <span class="constant">OK</span> ]
</code></pre><h2 id="配置挂载目录">配置挂载目录</h2>
<pre><code>显示本地挂载点，即hadoop01的共享目录：
[root<span class="property">@hadoop01</span> etc]<span class="comment"># showmount -e localhost</span>
Export list <span class="keyword">for</span> <span class="attribute">localhost</span>:
/home/grid /share *
[root<span class="property">@hadoop01</span> etc]<span class="comment"># mkdir /nfs_share</span>
[root<span class="property">@hadoop01</span> etc]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
[root<span class="property">@hadoop01</span> etc]<span class="comment"># cd /nfs_share/</span>
-<span class="attribute">bash</span>: <span class="attribute">cd</span>: <span class="regexp">/nfs_share/</span>: Permission denied
[root<span class="property">@hadoop01</span> etc]<span class="comment"># su - grid</span>
[grid<span class="property">@hadoop01</span> ~]$ cd <span class="regexp">/nfs_share/</span>
用同样的方法在其它客户端（hadoop02，hadoop03）上操作
hadoop02：
[root<span class="property">@hadoop02</span> ~]<span class="comment"># mkdir /nfs_share</span>
[root<span class="property">@hadoop02</span> ~]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
hadoop03
[root<span class="property">@hadoop03</span> ~]<span class="comment"># mkdir /nfs_share</span>
[root<span class="property">@hadoop03</span> ~]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
</code></pre><h2 id="设置开机后自动挂载nfs共享目录">设置开机后自动挂载nfs共享目录</h2>
<pre><code>[root<span class="variable">@hadoop01</span> ~]<span class="comment"># vi /etc/fstab</span>
增加行：
<span class="number">192.168</span>.<span class="number">18.60</span><span class="symbol">:/home/grid/share</span> /nfs_share nfs defaults <span class="number">1</span> <span class="number">1</span>
相同方法设置hadoop02，hadoop03
</code></pre><h1 id="使用NFS做免密码登录">使用NFS做免密码登录</h1>
<p>　　我们把每个节点生成的RSA密钥对中的公钥整合到共享目录的authorized_keys文件中，这样做的好处是当我们有新的节点<br>接入时，不再需要分别向其它节点各自添加自己公钥信息，只需要把公钥信息追加到共享的authorized_keys公钥当中，其它节<br>点可直接访问最新的公钥文件。</p>
<h2 id="生成密钥对">生成密钥对</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>ssh-keygen -t rsa
在其他节点上生成密钥对
</code></pre><h2 id="整合authorized_keys">整合authorized_keys</h2>
<pre><code>[grid@hadoop01 <span class="preprocessor">.ssh</span>]$ <span class="keyword">cp</span> ~/<span class="preprocessor">.ssh</span>/id_rsa<span class="preprocessor">.pub</span> authorized_keys
[grid@hadoop01 <span class="preprocessor">.ssh</span>]$ ssh hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span> cat ~/<span class="preprocessor">.ssh</span>/id_rsa<span class="preprocessor">.pub</span> &gt;&gt; ~/<span class="preprocessor">.ssh</span>/authorized_keys
[grid@hadoop01 <span class="preprocessor">.ssh</span>]$ ssh hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span> cat ~/<span class="preprocessor">.ssh</span>/id_rsa<span class="preprocessor">.pub</span> &gt;&gt; ~/<span class="preprocessor">.ssh</span>/authorized_keys
</code></pre><h2 id="创建共享目录文件authorized_keys的软连接">创建共享目录文件authorized_keys的软连接</h2>
<pre><code>在hadoop02：
[grid<span class="variable">@hadoop02</span> .ssh]<span class="variable">$ </span>ln -s /nfs_share/.ssh/authorized_keys ~<span class="regexp">/.ssh/authorized</span>_keys
在hadoop03：
[grid<span class="variable">@hadoop03</span> .ssh]<span class="variable">$ </span>ln -s /nfs_share/.ssh/authorized_keys ~<span class="regexp">/.ssh/authorized</span>_keys
</code></pre><h2 id="验证">验证</h2>
<pre><code><span class="attr_selector">[grid@hadoop01 .ssh]</span>$ <span class="tag">ssh</span> <span class="tag">hadoop02</span><span class="class">.myhadoop</span><span class="class">.com</span>
<span class="tag">Last</span> <span class="tag">login</span>: <span class="tag">Fri</span> <span class="tag">Feb</span> 21 15<span class="pseudo">:40</span><span class="pseudo">:14</span> 2014 <span class="tag">from</span> 192<span class="class">.168</span><span class="class">.18</span><span class="class">.60</span>
<span class="attr_selector">[grid@hadoop02 ~]</span>$
</code></pre><h1 id="安装hadoop系统">安装hadoop系统</h1>
<p>请先在各节点安装jdk</p>
<h2 id="解压hadoop">解压hadoop</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>tar -zxvf hadoop-<span class="number">1.2</span>.<span class="number">1</span>.tar.gz
</code></pre><h2 id="修改配置文件">修改配置文件</h2>
<pre><code>在/home/grid/hadoop-1.2.1/conf下：
[grid@hadoop01 conf]$ vi hadoop-env.sh
修改jdk路径
export JAVA_HOME=/usr/jdk1.7.0_40
[grid@hadoop01 conf]$ vi core-site.xml
<span class="tag">&lt;<span class="title">configuration</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://hadoop01.myhadoop.com:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/grid/hadoop-1.2.1/tmp<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
[grid@hadoop01 conf]$ vi hdfs-site.xml
<span class="tag">&lt;<span class="title">configuration</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>2<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;/<span class="title">configuration</span>&gt;</span>
[grid@hadoop01 conf]$ vi mapred-site.xml
<span class="tag">&lt;<span class="title">configuration</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop01.myhadoop.com:9001<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;/<span class="title">configuration</span>&gt;</span>
[grid@hadoop01 conf]$ vi masters
hadoop01.myhadoop.com
[grid@hadoop01 conf]$ vi slaves
hadoop02.myhadoop.com
hadoop03.myhadoop.com
</code></pre><h2 id="使用awk生成分发脚本">使用awk生成分发脚本</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>cat slaves| awk <span class="string">'{print "scp -rp hadoop-1.2.1
grid@"$1":/home/grid"}'</span> &gt; cphadoop.sh
[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>cat cphadoop.sh
scp -rp hadoop-<span class="number">1.2</span>.<span class="number">1</span> grid<span class="variable">@hadoop02</span>.myhadoop.<span class="symbol">com:</span>/home/grid
scp -rp hadoop-<span class="number">1.2</span>.<span class="number">1</span> grid<span class="variable">@hadoop03</span>.myhadoop.<span class="symbol">com:</span>/home/grid
[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>chmod <span class="number">755</span> cphadoop.sh
</code></pre><h2 id="分发">分发</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>cd
[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>hadoop-<span class="number">1.2</span>.<span class="number">1</span>/conf/cphadoop.sh
</code></pre><h2 id="格式化namenode">格式化namenode</h2>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ bin/hadoop namenode -<span class="keyword">format</span>
</code></pre><h2 id="启动hadoop集群">启动hadoop集群</h2>
<pre><code>[grid@hadoop01 hadoop-1.2.1]$ bin/start-all.sh
[grid@hadoop01 hadoop-1.2.1]$ hadoop dfsadmin -report
Configured Capacity: 79413821440 (73.96 GB)
Present Capacity: 60468486174 (56.32 GB)
DFS Remaining: 60468428800 (56.32 GB)
DFS Used: 57374 (56.03 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
<span class="header">Missing blocks: 0
-------------------------------------------------</span>
Datanodes available: 2 (2 total, 0 dead)
Name: 192.168.18.61:50010
Decommission Status : Normal
Configured Capacity: 39706910720 (36.98 GB)
DFS Used: 28687 (28.01 KB)
Non DFS Used: 9218297841 (8.59 GB)
DFS Remaining: 30488584192(28.39 GB)
DFS Used%: 0%
DFS Remaining%: 76.78%
Last contact: Fri Feb 21 17:06:28 CST 2014
Name: 192.168.18.62:50010
Decommission Status : Normal
Configured Capacity: 39706910720 (36.98 GB)
DFS Used: 28687 (28.01 KB)
Non DFS Used: 9727037425 (9.06 GB)
DFS Remaining: 29979844608(27.92 GB)
DFS Used%: 0%
DFS Remaining%: 75.5%
Last contact: Fri Feb 21 17:06:27 CST 2014
</code></pre><h2 id="验证-1">验证</h2>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ jps
<span class="number">3560</span> SecondaryNameNode
<span class="number">3634</span> JobTracker
<span class="number">3788</span> Jps
<span class="number">3392</span> NameNode
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -mkdir test
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -<span class="keyword">ls</span>
Found <span class="number">1</span> items
drwxr-xr-x - <span class="keyword">grid</span> supergroup <span class="number">0</span> <span class="number">2014</span>-<span class="number">02</span>-<span class="number">21</span> <span class="number">17</span>:<span class="number">08</span> /user/<span class="keyword">grid</span>/test
</code></pre><h1 id="问题">问题</h1>
<h2 id="挂载问题">挂载问题</h2>
<pre><code>挂载共享目录出现无法挂载点问题
[root<span class="variable">@hadoop01</span> nfs_share]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
mount.<span class="symbol">nfs:</span> access denied by server <span class="keyword">while</span> mounting <span class="number">192.168</span>.<span class="number">18.60</span><span class="symbol">:/home/grid/share</span>
解决方法：
修改挂载属性
[root<span class="variable">@hadoop01</span> nfs_share]<span class="comment"># vi /etc/exports</span>
/home/grid/share *(insecure,rw,async,no_root_squash)
~
</code></pre><h2 id="访问问题">访问问题</h2>
<pre><code>配置完免密码登录后，用ssh登录远程机器每次都要求输入密码，原来是目录访问权限问题
/home/<span class="keyword">grid</span>/share目录与/home/<span class="keyword">grid</span>/share/.ssh目录的访问权限应为<span class="number">700</span>，
authorized_keys文件的访问权限应为<span class="number">644</span>
[root<span class="variable">@hadoop01</span> <span class="keyword">grid</span>]# chmod <span class="number">700</span> share
[root<span class="variable">@hadoop01</span> <span class="keyword">grid</span>]# cd share
[root<span class="variable">@hadoop01</span> share]# chmod <span class="number">700</span> .ssh
这是另外一篇的参考 它的共享目录是/home/<span class="keyword">grid</span> 
h1免密码登录h2：发现无法免密码登录，修改了/etc/ssh/sshd_config，将StrictModes改为no就可以了
一定要注意文件权限
/home/<span class="keyword">grid</span> <span class="number">700</span>
/home/<span class="keyword">grid</span>/.ssh <span class="number">700</span>
/home/<span class="keyword">grid</span>/.ssh/authorized_keys <span class="number">644</span>
</code></pre><h2 id="格式化namenode问题">格式化namenode问题</h2>
<pre><code>[grid@hadoop01 bin]$ ./hadoop namenode -format
14/02/21 16:44:54 INFO namenode.NameNode: STARTUP_MSG:
/<span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span>
STARTUP_MSG: Starting NameNode
STARTUP_MSG: host = java.net.UnknownHostException: hadoop01: hadoop01: Name or service not known
STARTUP_MSG: args = [-format]
STARTUP_MSG: version = 1.2.1
STARTUP_MSG: build = https://svn.apache.org/repos/asf ... branches/branch-1.2 -r 1503152; compiled by 'mattf' on
Mon Jul 22 15:23:09 PDT 2013
STARTUP<span class="emphasis">_MSG: java = 1.7.0_</span>40
解决方法：
hostname为hadoop01，系统没有解析，应把hostname设为hadoop01.myhadoop.com
在线修改
hostname hadoop01.myhadoop.com
永久保存：
vi /etc/sysconfig/network
HOSTNAME=hadoop01.myhadoop.com
其他节点也要修改
</code></pre><p>文章参考</p>
<p><a href="http://blog.itpub.net/26686207/viewspace-742502/" target="_blank">帖子1</a></p>
<p><a href="http://blog.itpub.net/26686207/viewspace-753510/" target="_blank">帖子2</a></p>
<p><a href="http://blog.csdn.net/lichangzai/article/details/8647242" target="_blank">帖子3</a></p>
<p><a href="http://f.dataguru.cn/thread-222047-1-1.html" target="_blank">帖子4</a></p>
<p><a href="http://f.dataguru.cn/thread-244415-1-1.html" target="_blank">帖子5</a></p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Centos 6.4克隆系统后网卡eth0变成eth1的解决办法]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/11/Linux/linux_ethX_problem/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/11/Linux/linux_ethX_problem/</id>
    <published>2014-04-11T14:31:24.000Z</published>
    <updated>2014-04-11T14:40:08.000Z</updated>
    <content type="html"><![CDATA[<p>vbox 或者vmware克隆系统后里面的eth0变成了eht1，开始没有发现问题，当我/etc/init.d/network restart重启的时候，发现找不到eth0，ifconfig 后只有eth1，原来linux使用udev动态管理设备文件，并根据设备的信息对其进行持久化命名。克隆时就自动变成eth1了。还好找到了解决办法，于是备份并分享之。</p>
<pre><code>vi /etc/udev/rules.d/70-persistent-net.rules
dd删除关于eth0的记录或者是用<span class="comment">#注释掉eth0的记录(谨慎的可以注释掉，不过完全没有用)</span>
记住其中eth1的mac地址 复制到文本中，备用
将eth1改成eth0
vi /etc/sysconfig/network-scripts/ifcfg-eth0
将老文件eth0的mac地址换掉，(如果细心的话会发现ifconfig eth1会跟配置文件中的mac不一样)
<span class="constant">DEVICE</span>=eth0
<span class="constant">HWADDR</span>=08:00:27:b7:bb:f7            //将备用的mac地址替换
<span class="constant">TYPE</span>=Ethernet
<span class="constant">UUID</span>=010da721-97c3-49f6-b95d-fe485634956a
<span class="constant">ONBOOT</span>=yes
<span class="constant">NM_CONTROLLED</span>=yes
<span class="constant">BOOTPROTO</span>=dhcp                       //自动IP拨号
<span class="constant">BOOTPROTO</span>=static                    //静态IP设置
<span class="constant">IPADDR</span>=192.168.1.111
<span class="constant">NETMASK</span>=255.255.255.0
<span class="constant">BROADCAST</span>=192.168.1.255
<span class="constant">GATEWAY</span>=192.168.1.1
/etc/init.d/network restart     //重启network--如果不行的话
就shutdown -r now//重启系统了
</code></pre>]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[三次握手，四次挥手]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/internet_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/10/计算机网络/internet_1/</id>
    <published>2014-04-10T11:47:37.000Z</published>
    <updated>2014-04-10T12:44:22.000Z</updated>
    <content type="html"><![CDATA[<h1 id="一、TCP报文格式">一、TCP报文格式</h1>
<p>下面是TCP报文格式图：<br><img src="/img/Internet/Internet_1.png" alt="图1 TCP报文格式"><br>上图中有几个字段需要重点介绍下：<br>（1）序号：Seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记。<br>（2）确认序号：Ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，Ack=Seq+1。<br>（3）标志位：共6个，即URG、ACK、PSH、RST、SYN、FIN等，具体含义如下：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（A）URG：紧急指针（urgent pointer）有效。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（B）ACK：确认序号有效。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（C）PSH：接收方应该尽快将这个报文交给应用层。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（D）RST：重置连接。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（E）SYN：发起一个新连接。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（F）FIN：释放一个连接。<br>需要注意的是：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（A）不要将确认序号Ack与标志位中的ACK搞混了。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（B）确认方Ack=发起方Req+1，两端配对。 </p>
<h1 id="二、三次握手">二、三次握手</h1>
<p>所谓三次握手（Three-Way Handshake）即建立TCP连接，就是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发，整个流程如下图所示：<br><img src="/img/Internet/Internet_2.png" alt="图2 TCP三次握手"><br>（1）第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。<br>（2）第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。<br>（3）第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。</p>
<p>SYN攻击：<br>在三次握手过程中，Server发送SYN-ACK之后，收到Client的ACK之前的TCP连接称为半连接（half-open connect），此时Server处于SYN_RCVD状态，当收到ACK后，Server转入ESTABLISHED状态。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。SYN攻击时一种典型的DDOS攻击，检测SYN攻击的方式非常简单，即当Server上有大量半连接状态且源IP地址是随机的，则可以断定遭到SYN攻击了，使用如下命令可以让之现行：</p>
<pre><code><span class="preprocessor">#netstat -nap | grep SYN_RECV</span>
</code></pre><h1 id="三、四次挥手">三、四次挥手</h1>
<p>三次握手耳熟能详，四次挥手估计就不太熟了，所谓四次挥手（Four-Way Wavehand）即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发，整个流程如下图所示<br><img src="/img/Internet/Internet_3.png" alt="图3 TCP四次挥手"></p>
<p>由于TCP连接时全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭，上图描述的即是如此。<br>（1）第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。<br>（2）第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。<br>（3）第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。<br>（4）第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。</p>
<p> 上面是一方主动关闭，另一方被动关闭的情况，实际中还会出现同时发起主动关闭的情况，具体流程如下图<br><img src="/img/Internet/Internet_4.png" alt="图4 同时挥手"></p>
<h1 id="四、附注">四、附注</h1>
<p>关于三次握手与四次挥手通常都会有典型的面试题，在此提出供有需求的同学们参考：<br>（1）三次握手是什么或者流程？四次握手呢？答案前面分析就是。<br>（2）为什么建立连接是三次握手，而关闭连接却是四次挥手呢？</p>
<p> 这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。        </p>
<p> <a href="http://blog.chinaunix.net/uid-7411781-id-3812206.html" target="_blank">   原文链接，请点击这里    </a></p>
]]></content>
    
    
      <category term="网络" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="计算机网络" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[在Linux CentOS6.3下编译安装MySQL5.6.10]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_mysql_install/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_mysql_install/</id>
    <published>2014-04-09T12:15:51.000Z</published>
    <updated>2014-04-13T12:02:26.000Z</updated>
    <content type="html"><![CDATA[<h2 id="编译安装MySQL前的准备工作">编译安装MySQL前的准备工作</h2>
<pre><code>安装编译源码所需的工具和库
yum install gcc gcc-c++ ncurses-devel perl
安装cmake，从http://www<span class="preprocessor">.cmake</span><span class="preprocessor">.org</span>下载源码并编译安装
wget http://www<span class="preprocessor">.cmake</span><span class="preprocessor">.org</span>/files/v2<span class="number">.8</span>/cmake-<span class="number">2.8</span><span class="number">.10</span><span class="number">.2</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
tar -xzvf cmake-<span class="number">2.8</span><span class="number">.10</span><span class="number">.2</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
cd cmake-<span class="number">2.8</span><span class="number">.10</span><span class="number">.2</span>
./bootstrap <span class="comment">; make ; make install</span>
cd ~
</code></pre><h2 id="设置MySQL用户和组">设置MySQL用户和组</h2>
<pre><code>新增mysql用户组
groupadd mysql
新增mysql用户
useradd <span class="attribute">-r</span> <span class="attribute">-g</span> mysql mysql
</code></pre><h2 id="新建MySQL所需要的目录">新建MySQL所需要的目录</h2>
<pre><code>新建mysql安装目录
mkdir <span class="attribute">-p</span> /usr/<span class="built_in">local</span>/mysql
新建mysql数据库数据文件目录
mkdir <span class="attribute">-p</span> /<span class="built_in">data</span>/mysqldb
</code></pre><h2 id="下载MySQL源码包并解压">下载MySQL源码包并解压</h2>
<pre><code>从http://dev<span class="preprocessor">.mysql</span><span class="preprocessor">.com</span>/downloads/mysql/直接下载源码
解压mysql-<span class="number">5.6</span><span class="number">.10</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
tar -zxv -f mysql-<span class="number">5.6</span><span class="number">.10</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
cd mysql-<span class="number">5.6</span><span class="number">.10</span>
</code></pre><h2 id="编译安装MySQL">编译安装MySQL</h2>
<pre><code>从mysql5.5起，mysql源码安装开始使用cmake了，设置源码编译配置脚本。
-<span class="ruby"><span class="constant">DCMAKE_INSTALL_PREFIX</span>=dir_name    设置mysql安装目录
</span>-<span class="ruby"><span class="constant">DMYSQL_UNIX_ADDR</span>=file_name    设置监听套接字路径，这必须是一个绝对路径名。默认为/tmp/mysql.sock的中。
</span>-<span class="ruby"><span class="constant">DDEFAULT_CHARSET</span>=charset_name    设置服务器的字符集。缺省情况下，<span class="constant">MySQL</span>使用latin1的（<span class="constant">CP1252</span>西欧）字符集。
</span>cmake/character_sets.cmake文件包含允许的字符集名称列表。
-<span class="ruby"><span class="constant">DDEFAULT_COLLATION</span>=collation_name    设置服务器的排序规则。
</span>-<span class="ruby"><span class="constant">DWITH_INNOBASE_STORAGE_ENGINE</span>=<span class="number">1</span>
</span>-<span class="ruby"><span class="constant">DWITH_ARCHIVE_STORAGE_ENGINE</span>=<span class="number">1</span>
</span>-<span class="ruby"><span class="constant">DWITH_BLACKHOLE_STORAGE_ENGINE</span>=<span class="number">1</span>
</span>-<span class="ruby"><span class="constant">DWITH_PERFSCHEMA_STORAGE_ENGINE</span>=<span class="number">1</span>    存储引擎选项：
</span>MyISAM，MERGE，MEMORY，和CSV引擎是默认编译到服务器中，并不需要明确地安装。
静态编译一个存储引擎到服务器，使用-DWITH_engine_STORAGE_ENGINE= 1。
可用的存储引擎值有：ARCHIVE, BLACKHOLE, EXAMPLE, FEDERATED, INNOBASE (InnoDB),   PARTITION (partitioning support), 和 PERFSCHEMA (Performance Schema).
-<span class="ruby"><span class="constant">DMYSQL_DATADIR</span>=dir_name    设置mysql数据库文件目录
</span>-<span class="ruby"><span class="constant">DMYSQL_TCP_PORT</span>=port_num    设置mysql服务器监听端口，默认为<span class="number">3306</span>
</span>-<span class="ruby"><span class="constant">DENABLE_DOWNLOADS</span>=bool    是否要下载可选的文件。例如，启用此选项（设置为<span class="number">1</span>），cmake将下载谷歌所使用的测试套件运行单元测试。
</span>设置编译参数
cmake \
-<span class="ruby"><span class="constant">DCMAKE_INSTALL_PREFIX</span>=<span class="regexp">/usr/local</span><span class="regexp">/mysql \
</span></span>-<span class="ruby"><span class="constant">DMYSQL_UNIX_ADDR</span>=<span class="regexp">/usr/local</span><span class="regexp">/mysql/mysql</span>.sock \
</span>-<span class="ruby"><span class="constant">DDEFAULT_CHARSET</span>=utf8 \
</span>-<span class="ruby"><span class="constant">DDEFAULT_COLLATION</span>=utf8_general_ci \
</span>-<span class="ruby"><span class="constant">DWITH_INNOBASE_STORAGE_ENGINE</span>=<span class="number">1</span> \
</span>-<span class="ruby"><span class="constant">DWITH_ARCHIVE_STORAGE_ENGINE</span>=<span class="number">1</span> \
</span>-<span class="ruby"><span class="constant">DWITH_BLACKHOLE_STORAGE_ENGINE</span>=<span class="number">1</span> \
</span>-<span class="ruby"><span class="constant">DMYSQL_DATADIR</span>=<span class="regexp">/data/mysqldb</span> \
</span>-<span class="ruby"><span class="constant">DMYSQL_TCP_PORT</span>=<span class="number">3306</span> \
</span>-<span class="ruby"><span class="constant">DENABLE_DOWNLOADS</span>=<span class="number">1</span>
</span>注：重新运行配置，需要删除CMakeCache.txt文件
rm CMakeCache.txt
编译源码
make
安装
make install
</code></pre><h2 id="修改mysql目录所有者和组">修改mysql目录所有者和组</h2>
<pre><code>修改mysql安装目录
cd /usr/<span class="built_in">local</span>/mysql
chown <span class="attribute">-R</span> mysql:mysql <span class="built_in">.</span>
修改mysql数据库文件目录
cd /<span class="built_in">data</span>/mysqldb
chown <span class="attribute">-R</span> mysql:mysql <span class="built_in">.</span>
</code></pre><h2 id="初始化mysql数据库">初始化mysql数据库</h2>
<pre><code><span class="comment">cd</span> <span class="comment">/usr/local/mysql</span>
<span class="comment">scripts/mysql_install_db</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">user=mysql</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">datadir=/data/mysqldb</span>
</code></pre><h2 id="复制mysql服务启动配置文件">复制mysql服务启动配置文件</h2>
<pre><code>cp /usr/<span class="keyword">local</span>/mysql/support-files/<span class="keyword">my</span>-default.cnf /etc/<span class="keyword">my</span>.cnf
注：如果/etc/<span class="keyword">my</span>.cnf文件存在，则覆盖。
</code></pre><h2 id="复制mysql服务启动脚本及加入PATH路径">复制mysql服务启动脚本及加入PATH路径</h2>
<pre><code>cp support-files/mysql.server /etc/init.d/mysqld
vim /etc/profile
<span class="constant">PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/mysql/bin</span><span class="symbol">:/usr/local/mysql/lib</span><span class="symbol">:</span><span class="variable">$PATH</span>
export <span class="constant">PATH</span>
source /etc/profile
</code></pre><h2 id="启动mysql服务并加入开机自启动">启动mysql服务并加入开机自启动</h2>
<pre><code>service mysqld <span class="operator"><span class="keyword">start</span>
chkconfig --<span class="keyword">level</span> <span class="number">35</span> mysqld <span class="keyword">on</span></span>
</code></pre><h2 id="检查mysql服务是否启动">检查mysql服务是否启动</h2>
<pre><code>netstat <span class="attribute">-tulnp</span> <span class="subst">|</span> grep <span class="number">3306</span>
mysql <span class="attribute">-u</span> root <span class="attribute">-p</span>
密码为空，如果能登陆上，则安装成功。
</code></pre><h2 id="修改MySQL用户root的密码">修改MySQL用户root的密码</h2>
<pre><code>mysqladmin <span class="attribute">-u</span> root password <span class="string">'123456'</span>
注：也可运行安全设置脚本，修改MySQL用户root的密码，同时可禁止root远程连接，移除test数据库和匿名用户。
/usr/<span class="built_in">local</span>/mysql/bin/mysql_secure_installation
</code></pre><h2 id="可能会出现的错误">可能会出现的错误</h2>
<pre><code>问题：
Starting MySQL..The server quit <span class="keyword">without</span> updating PID <span class="type">file</span> ([FAILED]/mysql/Server03.mylinux.com.pid).
解决：
修改/etc/<span class="keyword">my</span>.cnf 中datadir,指向正确的mysql数据库文件目录

问题：
ERROR <span class="number">2002</span> (HY000): Can't connect <span class="keyword">to</span> <span class="keyword">local</span> MySQL server <span class="keyword">through</span> socket '/tmp/mysql.sock' (<span class="number">2</span>)
解决：
新建一个链接或在mysql中加入-S参数，直接指出mysql.sock位置。
ln -s /usr/<span class="keyword">local</span>/mysql/data/mysql.sock /tmp/mysql.sock
/usr/<span class="keyword">local</span>/mysql/bin/mysql -u root -S /usr/<span class="keyword">local</span>/mysql/data/mysql.sock
</code></pre>]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="Mysql" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Mysql/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CENTOS 6.5搭建本地YUM源]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_yum/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_yum/</id>
    <published>2014-04-09T12:01:08.000Z</published>
    <updated>2014-04-13T12:04:03.000Z</updated>
    <content type="html"><![CDATA[<h1 id="CENTOS_6-5搭建本地YUM源">CENTOS 6.5搭建本地YUM源</h1>
<h2 id="方法一：用系统光盘镜像创建">方法一：用系统光盘镜像创建</h2>
<h3 id="创建yum源目录并挂载系统光盘镜像">创建yum源目录并挂载系统光盘镜像</h3>
<pre><code>mkdir /opt/<span class="built_in">local</span><span class="attribute">-mirror</span>
mount /dev/cdrom  /opt/<span class="built_in">local</span><span class="attribute">-mirror</span>
</code></pre><h3 id="新建local-mirror-repo文件">新建local-mirror.repo文件</h3>
<pre><code>cd /etc/yum.repos.d/
rm -f  *  (删掉原来的repo文件，如果以后还想用，也可以重命名或者移到其他目录，我这里删掉)
vi /etc/yum.repos.d/local-mirror.repo
[local-mirror]
name=This <span class="keyword">is</span> a local repo
baseurl=file:<span class="comment"><span class="xmlDocTag">///</span>opt/local-mirror</span>
enabled=<span class="number">1</span>
gpgcheck=<span class="number">1</span>
gpgkey=file:<span class="comment"><span class="xmlDocTag">///</span>etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6</span>
</code></pre><h3 id="设置开机自动挂载">设置开机自动挂载</h3>
<p>vi  /etc/fstab   //在最后一行加上：</p>
<pre><code>/dev/cdrom    /opt/local-mirror     iso9660        <span class="default"><span class="keyword">default</span>      0    0</span>
</code></pre><h3 id="测试">测试</h3>
<pre><code>yum clean <span class="literal">all</span>
yum  makecache
yum <span class="attribute">-y</span> install ftp(或者安装其他软件包)
</code></pre><p>如果没有提示错误，就说明本地yum源搭建成功了！</p>
<h2 id="方法二：将所有rpm包复制到指定文件夹，通过createrepo命令创建">方法二：将所有rpm包复制到指定文件夹，通过createrepo命令创建</h2>
<h3 id="创建yum源目录并且挂载系统光盘镜像之后，将所有rpm包复制到yum源目录，如果镜像文件像CentOS6-5一样是分为2个iso文件的，分2次挂载并且复制即可">创建yum源目录并且挂载系统光盘镜像之后，将所有rpm包复制到yum源目录，如果镜像文件像CentOS6.5一样是分为2个iso文件的，分2次挂载并且复制即可</h3>
<pre><code>mkdir /opt/<span class="keyword">local</span>-mirror
mount /dev/cdrom  /mnt (临时供<span class="keyword">copy</span>的目录）
cp /mnt/Packages/*   /opt/<span class="keyword">local</span>-mirror
</code></pre><h3 id="通过rpm命令手动安装createrepo命令创建yum仓库">通过rpm命令手动安装createrepo命令创建yum仓库</h3>
<pre><code>rpm <span class="attribute">-ivh</span> createrepo     (若提示需要安装依赖包，根据提示安装即可）
createrepo <span class="attribute">-v</span>  /opt/<span class="built_in">local</span><span class="attribute">-mirror</span>  (创建索引信息）
</code></pre><h3 id="同方法一的第二步">同方法一的第二步</h3>
<h3 id="测试同方法一的第四步">测试同方法一的第四步</h3>
]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="yum" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/yum/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[第一篇Blog都是扯淡]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/07/%E6%9D%82%E8%B0%88/first_blog/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/07/杂谈/first_blog/</id>
    <published>2014-04-07T04:18:40.000Z</published>
    <updated>2014-04-09T09:05:06.000Z</updated>
    <content type="html"><![CDATA[<p>所谓的第一篇文章都是扯淡，原先学习都是用OneNote来管理个人笔记，现在整个Blog分享点学习心得,共享知识，共同进步。</p>
]]></content>
    
    
      <category term="杂谈" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E6%9D%82%E8%B0%88/"/>
    
      <category term="杂谈" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
</feed>
