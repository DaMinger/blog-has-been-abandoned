<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[DaMinger's blog]]></title>
  <subtitle><![CDATA[信春哥，系统稳，闭眼上线不回滚]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="https://github.com/DaMinger/DaMinger.github.io.git/"/>
  <updated>2014-04-23T11:39:15.984Z</updated>
  <id>https://github.com/DaMinger/DaMinger.github.io.git/</id>
  
  <author>
    <name><![CDATA[DaMinger]]></name>
    <email><![CDATA[564400632@qq.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Pig的简单使用]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/23/Hadoop/hadoop_pig/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/23/Hadoop/hadoop_pig/</id>
    <published>2014-04-23T10:53:16.000Z</published>
    <updated>2014-04-23T11:38:12.000Z</updated>
    <content type="html"><![CDATA[<h5 id="安装配置pig">安装配置pig</h5>
<p>下载你对应hadoop的pig版本,解压</p>
<h5 id="本地模式">本地模式</h5>
<pre><code>配置环境变量
vi <span class="preprocessor">.bash</span>_profile
PATH=$PATH:/home/grid/pig-<span class="number">0.12</span><span class="number">.1</span>/bin:$HOME/bin
JAVA_HOME=/usr/jdk1<span class="number">.7</span><span class="number">.0</span>_51

export PATH
export JAVA_HOME
[grid@hadoop01 ~]$ pig -<span class="built_in">x</span> local
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">34</span>,<span class="number">815</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Apache Pig version <span class="number">0.12</span><span class="number">.1</span> (r1585011) compiled Apr <span class="number">05</span> <span class="number">2014</span>, <span class="number">01</span>:<span class="number">41</span>:<span class="number">34</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">34</span>,<span class="number">815</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Logging error messages to: /home/grid/pig_1398238654807<span class="preprocessor">.log</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">040</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.impl</span><span class="preprocessor">.util</span><span class="preprocessor">.Utils</span> - Default bootup file /home/grid/<span class="preprocessor">.pigbootup</span> not found
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">843</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.backend</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.executionengine</span><span class="preprocessor">.HExecutionEngine</span> - Connecting to hadoop file system at: file:///
grunt&gt; 
</code></pre><h5 id="mapreduce模式">mapreduce模式</h5>
<pre><code>配置环境变量
PATH=$PATH:/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>:/bin:/home/grid/pig-<span class="number">0.12</span><span class="number">.1</span>/bin:$HOME/bin
JAVA_HOME=/usr/jdk1<span class="number">.7</span><span class="number">.0</span>_51
PIG_CLASSPATH=/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/conf/

export PIG_CLASSPATH
export PATH
export JAVA_HOME
[grid@hadoop01 ~]$ cat /etc/hosts
<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>   localhost localhost<span class="preprocessor">.localdomain</span> localhost4 localhost4<span class="preprocessor">.localdomain</span>4
::<span class="number">1</span>         localhost localhost<span class="preprocessor">.localdomain</span> localhost6 localhost6<span class="preprocessor">.localdomain</span>6
<span class="number">192.168</span><span class="number">.255</span><span class="number">.151</span> hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">192.168</span><span class="number">.255</span><span class="number">.152</span> hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">192.168</span><span class="number">.255</span><span class="number">.153</span> hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>

[grid@hadoop01 ~]$ pig
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.

<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">06</span>,<span class="number">090</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Apache Pig version <span class="number">0.12</span><span class="number">.1</span> (r1585011) compiled Apr <span class="number">05</span> <span class="number">2014</span>, <span class="number">01</span>:<span class="number">41</span>:<span class="number">34</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">06</span>,<span class="number">090</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.Main</span> - Logging error messages to: /home/grid/pig_1398239586082<span class="preprocessor">.log</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">06</span>,<span class="number">171</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.impl</span><span class="preprocessor">.util</span><span class="preprocessor">.Utils</span> - Default bootup file /home/grid/<span class="preprocessor">.pigbootup</span> not found
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">07</span>,<span class="number">199</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.backend</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.executionengine</span><span class="preprocessor">.HExecutionEngine</span> - Connecting to hadoop file system at: hdfs://hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">53</span>:<span class="number">08</span>,<span class="number">712</span> [main] INFO  org<span class="preprocessor">.apache</span><span class="preprocessor">.pig</span><span class="preprocessor">.backend</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.executionengine</span><span class="preprocessor">.HExecutionEngine</span> - Connecting to map-reduce job tracker at: hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9001</span>
</code></pre><h5 id="简单操作，可以help查看">简单操作，可以help查看</h5>
<pre><code>grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/demo<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;       <span class="number">29</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>  &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">out</span> &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out2        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out3        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out4        &lt;dir&gt;
grunt&gt; cd <span class="keyword">in</span>
grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;      <span class="number">39564893</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>/test1<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;   <span class="number">12</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>/test2<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;   <span class="number">13</span>
grunt&gt; cat test1<span class="preprocessor">.txt</span>
hello world
</code></pre><h5 id="做两个实验">做两个实验</h5>
<h6 id="实验一：准备数据">实验一：准备数据</h6>
<pre><code>[grid@hadoop01 ~]$ cat score<span class="preprocessor">.txt</span> 
James,Network,Tiger,<span class="number">100</span>
James,Database,Tiger,<span class="number">99</span>
James,PDE,Yao,<span class="number">95</span>
Vincent,Network,Tiger,<span class="number">95</span>
Vincent,PDE,Yao,<span class="number">98</span>
Vincent,PDE,
NocWei,PDE,Yao,<span class="number">100</span>
grunt&gt; copyFromLocal  /home/grid/score<span class="preprocessor">.txt</span> pig/score<span class="preprocessor">.txt</span>
grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/demo<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;       <span class="number">29</span>
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">in</span>  &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/<span class="keyword">out</span> &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out2        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out3        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/out4        &lt;dir&gt;
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/pig &lt;dir&gt;
grunt&gt; cd pig
grunt&gt; ls
<span class="label">hdfs:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9000</span>/user/grid/pig/score<span class="preprocessor">.txt</span>&lt;r <span class="number">2</span>&gt;  <span class="number">141</span>
grunt&gt; cat score<span class="preprocessor">.txt</span>
James,Network,Tiger,<span class="number">100</span>
James,Database,Tiger,<span class="number">99</span>
James,PDE,Yao,<span class="number">95</span>
Vincent,Network,Tiger,<span class="number">95</span>
Vincent,PDE,Yao,<span class="number">98</span>
Vincent,PDE,
NocWei,PDE,Yao,<span class="number">100</span>
</code></pre><h6 id="任务目标：计算一名学生被多少老师教过(这里用到了Pig_Latin)">任务目标：计算一名学生被多少老师教过(这里用到了Pig Latin)</h6>
<p>先DISTINCT,再计数。-使用DISTINCT 能够对所有数据去重</p>
<pre><code>grunt&gt; A = LOAD 'score.txt' USING PigStorage(',') AS (student,course,teacher,score:int);
grunt&gt; DESCRIBE A;
A: {student: bytearray,course: bytearray,teacher: bytearray,score: int}
grunt&gt; B = FOREACH A GENERATE student,teacher;
grunt&gt; DESCRIBE B;
B: {student: bytearray,teacher: bytearray}
grunt&gt; C = DISTINCT B;
grunt&gt; D = GROUP C BY student
grunt&gt; D = FOREACH (GROUP C BY student ) GENERATE group AS student,COUNT(C);
grunt&gt; DUMP D;
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">076</span> [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used <span class="keyword">in</span> the script: GROUP_BY,DISTINCT
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">241</span> [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier]}
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">326</span> [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned <span class="keyword">for</span> A: $<span class="number">1</span>, $<span class="number">3</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">760</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: <span class="number">100</span> optimistic? <span class="keyword">false</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span>,<span class="number">866</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer - Choosing <span class="keyword">to</span> move algebraic foreach <span class="keyword">to</span> combiner
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">011</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: <span class="number">2</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">018</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: <span class="number">2</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">388</span> [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added <span class="keyword">to</span> the job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">453</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set <span class="keyword">to</span> <span class="keyword">default</span> <span class="number">0.3</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">463</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # <span class="keyword">of</span> required reducers.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">464</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">528</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=<span class="number">1000000000</span> maxReducers=<span class="number">999</span> totalInputFileSize=<span class="number">141</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">535</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism <span class="keyword">to</span> <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>,<span class="number">536</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job8437033109018168730.jar
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">11</span>,<span class="number">994</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job8437033109018168730.jar created
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">071</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">106</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is <span class="keyword">false</span>, will not generate code.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">106</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process <span class="keyword">to</span> move generated code <span class="keyword">to</span> distributed cache
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">121</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] <span class="keyword">with</span> classes <span class="keyword">to</span> deserialize []
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">126</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting identity combiner <span class="keyword">class</span>.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">386</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">1</span> map-reduce job(s) waiting <span class="keyword">for</span> submission.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">12</span>,<span class="number">892</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">0</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">591</span> [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">592</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">639</span> [JobControl] INFO  org.apache.hadoop.util.NativeCodeLoader - Loaded the native-hadoop library
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">639</span> [JobControl] WARN  org.apache.hadoop.io.compress.snappy.LoadSnappy - Snappy native library not loaded
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">13</span>,<span class="number">645</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201404231546_0001
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases A,B
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: A[<span class="number">1</span>,<span class="number">4</span>],B[-<span class="number">1</span>,-<span class="number">1</span>] C:  R: 
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">16</span>,<span class="number">303</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http:<span class="comment">//hadoop01.myhadoop.com:50030/jobdetails.jsp?jobid=job_201404231546_0001</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">32</span>,<span class="number">455</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">25</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">45</span>,<span class="number">009</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">50</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">152</span> [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added <span class="keyword">to</span> the job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">154</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set <span class="keyword">to</span> <span class="keyword">default</span> <span class="number">0.3</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">155</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # <span class="keyword">of</span> required reducers.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">155</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">378</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=<span class="number">1000000000</span> maxReducers=<span class="number">999</span> totalInputFileSize=<span class="number">103</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">378</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism <span class="keyword">to</span> <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">36</span>:<span class="number">51</span>,<span class="number">379</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job4848190506187042517.jar
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">009</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job4848190506187042517.jar created
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">047</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">052</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is <span class="keyword">false</span>, will not generate code.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">052</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process <span class="keyword">to</span> move generated code <span class="keyword">to</span> distributed cache
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">052</span> [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] <span class="keyword">with</span> classes <span class="keyword">to</span> deserialize []
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">182</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">1</span> map-reduce job(s) waiting <span class="keyword">for</span> submission.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">663</span> [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">663</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">04</span>,<span class="number">671</span> [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201404231546_0002
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases <span class="number">1</span>-<span class="number">17</span>,D
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: D[<span class="number">5</span>,<span class="number">4</span>],<span class="number">1</span>-<span class="number">17</span>[<span class="number">5</span>,<span class="number">13</span>] C: D[<span class="number">5</span>,<span class="number">4</span>],<span class="number">1</span>-<span class="number">17</span>[<span class="number">5</span>,<span class="number">13</span>] R: D[<span class="number">5</span>,<span class="number">4</span>]
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">05</span>,<span class="number">676</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http:<span class="comment">//hadoop01.myhadoop.com:50030/jobdetails.jsp?jobid=job_201404231546_0002</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">17</span>,<span class="number">750</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">75</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">27</span>,<span class="number">518</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">83</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">568</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - <span class="number">100</span>% complete
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">591</span> [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: 

HadoopVersion   PigVersion      UserId  StartedAt       FinishedAt      Features
<span class="number">1.2</span><span class="number">.1</span>   <span class="number">0.12</span><span class="number">.1</span>  grid    <span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">57</span>     <span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>     GROUP_BY,DISTINCT

Success!

Job Stats (time <span class="keyword">in</span> seconds):
JobId   Maps    Reduces MaxMapTime      MinMapTIme      AvgMapTime      MedianMapTime   MaxReduceTime   MinReduceTime        AvgReduceTime   MedianReducetime        Alias   Feature Outputs
job_201404231546_0001   <span class="number">1</span>       <span class="number">1</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      A,B DISTINCT
job_201404231546_0002   <span class="number">1</span>       <span class="number">1</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      <span class="number">12</span>      <span class="number">1</span>-<span class="number">17</span>,D       GROUP_BY,COMBINER       hdfs:<span class="comment">//hadoop01.myhadoop.com:9000/tmp/temp1433820585/tmp1152433391,</span>

Input(s):
Successfully read <span class="number">7</span> records (<span class="number">517</span> bytes) from: <span class="string">"hdfs://hadoop01.myhadoop.com:9000/user/grid/pig/score.txt"</span>

Output(s):
Successfully stored <span class="number">3</span> records (<span class="number">41</span> bytes) <span class="keyword">in</span>: <span class="string">"hdfs://hadoop01.myhadoop.com:9000/tmp/temp1433820585/tmp1152433391"</span>

Counters:
Total records written : <span class="number">3</span>
Total bytes written : <span class="number">41</span>
Spillable Memory Manager spill count : <span class="number">0</span>
Total bags proactively spilled: <span class="number">0</span>
Total records proactively spilled: <span class="number">0</span>

Job DAG:
job_201404231546_0001   -&gt;      job_201404231546_0002,
job_201404231546_0002


<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">627</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">641</span> [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">654</span> [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
<span class="number">2014</span>-<span class="number">04</span>-<span class="number">23</span> <span class="number">16</span>:<span class="number">37</span>:<span class="number">35</span>,<span class="number">655</span> [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths <span class="keyword">to</span> process : <span class="number">1</span>
(James,<span class="number">2</span>)
(NocWei,<span class="number">1</span>)
(Vincent,<span class="number">3</span>）
</code></pre><p>最后几行展现了结果</p>
<h6 id="实验二：请使用Pig_latin语言处理access_log-txt日志，计算出每个IP的点击数">实验二：请使用Pig latin语言处理access_log.txt日志，计算出每个IP的点击数</h6>
<p>这个实验是采用了大牛刘盛的BLOG <a href="http://blog.itpub.net/26686207" target="_blank">http://blog.itpub.net/26686207</a><br>做实验的时候我没有做操作日志，下面是他的操作日志</p>
<pre><code>grunt&gt; cat <span class="filename">access_log.txt 我们来看一下文件的内容之后进行数据分析
119.146.220.12 - - [31/Jan/2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">51</span> +<span class="number">0800</span>] <span class="string">"GET /static/js/jquery-1.6.js HTTP/1.1"</span> <span class="number">404</span> <span class="number">299</span> <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">52</span> +<span class="number">0800</span>] <span class="string">"GET /static/js/floating-jf.js HTTP/1.1"</span> <span class="number">404</span> <span class="number">300</span> <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">55</span> +<span class="number">0800</span>] <span class="string">"GET /popwin_js.php?fid=53 HTTP/1.1"</span> <span class="number">404</span> <span class="number">289</span> <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">55</span> +<span class="number">0800</span>] <span class="string">"GET /static/js/smilies.js?AZH HTTP/1.1"</span> <span class="number">304</span> - <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> - - [<span class="number">31</span>/Jan/<span class="number">2012</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">55</span> +<span class="number">0800</span>] <span class="string">"GET /data/cache/common_smilies_var.js?AZH HTTP/1.1"</span> <span class="number">304</span> - <span class="string">"http://f.dataguru.cn/forum.php?mod=forumdisplay&amp;fid=53&amp;page=1"</span> <span class="string">"Mozilla/5.0 (Windows NT 5.1; rv:8.0.1) Gecko/20100101 Firefox/8.0.1"</span>
</code></pre><p>数据算法：这是一部分dataguru上网日志，从日志内容结构看，ip地址是放在前面的，我们只要抽取出ip地址写入一张ip_text表，然后对ip列进行分组相当于分成若干个小表，每个ip集合为一个小表，再单独算出每个小表总行数即ip点击次数</p>
<p>（1）加载HDFS文件系统中access_log.txt文件内容放到pig的一个关系(表)里</p>
<pre><code>使用空格作为分隔符，只加载ip列即可。
grunt&gt; ip_text = LOAD <span class="string">'pig/access_log.txt'</span> <span class="keyword">USING</span> PigStorage(<span class="string">' '</span>) <span class="keyword">AS</span> (ip:chararray);
ip_text：代表一个关系，一个表，一个变量，这个表中存放了所有ip记录
LOAD <span class="string">'pig/access_log.txt'</span>：要加载的文件
<span class="keyword">USING</span> PigStorage(<span class="string">' '</span>)：使用空格作为分隔符
ip:chararray：表中第一列名ip，数据类型chararray字符型
</code></pre><p>（2）查看ip_text表结构与内容</p>
<pre><code>一定要仔细，例如命令结尾符不要丢掉，当我们执行一条<span class="tag">pig</span> <span class="tag">latin</span>语句时，
<span class="tag">pig</span>自动转换成<span class="tag">MapReduce</span>作业对用户来说是透明的，先创建一个<span class="tag">jar</span>包，再提交<span class="tag">MR</span> <span class="tag">job</span>，
生成<span class="tag">Hadoop</span> <span class="tag">job</span> <span class="tag">id</span>在执行，最后显示结果
<span class="tag">grunt</span>&gt; <span class="tag">DESCRIBE</span> <span class="tag">ip_text</span>; 显示表的结构，只有一列，类型为字符型
<span class="tag">ip_text</span>: <span class="rules">{<span class="rule"><span class="attribute">ip</span>:<span class="value"> chararray</span></span></span>}
<span class="tag">grunt</span>&gt; <span class="tag">DUMP</span> <span class="tag">ip_text</span>; 显示表的内容，只截取部分内容
<span class="tag">creating</span> <span class="tag">jar</span> <span class="tag">file</span> <span class="tag">Job2594979755419279957</span><span class="class">.jar</span>
1 <span class="tag">map-reduce</span> <span class="tag">job</span>(<span class="tag">s</span>) <span class="tag">waiting</span> <span class="tag">for</span> <span class="tag">submission</span>
<span class="tag">HadoopJobId</span>: <span class="tag">job_201210121146_0002</span>
(119<span class="class">.146</span><span class="class">.220</span><span class="class">.12</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.41</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.44</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.44</span>)
(180<span class="class">.153</span><span class="class">.227</span><span class="class">.44</span>)
(221<span class="class">.194</span><span class="class">.180</span><span class="class">.166</span>)
(119<span class="class">.146</span><span class="class">.220</span><span class="class">.12</span>)
....
</code></pre><p>（3）对ip列进行分组，并查看分组后表的内容和结构，注意关键字大小写</p>
<p>把每个ip集合分成一个个小表，把分组后的结果存放在 group_ip 这个表中</p>
<pre><code>grunt<span class="subst">&gt;</span> group_ip <span class="subst">=</span> <span class="keyword">GROUP</span> ip_text <span class="keyword">BY</span> ip; 按照ip进行分组赋给group_ip表
grunt<span class="subst">&gt;</span> DESCRIBE group_ip; 查看group_ip表结构
group_ip: {<span class="keyword">group</span>: chararray,ip_text: {(ip: chararray)}}
我们一眼就看出group_ip表是一个嵌套表，第一个field是<span class="keyword">group</span>，这就是分组后的ip值
第二个field是一个嵌套的小表又叫包，是前面分组ip的整个集合
grunt<span class="subst">&gt;</span> DUMP group_ip; 又提交一个MR job运行
Pig script settings are added <span class="keyword">to</span> the job Pig脚本自动转换MR job
creating jar file Job2785495206577164389<span class="built_in">.</span>jar 创建jar包
jar file Job2785495206577164389<span class="built_in">.</span>jar created jar包创建完毕
<span class="built_in">map</span><span class="attribute">-reduce</span> job(s) waiting for submission<span class="built_in">.</span> 提交job
HadoopJobId: job_201210121146_0003 job id：job_201210121146_0003
(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="built_in">.</span>
<span class="number">180.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>),(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>
<span class="attribute">...</span><span class="built_in">.</span>
</code></pre><p>（4）统计每个小表总行数即ip点击次数</p>
<pre><code>grunt&gt; count_ip = FOREACH group_ip GENERATE <span class="keyword">group</span>,COUNT($<span class="number">1</span>) <span class="keyword">AS</span> count_ip;
FOREACH group_ip：逐行扫描group_ip表，赋给count_ip表
GENERATE <span class="keyword">group</span>：读取分组ip值
COUNT($<span class="number">1</span>) <span class="keyword">AS</span> count_ip：统计嵌套小表(包)总行数即ip点击次数，把此列取别名叫count_ip方便倒序排列，$<span class="number">1</span>统计第一列，等价于COUNT(ip_text.ip)
grunt&gt; sort_count_ip = <span class="keyword">ORDER</span> count_ip <span class="keyword">BY</span> count_ip <span class="keyword">DESC</span>; 按照count_ip列从大到小排序
# grunt&gt; sort_count_ip = <span class="keyword">ORDER</span> count_ip <span class="keyword">BY</span> count_ip <span class="keyword">ASC</span>; 从小到大排序
</code></pre><p>（5）查看sort_count_ip表结构和内容</p>
<pre><code>grunt&gt; DESCRIBE sort_count_ip; 显示表的结构，有二列
sort_count_ip: {group: chararray,count_ip: long} 第一个field是group字符型（分组ip值），第二个field是count_ip长类型（ip点击次数）
grunt&gt; DUMP sort_count_ip; 显示表的内容，只截取部分结果，先输出统计信息后显示结果
HadoopVersion PigVersion UserId StartedAt FinishedAt Features
<span class="number">0.20</span><span class="number">.2</span> <span class="number">0.9</span><span class="number">.2</span> grid <span class="number">2012</span>-<span class="number">11</span>-<span class="number">03</span> <span class="number">21</span>:<span class="number">13</span>:<span class="number">05</span> <span class="number">2012</span>-<span class="number">11</span>-<span class="number">03</span> <span class="number">21</span>:<span class="number">18</span>:<span class="number">39</span> GROUP_BY,ORDER_BY
Success!
Input(s):
Successfully read <span class="number">28134</span> records (<span class="number">7118627</span> bytes) from:
<span class="keyword">...</span>
Job DAG:
job_201210121146_0004 -&gt; job_201210121146_0005,
job_201210121146_0005 -&gt; job_201210121146_0006,
job_201210121146_0006
(<span class="number">218.20</span><span class="number">.24</span><span class="number">.203</span>,<span class="number">4597</span>)
(<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span>,<span class="number">4576</span>)
(<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span>,<span class="number">1850</span>)
(<span class="number">117.136</span><span class="number">.31</span><span class="number">.144</span>,<span class="number">1647</span>)
(<span class="number">121.28</span><span class="number">.95</span><span class="number">.48</span>,<span class="number">1597</span>)
<span class="keyword">...</span>
</code></pre><p>（6）把sort_count_ip表内容写入HDFS文件系统中，即固化到硬盘存入文件</p>
<pre><code>grunt<span class="subst">&gt;</span> STORE sort_count_ip <span class="keyword">INTO</span> <span class="string">'pig/sort_count_ip'</span>;
Counters:
Total <span class="keyword">records</span> written : <span class="number">476</span>
Total <span class="built_in">bytes</span> written : <span class="number">8051</span>
Spillable Memory Manager spill count : <span class="number">0</span>
Total bags proactively spilled: <span class="number">0</span>
Total <span class="keyword">records</span> proactively spilled: <span class="number">0</span>
Job DAG:
job_201210121146_0007 <span class="subst">-&gt; </span>job_201210121146_0008,
job_201210121146_0008 <span class="subst">-&gt; </span>job_201210121146_0009,
job_201210121146_0009
<span class="number">2012</span><span class="subst">-</span><span class="number">11</span><span class="subst">-</span><span class="number">03</span> <span class="number">21</span>:<span class="number">28</span>:<span class="number">41</span>,<span class="number">520</span> <span class="preprocessor">[</span>main<span class="preprocessor">]</span><span class="markup"> INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!</span>
</code></pre><p>当我们看到Success时就说明我们已经保存成功！</p>
<p>（7）查看保存在HDFS中的结果文件</p>
<pre><code>grunt&gt; <span class="built_in">cat</span> sort_count_ip
<span class="number">218.20</span><span class="number">.24</span><span class="number">.203</span> <span class="number">4597</span>
<span class="number">221.194</span><span class="number">.180</span><span class="number">.166</span> <span class="number">4576</span>
<span class="number">119.146</span><span class="number">.220</span><span class="number">.12</span> <span class="number">1850</span>
<span class="number">117.136</span><span class="number">.31</span><span class="number">.144</span> <span class="number">1647</span>
<span class="number">121.28</span><span class="number">.95</span><span class="number">.48</span> <span class="number">1597</span>
<span class="number">113.109</span><span class="number">.183</span><span class="number">.126</span> <span class="number">1596</span>
<span class="number">182.48</span><span class="number">.112</span><span class="number">.2</span> <span class="number">870</span>
<span class="number">120.84</span><span class="number">.24</span><span class="number">.200</span> <span class="number">773</span>
<span class="number">61.144</span><span class="number">.125</span><span class="number">.162</span> <span class="number">750</span>
<span class="number">27.115</span><span class="number">.124</span><span class="number">.75</span> <span class="number">470</span>
<span class="number">115.236</span><span class="number">.48</span><span class="number">.226</span> <span class="number">439</span>
</code></pre><p>OK！！！</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Pig" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Pig/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[eclipse连接hadoop1.2.1集群]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/22/Hadoop/hadoop-eclispe/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/22/Hadoop/hadoop-eclispe/</id>
    <published>2014-04-22T12:43:01.000Z</published>
    <updated>2014-04-22T13:41:56.000Z</updated>
    <content type="html"><![CDATA[<h5 id="下载安装eclipse">下载安装eclipse</h5>
<p>去官网下载eclipse，我下载的是eclipse-standard-kepler-SR2-linux-gtk-x86_64.tar.gz<br>移到目录/opt下,解压tar -zxvf eclipse-standard-kepler-SR2-linux-gtk-x86_64.tar.gz 进入/opt/eclipse/目录，双击eclipse,eclipse启动了</p>
<p>为eclipse创建桌面快捷方式，在linux桌面空白处,点击鼠标右键,弹出一个对话框,选择“create Launcher”(中文是“新建启动器”),在“Create Launcher”(中文“创建快捷方式”)对话框中选择Application(中文“应用程序”)，输入名字：eclipse ，在Command(中文“命令”)找到eclipse的启动程序(比如我的是：/opt/eclipse/eclipse),双击左上侧的图标会弹出一个选择图标的对话框，找到安装eclipse的目录,在目录中找到icon.xpm，选择Open(确定),设置好后类似于下图：<br><img src="/img/Hadoop/eclipse/1.png" alt="图1"></p>
<h5 id="修改配置文件">修改配置文件</h5>
<p>hadoop-1.2.1/src/contrib/eclipse-plugin/build.xml<br>添加相应位置下内容：</p>
<pre><code><span class="tag">&lt;<span class="title">path</span> <span class="attribute">id</span>=<span class="value">"eclipse-sdk-jars"</span>&gt;</span>    <span class="tag">&lt;<span class="title">fileset</span> <span class="attribute">dir</span>=<span class="value">"${eclipse.home}/plugins/"</span>&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.ui*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.jdt*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.core*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.equinox*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.debug*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.osgi*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.swt*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.jface*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"org.eclipse.team.cvs.ssh2*.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"com.jcraft.jsch*.jar"</span>/&gt;</span>
<span class="tag">&lt;/<span class="title">fileset</span>&gt;</span>
<span class="tag">&lt;/<span class="title">path</span>&gt;</span>
//添加这些内容
<span class="tag">&lt;<span class="title">path</span> <span class="attribute">id</span>=<span class="value">"hadoop-lib-jars"</span>&gt;</span>
<span class="tag">&lt;<span class="title">fileset</span> <span class="attribute">dir</span>=<span class="value">"${hadoop.root}/"</span>&gt;</span>
<span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"hadoop*.jar"</span> /&gt;</span>
<span class="tag">&lt;/<span class="title">fileset</span>&gt;</span>
<span class="tag">&lt;/<span class="title">path</span>&gt;</span>
...

<span class="tag">&lt;<span class="title">path</span> <span class="attribute">id</span>=<span class="value">"classpath"</span>&gt;</span>
<span class="tag">&lt;<span class="title">pathelement</span> <span class="attribute">location</span>=<span class="value">"${build.classes}"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">pathelement</span> <span class="attribute">location</span>=<span class="value">"${hadoop.root}/build/classes"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">pathelement</span> <span class="attribute">location</span>=<span class="value">"${hadoop.root}/hadoop-core-1.1.2.jar"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">path</span> <span class="attribute">refid</span>=<span class="value">"eclipse-sdk-jars"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">path</span> <span class="attribute">refid</span>=<span class="value">"hadoop-lib-jars"</span> /&gt;</span>  //添加这一句
<span class="tag">&lt;/<span class="title">path</span>&gt;</span>
...
先注释掉粗体内容前两项，后再添加七项
<span class="tag">&lt;<span class="title">target</span> <span class="attribute">name</span>=<span class="value">"jar"</span> <span class="attribute">depends</span>=<span class="value">"compile"</span> <span class="attribute">unless</span>=<span class="value">"skip.contrib"</span>&gt;</span>    <span class="tag">&lt;<span class="title">mkdir</span> <span class="attribute">dir</span>=<span class="value">"${build.dir}/lib"</span>/&gt;</span>
<span class="comment">&lt;!-- &lt;copy file="${hadoop.root}/build/hadoop-core-${version}.jar" tofile="${build.dir}/lib/hadoop-core.jar" verbose="true"/&gt;
&lt;copy file="${hadoop.root}/build/ivy/lib/Hadoop/common/commons-cli-${commons-cli.version}.jar"  todir="${build.dir}/lib" verbose="true"/&gt;--&gt;</span>
//添加这七项
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/hadoop-core-${version}.jar"</span> <span class="attribute">tofile</span>=<span class="value">"${build.dir}/lib/hadoop-core.jar"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-cli-${commons-cli.version}.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-configuration-1.6.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-httpclient-3.0.1.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/commons-lang-2.4.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/jackson-core-asl-1.8.8.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">copy</span> <span class="attribute">file</span>=<span class="value">"${hadoop.root}/lib/jackson-mapper-asl-1.8.8.jar"</span> <span class="attribute">todir</span>=<span class="value">"${build.dir}/lib"</span> <span class="attribute">verbose</span>=<span class="value">"true"</span>/&gt;</span>

找到src\contrib\build-contrib.xml，添加以下几行：

<span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"version"</span> <span class="attribute">value</span>=<span class="value">"1.2.1"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"ivy.version"</span> <span class="attribute">value</span>=<span class="value">"2.1.0"</span>/&gt;</span>
<span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"eclipse.home"</span> <span class="attribute">location</span>=<span class="value">"..."</span>/&gt;</span>
eclipse的路径请换成你主机上的eclipse存放路径。
</code></pre><h5 id="编译，并将插件拷到对应目录">编译，并将插件拷到对应目录</h5>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ ant
Buildfile: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/eclipse-plugin/build.xml

check-contrib:

init:
     [echo] contrib: eclipse-plugin

init-contrib:

ivy-download:
      [get] Getting: http:<span class="comment">//repo2.maven.org/maven2/org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar</span>
      [get] To: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivy-<span class="number">2.1</span><span class="number">.0</span>.jar
      [get] Not modified - so not downloaded

ivy-probe-antlib:

ivy-init-antlib:

ivy-init:
[ivy:configure] :: Ivy <span class="number">2.1</span><span class="number">.0</span> - <span class="number">20090925235825</span> :: http:<span class="comment">//ant.apache.org/ivy/ ::</span>
[ivy:configure] :: loading settings :: <span class="keyword">file</span> = /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivysettings.xml

ivy-resolve-common:

ivy-retrieve-common:
[ivy:cachepath] DEPRECATED: <span class="string">'ivy.conf.file'</span> is deprecated, use <span class="string">'ivy.settings.file'</span> instead
[ivy:cachepath] :: loading settings :: <span class="keyword">file</span> = /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivysettings.xml

compile:
     [echo] contrib: eclipse-plugin
    [javac] /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/eclipse-plugin/build.xml:<span class="number">68</span>: <span class="keyword">warning</span>: <span class="string">'includeantruntime'</span> was not set, defaulting to build.sysclasspath=last; set to false <span class="keyword">for</span> repeatable builds

jar:
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-core-<span class="number">1.2</span><span class="number">.1</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/hadoop-core.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-cli-<span class="number">1.2</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-cli-<span class="number">1.2</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-lang-<span class="number">2.4</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-lang-<span class="number">2.4</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-configuration-<span class="number">1.6</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-configuration-<span class="number">1.6</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-mapper-asl-<span class="number">1.8</span><span class="number">.8</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/jackson-mapper-asl-<span class="number">1.8</span><span class="number">.8</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-core-asl-<span class="number">1.8</span><span class="number">.8</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/jackson-core-asl-<span class="number">1.8</span><span class="number">.8</span>.jar
     [copy] Copying <span class="number">1</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib
     [copy] Copying /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-httpclient-<span class="number">3.0</span><span class="number">.1</span>.jar to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/lib/commons-httpclient-<span class="number">3.0</span><span class="number">.1</span>.jar
      [jar] Building jar: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/hadoop-eclipse-plugin-<span class="number">1.2</span><span class="number">.1</span>.jar

BUILD SUCCESSFUL
Total time: <span class="number">10</span> seconds
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ <span class="keyword">ls</span>
build.properties  build.xml  ivy  ivy.xml  META-INF  plugin.xml  resources  src
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ <span class="keyword">pwd</span>
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/eclipse-plugin
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ cd /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> eclipse-plugin]$ <span class="keyword">ls</span>
classes  examples  hadoop-eclipse-plugin-<span class="number">1.2</span><span class="number">.1</span>.jar  lib  <span class="keyword">system</span>  test
[root<span class="variable">@hadoop01</span> plugins]# cp /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/build/contrib/eclipse-plugin/hadoop-eclipse-plugin-<span class="number">1.2</span><span class="number">.1</span>.jar  /opt/eclipse/plugins/
</code></pre><h5 id="建立测试数据文件word-txt">建立测试数据文件word.txt</h5>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">ls</span>
FileSystemCat.class  FileSystemCat.java  noaa  <span class="keyword">python</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ touch word.txt
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ vi word.txt 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ hadoop fs -mkdir /tmp/wordcount
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ hadoop fs -copyFromLocal /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/word.txt  /tmp/wordcount/word.txt
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ cat word.txt 
java c++ <span class="keyword">python</span> c
java c++ javascript 
helloworld hadoop
mapreduce java hadoop hbase 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$
</code></pre><h5 id="在eclipse上安装hadoop插件">在eclipse上安装hadoop插件</h5>
<p>启动eclipse，配置hadoop installation directory。 如果安装插件成功，打开Window—&gt;Preferens，你会发现Hadoop Map/Reduce选项，在这个选项里你需要配置Hadoop installation directory。配置完成后退出。<br><img src="/img/Hadoop/eclipse/2.png" alt="图2"></p>
<h5 id="配置Map/Reduce_Locations。">配置Map/Reduce Locations。</h5>
<p>在Window—&gt;Show View中打开Map/Reduce Locations。 在Map/Reduce Locations中新建一个Hadoop Location。在这个View中，右键—&gt;New Hadoop Location。在弹出的对话框中你需要配置Location name，如Hadoop，还有Map/Reduce Master和DFS Master。这里面的Host、Port分别为你在mapred-site.xml、core-site.xml中配置的地址及端口。<br><img src="/img/Hadoop/eclipse/3.png" alt="图3"><br><img src="/img/Hadoop/eclipse/4.png" alt="图4"><br>配置完后退出。点击DFS Locations—&gt;Hadoop如果能显示文件夹(2)说明配置正确，如果显示”拒绝连接”，请检查你的配置。<br><img src="/img/Hadoop/eclipse/5.png" alt="图5"></p>
<h5 id="建立项目">建立项目</h5>
<p>新建项目 File—&gt;New—&gt;Other—&gt;Map/Reduce Project 项目名可以随便取，如WordCount。 复制 hadoop安装目录/src/example/org/apache/hadoop/example/WordCount.java到刚才新建的项目下面。</p>
<p>通过hadoop的命令在HDFS上创建/tmp/workcount目录，命令如下：bin/hadoop fs -mkdir /tmp/wordcount</p>
<p>通过copyFromLocal命令把本地的word.txt复制到HDFS上，命令如下：bin/hadoop fs -copyFromLocal /home/grid/word.txt  /tmp/wordcount/word.txt </p>
<h5 id="运行项目">运行项目</h5>
<p>在新建的项目Hadoop，点击WordCount.java，右键—&gt;Run As—&gt;Run Configurations  在弹出的Run Configurations对话框中，点Java Application，右键—&gt;New，这时会新建一个application名为WordCount 配置运行参数，点Arguments，在Program arguments中输入“你要传给程序的输入文件夹和你要求程序将计算结果保存的文件夹”，如：<br><img src="/img/Hadoop/eclipse/6.png" alt="图6"></p>
<p>运行<br><img src="/img/Hadoop/eclipse/7.png" alt="图7"></p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Eclipse" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Eclipse/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Map-Reduce入门]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/map_reduce/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/map_reduce/</id>
    <published>2014-04-21T13:50:52.000Z</published>
    <updated>2014-04-21T15:15:41.000Z</updated>
    <content type="html"><![CDATA[<h5 id="Map-Reduce的逻辑过程">Map-Reduce的逻辑过程</h5>
<p>假设我们需要处理一批有关天气的数据，其格式如下：按照ASCII码存储，每行一条记录,每一行字符从0开始计数，第15个到第18个字符为年,第25个到第29个字符为温度，其中第25位是符号+/-<br>    0067011990999991950051507+0000+<br>    0043011990999991950051512+0022+<br>    0043011990999991950051518-0011+<br>    0043012650999991949032412+0111+<br>    0043012650999991949032418+0078+<br>    0067011990999991937051507+0001+<br>    0043011990999991937051512-0002+<br>    0043011990999991945051518+0001+<br>    0043012650999991945032412+0002+<br>    0043012650999991945032418+0078+</p>
<p>现在需要统计出每年的最高温度。Map-Reduce主要包括两个步骤：Map和Reduce。每一步都有key-value对作为输入和输出：map阶段的key-value对的格式是由输入的格式所决定的，如果是默认的TextInputFormat，则每行作为一个记录进程处理，其中key为此行的开头相对于文件的起始位置，value就是此行的字符文本map阶段的输出的key-value对的格式必须同reduce阶段的输入key-value对的格式相对应。</p>
<p>对于上面的例子，在map过程，输入的key-value对如下：</p>
<pre><code><span class="list">(<span class="title">0</span>,<span class="number"> 0067011990999991950051507</span>+0000+)</span>
<span class="list">(<span class="title">33</span>,<span class="number"> 0043011990999991950051512</span>+0022+)</span>
<span class="list">(<span class="title">66</span>,<span class="number"> 0043011990999991950051518</span>-0011+)</span>
<span class="list">(<span class="title">99</span>,<span class="number"> 0043012650999991949032412</span>+0111+)</span>
<span class="list">(<span class="title">132</span>,<span class="number"> 0043012650999991949032418</span>+0078+)</span>
<span class="list">(<span class="title">165</span>,<span class="number"> 0067011990999991937051507</span>+0001+)</span>
<span class="list">(<span class="title">198</span>,<span class="number"> 0043011990999991937051512</span>-0002+)</span>
<span class="list">(<span class="title">231</span>,<span class="number"> 0043011990999991945051518</span>+0001+)</span>
<span class="list">(<span class="title">264</span>,<span class="number"> 0043012650999991945032412</span>+0002+)</span>
<span class="list">(<span class="title">297</span>,<span class="number"> 0043012650999991945032418</span>+0078+)</span>
</code></pre><p>在map过程中，通过对每一行字符串的解析，得到年-温度的key-value对作为输出：</p>
<pre><code><span class="list">(<span class="title">1950</span>,<span class="number"> 0</span>)</span>
<span class="list">(<span class="title">1950</span>,<span class="number"> 22</span>)</span>
<span class="list">(<span class="title">1950</span>, -11)</span>
<span class="list">(<span class="title">1949</span>,<span class="number"> 111</span>)</span>
<span class="list">(<span class="title">1949</span>,<span class="number"> 78</span>)</span>
<span class="list">(<span class="title">1937</span>,<span class="number"> 1</span>)</span>
<span class="list">(<span class="title">1937</span>, -2)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 1</span>)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 2</span>)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 78</span>)</span>
</code></pre><p>在reduce过程，将map过程中的输出，按照相同的key将value放到同一个列表中作为reduce的输入</p>
<pre><code><span class="list">(<span class="title">1950</span>, <span class="collection">[0,<span class="number"> 22</span>, –11]</span>)</span>
<span class="list">(<span class="title">1949</span>, <span class="collection">[111,<span class="number"> 78</span>]</span>)</span>
<span class="list">(<span class="title">1937</span>, <span class="collection">[1, -2]</span>)</span>
<span class="list">(<span class="title">1945</span>, <span class="collection">[1,<span class="number"> 2</span>,<span class="number"> 78</span>]</span>)</span>
</code></pre><p>在reduce过程中，在列表中选择出最大的温度，将年-最大温度的key-value作为输出：</p>
<pre><code><span class="list">(<span class="title">1950</span>,<span class="number"> 22</span>)</span>
<span class="list">(<span class="title">1949</span>,<span class="number"> 111</span>)</span>
<span class="list">(<span class="title">1937</span>,<span class="number"> 1</span>)</span>
<span class="list">(<span class="title">1945</span>,<span class="number"> 78</span>)</span>
</code></pre><p>其逻辑过程可用如下图表示：<br><img src="/img/Hadoop/MapReduce/1.png" alt="图1"></p>
<h5 id="编写Map-Reduce程序">编写Map-Reduce程序</h5>
<p>编写Map-Reduce程序，一般需要实现两个函数：mapper中的map函数和reducer中的reduce函数。<br>一般遵循以下格式：</p>
<pre><code>map: (K1, V1)  -&gt;  <span class="keyword">list</span>(K2, V2)
<span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Mapper</span>&lt;<span class="title">K1</span>, <span class="title">V1</span>, <span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">extends</span> <span class="title">JobConfigurable</span>, <span class="title">Closeable</span> {</span>

  void map(K1 key, V1 value, OutputCollector&lt;K2, V2&gt; output, Reporter reporter)

  throws IOException;

}

reduce: (K2, <span class="keyword">list</span>(V))  -&gt;  <span class="keyword">list</span>(K3, V3) 
<span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Reducer</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>, <span class="title">K3</span>, <span class="title">V3</span>&gt; <span class="keyword">extends</span> <span class="title">JobConfigurable</span>, <span class="title">Closeable</span> {</span>

  void reduce(K2 key, Iterator&lt;V2&gt; values,

              OutputCollector&lt;K3, V3&gt; output, Reporter reporter)

    throws IOException;

}
</code></pre><p>对于上面的例子，则实现的mapper如下：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxTemperatureMapper</span> <span class="inheritance"><span class="keyword">extends</span></span> <span class="title">MapReduceBase</span> <span class="inheritance"><span class="keyword">implements</span></span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; {</span>

    @Override

    <span class="keyword">public</span> <span class="keyword">void</span> map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {

        String line = value.toString();

        String year = line.substring(<span class="number">15</span>, <span class="number">19</span>);

        <span class="keyword">int</span> airTemperature;

        <span class="keyword">if</span> (line.charAt(<span class="number">25</span>) == <span class="string">'+'</span>) {

            airTemperature = Integer.parseInt(line.substring(<span class="number">26</span>, <span class="number">30</span>));

        } <span class="keyword">else</span> {

            airTemperature = Integer.parseInt(line.substring(<span class="number">25</span>, <span class="number">30</span>));

        }

        output.collect(<span class="keyword">new</span> Text(year), <span class="keyword">new</span> IntWritable(airTemperature));

    }

}
</code></pre><p>实现的reducer如下：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxTemperatureReducer</span> <span class="inheritance"><span class="keyword">extends</span></span> <span class="title">MapReduceBase</span> <span class="inheritance"><span class="keyword">implements</span></span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; {</span>

    <span class="keyword">public</span> <span class="keyword">void</span> reduce(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {

        <span class="keyword">int</span> maxValue = Integer.MIN_VALUE;

        <span class="keyword">while</span> (values.hasNext()) {

            maxValue = Math.max(maxValue, values.next().get());

        }

        output.collect(key, <span class="keyword">new</span> IntWritable(maxValue));

    }

}
</code></pre><p>欲运行上面实现的Mapper和Reduce，则需要生成一个Map-Reduce得任务(Job)，其基本包括以下三部分：</p>
<pre><code>输入的数据，也即需要处理的数据
<span class="keyword">Map</span>-<span class="keyword">Reduce</span>程序，也即上面实现的Mapper和Reducer
此任务的配置项JobConf
</code></pre><p>欲配置JobConf，需要大致了解Hadoop运行job的基本原理：</p>
<pre><code>Hadoop将Job分成task进行处理，共两种task：map task和reduce task
Hadoop有两类的节点控制job的运行：JobTracker和TaskTracker
JobTracker协调整个job的运行，将task分配到不同的TaskTracker上
TaskTracker负责运行task，并将结果返回给JobTracker
Hadoop将输入数据分成固定大小的块，我们称之input split
Hadoop为每一个input split创建一个task，在此task中依次处理此split中的一个个记录(record)
Hadoop会尽量让输入数据块所在的DataNode和task所执行的DataNode(每个DataNode上都有一个TaskTracker)为同一个，
可以提高运行效率，所以input split的大小也一般是HDFS的block的大小。
Reduce task的输入一般为Map Task的输出，Reduce Task的输出为整个job的输出，保存在HDFS上。
在reduce中，相同key的所有的记录一定会到同一个TaskTracker上面运行，
然而不同的key可以在不同的TaskTracker上面运行，我们称之为partition
partition的规则为：(K2, V2) –&gt; Integer， 也即根据K2，生成一个partition的id，
具有相同id的K2则进入同一个partition，被同一个TaskTracker上被同一个Reducer进行处理。
<span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="inheritance"><span class="keyword">extends</span></span> <span class="title">JobConfigurable</span> {</span>

<span class="keyword">int</span> getPartition(K2 key, V2 value, <span class="keyword">int</span> numPartitions);

}
</code></pre><p>下图大概描述了Map-Reduce的Job运行的基本原理：<br><img src="/img/Hadoop/MapReduce/2.png" alt="图2"></p>
<p>下面我们讨论JobConf，其有很多的项可以进行配置：<br>1、setInputFormat：设置map的输入格式，默认为TextInputFormat，key为LongWritable, value为Text<br>2、setNumMapTasks：设置map任务的个数，此设置通常不起作用，map任务的个数取决于输入的数据所能分成的input split的个数<br>3、setMapperClass：设置Mapper，默认为IdentityMapper<br>4、setMapRunnerClass：设置MapRunner, map task是由MapRunner运行的，默认为MapRunnable，其功能为读取input split的一个个record，依次调用Mapper的map函数<br>5、setMapOutputKeyClass和setMapOutputValueClass：设置Mapper的输出的key-value对的格式<br>6、setOutputKeyClass和setOutputValueClass：设置Reducer的输出的key-value对的格式<br>7、setPartitionerClass和setNumReduceTasks：设置Partitioner，默认为HashPartitioner，其根据key的hash值来决定进入哪个partition，每个partition被一个reduce task处理，所以partition的个数等于reduce task的个数<br>8、setReducerClass：设置Reducer，默认为IdentityReducer<br>9、setOutputFormat：设置任务的输出格式，默认为TextOutputFormat<br>10、FileInputFormat.addInputPath：设置输入文件的路径，可以使一个文件，一个路径，一个通配符。可以被调用多次添加多个路径<br>11、FileOutputFormat.setOutputPath：设置输出文件的路径，在job运行前此路径不应该存在</p>
<p>当然不用所有的都设置，由上面的例子，可以编写Map-Reduce程序如下：</p>
<pre><code>public class MaxTemperature {

    public static void main(String[] args) throws IOException {

        if (args<span class="preprocessor">.length</span> != <span class="number">2</span>) {

            System<span class="preprocessor">.err</span><span class="preprocessor">.println</span>(<span class="string">"Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"</span>)<span class="comment">;</span>

            System<span class="preprocessor">.exit</span>(-<span class="number">1</span>)<span class="comment">;</span>

        }

        JobConf conf = new JobConf(MaxTemperature<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setJobName</span>(<span class="string">"Max temperature"</span>)<span class="comment">;</span>

        FileInputFormat<span class="preprocessor">.addInputPath</span>(conf, new Path(args[<span class="number">0</span>]))<span class="comment">;</span>

        FileOutputFormat<span class="preprocessor">.setOutputPath</span>(conf, new Path(args[<span class="number">1</span>]))<span class="comment">;</span>

        conf<span class="preprocessor">.setMapperClass</span>(MaxTemperatureMapper<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setReducerClass</span>(MaxTemperatureReducer<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setOutputKeyClass</span>(Text<span class="preprocessor">.class</span>)<span class="comment">;</span>

        conf<span class="preprocessor">.setOutputValueClass</span>(IntWritable<span class="preprocessor">.class</span>)<span class="comment">;</span>

        JobClient<span class="preprocessor">.runJob</span>(conf)<span class="comment">;</span>

    }

}
</code></pre><h5 id="Map-Reduce数据流(data_flow)">Map-Reduce数据流(data flow)</h5>
<p>Map-Reduce的处理过程主要涉及以下四个部分：<br>1、客户端Client：用于提交Map-reduce任务job<br>2、JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker<br>3、TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker<br>4、HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件<br><img src="/img/Hadoop/MapReduce/3.png" alt="图3"></p>
<h6 id="任务提交">任务提交</h6>
<p>JobClient.runJob()创建一个新的JobClient实例，调用其submitJob()函数。</p>
<p>向JobTracker请求一个新的job ID</p>
<p>检测此job的output配置</p>
<p>计算此job的input splits</p>
<p>将Job运行所需的资源拷贝到JobTracker的文件系统中的文件夹中，包括job jar文件，job.xml配置文件，input splits</p>
<p>通知JobTracker此Job已经可以运行了</p>
<p>提交任务后，runJob每隔一秒钟轮询一次job的进度，将进度返回到命令行，直到任务运行完毕。</p>
<h6 id="任务初始化">任务初始化</h6>
<p>当JobTracker收到submitJob调用的时候，将此任务放到一个队列中，job调度器将从队列中获取任务并初始化任务。</p>
<p>初始化首先创建一个对象来封装job运行的tasks, status以及progress。</p>
<p>在创建task之前，job调度器首先从共享文件系统中获得JobClient计算出的input splits。</p>
<p>其为每个input split创建一个map task。</p>
<p>每个task被分配一个ID。</p>
<h6 id="任务分配">任务分配</h6>
<p>TaskTracker周期性的向JobTracker发送heartbeat。</p>
<p>在heartbeat中，TaskTracker告知JobTracker其已经准备运行一个新的task，JobTracker将分配给其一个task。</p>
<p>在JobTracker为TaskTracker选择一个task之前，JobTracker必须首先按照优先级选择一个Job，在最高优先级的Job中选择一个task。</p>
<p>TaskTracker有固定数量的位置来运行map task或者reduce task。</p>
<p>默认的调度器对待map task优先于reduce task</p>
<p>当选择reduce task的时候，JobTracker并不在多个task之间进行选择，而是直接取下一个，因为reduce task没有数据本地化的概念。</p>
<h6 id="任务执行">任务执行</h6>
<p>TaskTracker被分配了一个task，下面便要运行此task。</p>
<p>首先，TaskTracker将此job的jar从共享文件系统中拷贝到TaskTracker的文件系统中。</p>
<p>TaskTracker从distributed cache中将job运行所需要的文件拷贝到本地磁盘。</p>
<p>其次，其为每个task创建一个本地的工作目录，将jar解压缩到文件目录中。</p>
<p>其三，其创建一个TaskRunner来运行task。</p>
<p>TaskRunner创建一个新的JVM来运行task。</p>
<p>被创建的child JVM和TaskTracker通信来报告运行进度。</p>
<h6 id="Map的过程">Map的过程</h6>
<p>MapRunnable从input split中读取一个个的record，然后依次调用Mapper的map函数，将结果输出。</p>
<p>map的输出并不是直接写入硬盘，而是将其写入缓存memory buffer。</p>
<p>当buffer中数据的到达一定的大小，一个背景线程将数据开始写入硬盘。</p>
<p>在写入硬盘之前，内存中的数据通过partitioner分成多个partition。</p>
<p>在同一个partition中，背景线程会将数据按照key在内存中排序。</p>
<p>每次从内存向硬盘flush数据，都生成一个新的spill文件。</p>
<p>当此task结束之前，所有的spill文件被合并为一个整的被partition的而且排好序的文件。</p>
<p>reducer可以通过http协议请求map的输出文件，tracker.http.threads可以设置http服务线程数。</p>
<h6 id="Reduce的过程">Reduce的过程</h6>
<p>当map task结束后，其通知TaskTracker，TaskTracker通知JobTracker。</p>
<p>对于一个job，JobTracker知道TaskTracer和map输出的对应关系。</p>
<p>reducer中一个线程周期性的向JobTracker请求map输出的位置，直到其取得了所有的map输出。</p>
<p>reduce task需要其对应的partition的所有的map输出。</p>
<p>reduce task中的copy过程即当每个map task结束的时候就开始拷贝输出，因为不同的map task完成时间不同。</p>
<p>reduce task中有多个copy线程，可以并行拷贝map输出。</p>
<p>当很多map输出拷贝到reduce task后，一个背景线程将其合并为一个大的排好序的文件。</p>
<p>当所有的map输出都拷贝到reduce task后，进入sort过程，将所有的map输出合并为大的排好序的文件。</p>
<p>最后进入reduce过程，调用reducer的reduce函数，处理排好序的输出的每个key，最后的结果写入HDFS。</p>
<p><img src="/img/Hadoop/MapReduce/4.png" alt="图4"></p>
<h6 id="任务结束">任务结束</h6>
<p>当JobTracker获得最后一个task的运行成功的报告后，将job得状态改为成功。</p>
<p>当JobClient从JobTracker轮询的时候，发现此job已经成功结束，则向用户打印消息，从runJob函数中返回。</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Map-Reduce" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Map-Reduce/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用hadoop流的方法来实现对气象数据集求最高温度的分析任务（使用python）]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_stream_python/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_stream_python/</id>
    <published>2014-04-21T13:50:46.000Z</published>
    <updated>2014-04-21T14:18:29.000Z</updated>
    <content type="html"><![CDATA[<p>使用hadoop流的方法来实现对气象数据集求最高温度的分析任务（使用python）</p>
<p>创建python脚本</p>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ <span class="keyword">ls</span>
max_temperature_map.py  max_temperature_reduce.py
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ cat max_temperature_map.py 
#!/bin/<span class="keyword">env</span> <span class="keyword">python</span>
import re
import sys
<span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:
  val = line.<span class="keyword">strip</span>()
  (year, temp) = (val[<span class="number">0</span>:<span class="number">4</span>], val[<span class="number">13</span>:<span class="number">19</span>])
  <span class="keyword">if</span> (temp != <span class="string">"+9999"</span>):
    <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (year, temp)
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ cat max_temperature_reduce.py 
#!/bin/<span class="keyword">env</span> <span class="keyword">python</span>
import sys

(last_key, max_val) = (None, -sys.maxint)
<span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:
  (key, val) = line.<span class="keyword">strip</span>().split(<span class="string">"\t"</span>)
  <span class="keyword">if</span> last_key and last_key != key:
    <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (last_key, max_val)
    (last_key, max_val) = (key, <span class="keyword">int</span>(val))
  <span class="keyword">else</span>:
    (last_key, max_val) = (key, <span class="keyword">max</span>(max_val, <span class="keyword">int</span>(val)))

<span class="keyword">if</span> last_key:
  <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (last_key, max_val)
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> <span class="keyword">python</span>]$ 
</code></pre><p>编译运行</p>
<pre><code>[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ bin/hadoop jar contrib/streaming/hadoop-streaming-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span> -input ./<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span> -output ./out3 -mapper /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_map<span class="preprocessor">.py</span> -reducer /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_reduce<span class="preprocessor">.py</span> -file /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_map<span class="preprocessor">.py</span> -file /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_reduce<span class="preprocessor">.py</span> 
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.
<span class="label">packageJobJar:</span> [/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_map<span class="preprocessor">.py</span>, /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass/python/max_temperature_reduce<span class="preprocessor">.py</span>, /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/tmp/hadoop-unjar7471324797393042067/] [] /tmp/streamjob6919467714598114562<span class="preprocessor">.jar</span> tmpDir=null
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">12</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">12</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">12</span> INFO mapred<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: getLocalDirs(): [/home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/tmp/mapred/local]
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Running job: job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: To kill this job, run:
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/libexec/../bin/hadoop job  -Dmapred<span class="preprocessor">.job</span><span class="preprocessor">.tracker</span>=hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">9001</span> -kill job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">13</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Tracking URL: http://hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>:<span class="number">50030</span>/jobdetails<span class="preprocessor">.jsp</span>?jobid=job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">14</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">0</span>%  reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">26</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">100</span>%  reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">35</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">100</span>%  reduce <span class="number">33</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">40</span> INFO streaming<span class="preprocessor">.StreamJob</span>:  map <span class="number">100</span>%  reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">43</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Job complete: job_201404211930_0005
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">43</span> INFO streaming<span class="preprocessor">.StreamJob</span>: Output: ./out3
</code></pre><p>查看结果</p>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -<span class="keyword">ls</span> ./out3
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span> /user/<span class="keyword">grid</span>/out3/_SUCCESS
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span> /user/<span class="keyword">grid</span>/out3/_logs
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup       <span class="number">5184</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">35</span> /user/<span class="keyword">grid</span>/out3/part-<span class="number">00000</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ 
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop_Stream" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop_Stream/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[下载气象数据集部分数据，写一个Map-Reduce作业，求每年的最低温度，部署并运行之]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_mapreduce_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/21/Hadoop/hadoop_mapreduce_1/</id>
    <published>2014-04-21T13:50:41.000Z</published>
    <updated>2014-04-21T14:10:31.000Z</updated>
    <content type="html"><![CDATA[<p>下载气象数据集部分数据，写一个Map-Reduce作业，求每年的最低温度，部署并运行之，关于下载2013noaa上的天气数据，去<a href="http://f.dataguru.cn/" target="_blank">http://f.dataguru.cn/</a> 的hadoop论坛上去搜就行</p>
<pre><code>是这种格式，<span class="number">2013</span>的部分数据
<span class="number">023170</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032260</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064280</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080440</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">121200</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
<span class="number">023190</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032270</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064310</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080450</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">121240</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
<span class="number">023210</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032300</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064320</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080480</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  noaaSample<span class="preprocessor">.txt</span>
<span class="number">023250</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032380</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064325</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080530</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
<span class="number">023270</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">032400</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">064340</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz  <span class="number">080550</span>-<span class="number">99999</span>-<span class="number">2013.</span>gz
[grid@hadoop01 noaa]$ zcat  *<span class="preprocessor">.gz</span> &gt; noaaSample<span class="preprocessor">.txt</span>
</code></pre><p>编写Java文件 MinTemperatureMapper.java,MinTemperatureReducer.java, 和 MinTemperature.java</p>
<pre><code>MinTemperatureMapper<span class="preprocessor">.java</span>
import java<span class="preprocessor">.io</span><span class="preprocessor">.IOException</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IntWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.LongWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.Mapper</span><span class="comment">;</span>
public class MinTemperatureMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private static final int MISSING = <span class="number">9999</span><span class="comment">;</span>
    @Override
    public void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {
        String line = value<span class="preprocessor">.toString</span>()<span class="comment">;</span>
        String year = line<span class="preprocessor">.substring</span>(<span class="number">15</span>, <span class="number">19</span>)<span class="comment">;</span>
        int airTemperature<span class="comment">;</span>
        if (line<span class="preprocessor">.charAt</span>(<span class="number">87</span>) == <span class="string">'+'</span>) { 
            airTemperature = Integer<span class="preprocessor">.parseInt</span>(line<span class="preprocessor">.substring</span>(<span class="number">88</span>, <span class="number">92</span>))<span class="comment">;</span>
        } else {
            airTemperature = Integer<span class="preprocessor">.parseInt</span>(line<span class="preprocessor">.substring</span>(<span class="number">87</span>, <span class="number">92</span>))<span class="comment">;</span>
        }
        String quality = line<span class="preprocessor">.substring</span>(<span class="number">92</span>, <span class="number">93</span>)<span class="comment">;</span>
        if (airTemperature != MISSING &amp;&amp; quality<span class="preprocessor">.matches</span>(<span class="string">"[01459]"</span>)) {
            context<span class="preprocessor">.write</span>(new Text(year), new IntWritable(airTemperature))<span class="comment">;</span>
        }
    }
}

MinTemperatureReducer<span class="preprocessor">.java</span>
import java<span class="preprocessor">.io</span><span class="preprocessor">.IOException</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IntWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.Reducer</span><span class="comment">;</span>
public class MinTemperatureReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    @Override
    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)
            throws IOException, InterruptedException {
        int minValue = Integer<span class="preprocessor">.MAX</span>_VALUE<span class="comment">;</span>
        for (IntWritable value : values) {
            minValue = Math<span class="preprocessor">.min</span>(minValue, value<span class="preprocessor">.get</span>())<span class="comment">;</span>
        }
        context<span class="preprocessor">.write</span>(key, new IntWritable(minValue))<span class="comment">;</span>
    }
}

MinTemperature<span class="preprocessor">.java</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.fs</span><span class="preprocessor">.Path</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IntWritable</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.Text</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.Job</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.lib</span><span class="preprocessor">.input</span><span class="preprocessor">.FileInputFormat</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.mapreduce</span><span class="preprocessor">.lib</span><span class="preprocessor">.output</span><span class="preprocessor">.FileOutputFormat</span><span class="comment">;</span>
public class MinTemperature {
    public static void main(String[] args) throws Exception {
        if (args<span class="preprocessor">.length</span> != <span class="number">2</span>) {
            System<span class="preprocessor">.err</span><span class="preprocessor">.println</span>(<span class="string">"Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"</span>)<span class="comment">;</span>
            System<span class="preprocessor">.exit</span>(-<span class="number">1</span>)<span class="comment">;</span>
        }
        Job job = new Job()<span class="comment">;</span>
        job<span class="preprocessor">.setJarByClass</span>(MinTemperature<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setJobName</span>(<span class="string">"Min temperature"</span>)<span class="comment">;</span>
        FileInputFormat<span class="preprocessor">.addInputPath</span>(job, new Path(args[<span class="number">0</span>]))<span class="comment">;</span>
        FileOutputFormat<span class="preprocessor">.setOutputPath</span>(job, new Path(args[<span class="number">1</span>]))<span class="comment">;</span>
        job<span class="preprocessor">.setMapperClass</span>(MinTemperatureMapper<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setReducerClass</span>(MinTemperatureReducer<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setOutputKeyClass</span>(Text<span class="preprocessor">.class</span>)<span class="comment">;</span>
        job<span class="preprocessor">.setOutputValueClass</span>(IntWritable<span class="preprocessor">.class</span>)<span class="comment">;</span>
        System<span class="preprocessor">.exit</span>(job<span class="preprocessor">.waitForCompletion</span>(true) ? <span class="number">0</span> : <span class="number">1</span>)<span class="comment">;</span>
    }
}
</code></pre><p>编译 打包  将jar包移到$HADOOP_HOME, 删除原class文件</p>
<pre><code>[grid@hadoop01 noaa]$ javac -classpath  /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-core-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span> *<span class="preprocessor">.java</span>
[grid@hadoop01 noaa]$ ls
MinTemperature<span class="preprocessor">.class</span>  MinTemperatureMapper<span class="preprocessor">.class</span>  MinTemperatureReducer<span class="preprocessor">.class</span>
MinTemperature<span class="preprocessor">.java</span>   MinTemperatureMapper<span class="preprocessor">.java</span>   MinTemperatureReducer<span class="preprocessor">.java</span>
[grid@hadoop01 noaa]$ hadoop  fs -put  /home/grid/Downloads/noaa/noaaSample<span class="preprocessor">.txt</span>   ./<span class="keyword">in</span>
[grid@hadoop01 noaa]$ hadoop  fs -ls ./<span class="keyword">in</span>
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> grid supergroup   <span class="number">39564893</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">43</span> /user/grid/<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span>
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">12</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span>:<span class="number">57</span> /user/grid/<span class="keyword">in</span>/test1<span class="preprocessor">.txt</span>
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">13</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span>:<span class="number">57</span> /user/grid/<span class="keyword">in</span>/test2<span class="preprocessor">.txt</span>
[grid@hadoop01 noaa]$ jar cvf ./MinTemperature<span class="preprocessor">.jar</span> *<span class="preprocessor">.class</span>
added manifest
<span class="label">adding:</span> MinTemperature<span class="preprocessor">.class</span>(<span class="keyword">in</span> = <span class="number">1418</span>) (<span class="keyword">out</span>= <span class="number">802</span>)(deflated <span class="number">43</span>%)
<span class="label">adding:</span> MinTemperatureMapper<span class="preprocessor">.class</span>(<span class="keyword">in</span> = <span class="number">1876</span>) (<span class="keyword">out</span>= <span class="number">804</span>)(deflated <span class="number">57</span>%)
<span class="label">adding:</span> MinTemperatureReducer<span class="preprocessor">.class</span>(<span class="keyword">in</span> = <span class="number">1664</span>) (<span class="keyword">out</span>= <span class="number">706</span>)(deflated <span class="number">57</span>%)
[grid@hadoop01 noaa]$ ls
MinTemperature<span class="preprocessor">.class</span>  MinTemperature<span class="preprocessor">.java</span>         MinTemperatureMapper<span class="preprocessor">.java</span>    MinTemperatureReducer<span class="preprocessor">.java</span>
MinTemperature<span class="preprocessor">.jar</span>    MinTemperatureMapper<span class="preprocessor">.class</span>  MinTemperatureReducer<span class="preprocessor">.class</span>
[grid@hadoop01 noaa]$ mv MinTemperature<span class="preprocessor">.jar</span> /home/grid/hadoop-<span class="number">1.2</span><span class="number">.1</span>/
[grid@hadoop01 noaa]$ rm  *<span class="preprocessor">.class</span>
[grid@hadoop01 noaa]$ ls
MinTemperature<span class="preprocessor">.java</span>  MinTemperatureMapper<span class="preprocessor">.java</span>  MinTemperatureReducer<span class="preprocessor">.java</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ ls
<span class="number">7287</span>OS_Code  contrib                    Hadoop MapReduce Cookbook<span class="preprocessor">.zip</span>  lib                 NOTICE<span class="preprocessor">.txt</span>  webapps
bin          docs                       hadoop-minicluster-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>   libexec             README<span class="preprocessor">.txt</span>
build<span class="preprocessor">.xml</span>    hadoop-ant-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>       hadoop-test-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>          LICENSE<span class="preprocessor">.txt</span>         sbin
c++          hadoop-client-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>    hadoop-tools-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>         logs                share
CHANGES<span class="preprocessor">.txt</span>  hadoop-core-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>      ivy                            MinTemperature<span class="preprocessor">.jar</span>  src
conf         hadoop-examples-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>  ivy<span class="preprocessor">.xml</span>                        myclass             tmp
</code></pre><p>运行程序</p>
<pre><code>[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop jar ./MinTemperature<span class="preprocessor">.jar</span> MinTemperature ./<span class="keyword">in</span>/noaaSample<span class="preprocessor">.txt</span> ./out2
<span class="label">Warning:</span> $HADOOP_HOME is deprecated.
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> WARN mapred<span class="preprocessor">.JobClient</span>: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">28</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">30</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201404211930_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">31</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">48</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span>:<span class="number">58</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">33</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">01</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201404211930_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">29</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched reduce tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">16513</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Data-local map tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">12911</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">10</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_READ=<span class="number">1652569</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">39565019</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">3419762</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_WRITTEN=<span class="number">10</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">39564893</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output materialized bytes=<span class="number">1652569</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">154375</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce shuffle bytes=<span class="number">1652569</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">300466</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output bytes=<span class="number">1352097</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">176033792</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">7420</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine input records=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">126</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input records=<span class="number">150233</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input groups=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine output records=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">302170112</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce output records=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">1453506560</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span>:<span class="number">04</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">150233</span>
</code></pre><p>查看结果</p>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -<span class="keyword">ls</span> ./out2
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">53</span> /user/<span class="keyword">grid</span>/out2/_SUCCESS
drwxr-xr-x   - <span class="keyword">grid</span> supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span> /user/<span class="keyword">grid</span>/out2/_logs
-rw-r--r--   <span class="number">2</span> <span class="keyword">grid</span> supergroup         <span class="number">10</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">21</span> <span class="number">20</span>:<span class="number">52</span> /user/<span class="keyword">grid</span>/out2/part-r-<span class="number">00000</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -cat ./out2/part-r-<span class="number">00000</span>
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.
<span class="number">2013</span>    -<span class="number">604</span>
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ 
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Map-Reduce" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Map-Reduce/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive简单认识]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_hive/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_hive/</id>
    <published>2014-04-18T11:58:57.000Z</published>
    <updated>2014-04-18T12:44:46.000Z</updated>
    <content type="html"><![CDATA[<h3 id="一、HDFS的基本概念">一、HDFS的基本概念</h3>
<h4 id="1-1、数据块(block)">1.1、数据块(block)</h4>
<p>HDFS(Hadoop Distributed File System)默认的最基本的存储单位是64M的数据块。和普通文件系统相同的是，HDFS中的文件是被分成64M一块的数据块存储的。不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。</p>
<h4 id="1-2、元数据节点(Namenode)和数据节点(datanode)">1.2、元数据节点(Namenode)和数据节点(datanode)</h4>
<p>元数据节点用来管理文件系统的命名空间,其将所有的文件和文件夹的元数据保存在一个文件系统树中。这些信息也会在硬盘上保存成以下文件：命名空间镜像(namespace image)及修改日志(edit log)。其还保存了一个文件包括哪些数据块，分布在哪些数据节点上。然而这些信息并不存储在硬盘上，而是在系统启动的时候从数据节点收集而成的。</p>
<p>数据节点是文件系统中真正存储数据的地方。客户端(client)或者元数据信息(namenode)可以向数据节点请求写入或者读出数据块。其周期性的向元数据节点回报其存储的数据块信息。</p>
<p>从元数据节点(secondary namenode)从元数据节点并不是元数据节点出现问题时候的备用节点，它和元数据节点负责不同的事情。其主要功能就是周期性将元数据节点的命名空间镜像文件和修改日志合并，以防日志文件过大。这点在下面会相信叙述。合并过后的命名空间镜像文件也在从元数据节点保存了一份，以防元数据节点失败的时候，可以恢复。</p>
<h4 id="1-2-1、元数据节点文件夹结构">1.2.1、元数据节点文件夹结构</h4>
<p><img src="/img/Hadoop/Hive/1.png" alt="图1"><br>VERSION文件是java properties文件，保存了HDFS的版本号。</p>
<pre><code>layoutVersion是一个负整数，保存了HDFS的持续化在硬盘上的数据结构的格式版本号。
namespaceID是文件系统的唯一标识符，是在文件系统初次格式化时生成的。
cTime此处为0
storageType表示此文件夹中保存的是元数据节点的数据结构。
<span class="constant">namespaceID</span>=1232737062
<span class="constant">cTime</span>=0
<span class="constant">storageType</span>=NAME_NODE
<span class="constant">layoutVersion</span>=-18
</code></pre><h4 id="1-2-2、文件系统命名空间映像文件及修改日志">1.2.2、文件系统命名空间映像文件及修改日志</h4>
<p>当文件系统客户端(client)进行写操作时，首先把它记录在修改日志中(edit log),元数据节点在内存中保存了文件系统的元数据信息。在记录了修改日志后，元数据节点则修改内存中的数据结构。每次的写操作成功之前，修改日志都会同步(sync)到文件系统。</p>
<p>fsimage文件，也即命名空间映像文件，是内存中的元数据在硬盘上的checkpoint，它是一种序列化的格式，并不能够在硬盘上直接修改。同数据的机制相似，当元数据节点失败时，则最新checkpoint的元数据信息从fsimage加载到内存中，然后逐一重新执行修改日志中的操作。从元数据节点就是用来帮助元数据节点将内存中的元数据信息checkpoint到硬盘上的。<br>checkpoint的过程如下：</p>
<pre><code>从元数据节点通知元数据节点生成新的日志文件，以后的日志都写到新的日志文件中。
从元数据节点用<span class="keyword">http</span> <span class="built_in">get</span>从元数据节点获得fsimage文件及旧的日志文件。
从元数据节点将fsimage文件加载到内存中，并执行日志文件中的操作，然后生成新的fsimage文件。
从元数据节点奖新的fsimage文件用<span class="keyword">http</span> <span class="built_in">post</span>传回元数据节点
元数据节点可以将旧的fsimage文件及旧的日志文件，换为新的fsimage文件和新的日志文件(第一步生成的)，
然后更新fstime文件，写入此次checkpoint的时间。
</code></pre><p>这样元数据节点中的fsimage文件保存了最新的checkpoint的元数据信息，日志文件也重新开始，不会变的很大了。<br><img src="/img/Hadoop/Hive/2.png" alt="图2"></p>
<h4 id="1-2-3、从元数据节点的目录结构">1.2.3、从元数据节点的目录结构</h4>
<p><img src="/img/Hadoop/Hive/3.png" alt="图3"></p>
<h4 id="1-2-4、数据节点的目录结构">1.2.4、数据节点的目录结构</h4>
<p><img src="/img/Hadoop/Hive/4.png" alt="图4"><br>数据节点的VERSION文件格式如下：</p>
<pre><code><span class="setting">namespaceID=<span class="value"><span class="number">1232737062</span></span></span>
<span class="setting">storageID=<span class="value">DS-<span class="number">1640411682</span>-<span class="number">127.0</span>.<span class="number">1.1</span>-<span class="number">50010</span>-<span class="number">1254997319480</span></span></span>
<span class="setting">cTime=<span class="value"><span class="number">0</span></span></span>
<span class="setting">storageType=<span class="value">DATA_NODE</span></span>
<span class="setting">layoutVersion=<span class="value">-<span class="number">18</span></span></span>
</code></pre><p>blk_<id>保存的是HDFS的数据块，其中保存了具体的二进制数据。</p>
<p>blk_<id>.meta保存的是数据块的属性信息：版本信息，类型信息，和checksum。</p>
<p>当一个目录中的数据块到达一定数量的时候，则创建子文件夹来保存数据块及数据块属性信息。</p>
<h3 id="二、数据流(data_flow)">二、数据流(data flow)</h3>
<h4 id="2-1、读文件的过程">2.1、读文件的过程</h4>
<p>客户端(client)用FileSystem的open()函数打开文件，DistributedFileSystem用RPC调用元数据节点，得到文件的数据块信息。对于每一个数据块，元数据节点返回保存数据块的数据节点的地址。DistributedFileSystem返回FSDataInputStream给客户端，用来读取数据。客户端调用stream的read()函数开始读取数据。DFSInputStream连接保存此文件第一个数据块的最近的数据节点。Data从数据节点读到客户端(client)。<br>当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。</p>
<p>当客户端读取完毕数据的时候，调用FSDataInputStream的close函数。在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。失败的数据节点将被记录，以后不再连接。<br><img src="/img/Hadoop/Hive/5.png" alt="图5"></p>
<h4 id="2-2、写文件的过程">2.2、写文件的过程</h4>
<p>客户端调用create()来创建文件，DistributedFileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件。元数据节点首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件。DistributedFileSystem返回DFSOutputStream，客户端用于写数据。</p>
<p>客户端开始写入数据，DFSOutputStream将数据分成块，写入data queue。Data queue由Data Streamer读取，并通知元数据节点分配数据节点，用来存储数据块(每块默认复制3块)。分配的数据节点放在一个pipeline里。Data Streamer将数据块写入pipeline中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。第二个数据节点将数据发送给第三个数据节点。DFSOutputStream为发出去的数据块保存了ack queue，等待pipeline中的数据节点告知数据已经写入成功。</p>
<p>如果数据节点在写入的过程中失败：关闭pipeline，将ack queue中的数据块放入data queue的开始。当前的数据块在已经写入的数据节点中被元数据节点赋予新的标示，则错误节点重启后能够察觉其数据块是过时的，会被删除。失败的数据节点从pipeline中移除，另外的数据块则写入pipeline中的另外两个数据节点。元数据节点则被通知此数据块是复制块数不足，将来会再创建第三份备份。</p>
<p>当客户端结束写入数据，则调用stream的close函数。此操作将所有的数据块写入pipeline中的数据节点，并等待ack queue返回成功。最后通知元数据节点写入完毕。<br><img src="/img/Hadoop/Hive/6.png" alt="图6"></p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hive/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop集群中编译并运行《权威指南》中的例3.2]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_test_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_test_1/</id>
    <published>2014-04-18T11:58:49.000Z</published>
    <updated>2014-04-18T12:15:20.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1-_install_ant">1. install ant</h3>
<h3 id="2-_download_7278OS_Code">2. download 7278OS_Code</h3>
<p><a href="http://www.packtub.com/code_download/11101" target="_blank">http://www.packtub.com/code_download/11101</a></p>
<h3 id="3-vi_/etc/profile">3.vi /etc/profile</h3>
<pre><code><span class="keyword">export</span> HADOOP_HOME=/home/grid/hadoop-<span class="number">1.2</span>.<span class="number">1</span>
</code></pre><h3 id="4-vi_/etc/profile">4.vi /etc/profile</h3>
<pre><code>export <span class="constant">PATH</span>=<span class="regexp">/home/grid</span><span class="regexp">/apache-ant-1.9.3/bin</span><span class="symbol">:</span><span class="variable">$PATH</span>
</code></pre><h3 id="5-vi_/home/grid/hadoop-1-2-1/conf/hadoop-env-sh">5.vi /home/grid/hadoop-1.2.1/conf/hadoop-env.sh</h3>
<pre><code>export <span class="constant">HADOOP_CLASSPATH</span>=<span class="regexp">/home/grid</span><span class="regexp">/hadoop-1.2.1/myclass</span>
</code></pre><h3 id="6-ant_测试">6.ant 测试</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ <span class="keyword">pwd</span>
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ <span class="keyword">ls</span>
build.xml  src
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ ant
Buildfile: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build.xml

compile:
[mkdir] Created dir: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build
[javac] Compiling <span class="number">1</span> <span class="keyword">source</span> <span class="keyword">file</span> to /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build
[jar] Building jar: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/HDFSJavaAPI.jar

BUILD SUCCESSFUL
Total time: <span class="number">7</span> seconds
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ <span class="keyword">ls</span>
build  build.xml  HDFSJavaAPI.jar  src
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ hadoop jar HDFSJavaAPI.jar  HDFSJavaAPIDemo
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

hdfs:<span class="comment">//hadoop01.myhadoop.com:9000</span>
File <span class="keyword">exists</span>.
Welcome to HDFS Java API!!!
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ 
</code></pre><h3 id="7、ant_print-cp">7、ant print-cp</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ ant <span class="keyword">print</span>-cp
Buildfile: /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/build.xml

<span class="keyword">print</span>-cp:
[echo] classpath= /home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/asm-<span class="number">3.2</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/aspectjrt-<span class="number">1.6</span><span class="number">.11</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/aspectjtools-<span class="number">1.6</span><span class="number">.11</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-beanutils-<span class="number">1.7</span><span class="number">.0</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-beanutils-core-<span class="number">1.8</span><span class="number">.0</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-cli-<span class="number">1.2</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-codec-<span class="number">1.4</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-collections-<span class="number">3.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-configuration-<span class="number">1.6</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-daemon-<span class="number">1.0</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-digester-<span class="number">1.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-el-<span class="number">1.0</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-httpclient-<span class="number">3.0</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-io-<span class="number">2.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-lang-<span class="number">2.4</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-logging-<span class="number">1.1</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-logging-api-<span class="number">1.0</span><span class="number">.4</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-math-<span class="number">2.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/commons-net-<span class="number">3.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/core-<span class="number">3.1</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hadoop-capacity-scheduler-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hadoop-fairscheduler-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hadoop-thriftfs-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/hsqldb-<span class="number">1.8</span><span class="number">.0</span><span class="number">.10</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-core-asl-<span class="number">1.8</span><span class="number">.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jackson-mapper-asl-<span class="number">1.8</span><span class="number">.8</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jasper-compiler-<span class="number">5.5</span><span class="number">.12</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jasper-runtime-<span class="number">5.5</span><span class="number">.12</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jdeb-<span class="number">0.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jersey-core-<span class="number">1.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jersey-json-<span class="number">1.8</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jersey-server-<span class="number">1.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jets3t-<span class="number">0.6</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jetty-<span class="number">6.1</span><span class="number">.26</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jetty-util-<span class="number">6.1</span><span class="number">.26</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jsch-<span class="number">0.1</span><span class="number">.42</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jsp-<span class="number">2.1</span>/jsp-<span class="number">2.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/jsp-<span class="number">2.1</span>/jsp-api-<span class="number">2.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/junit-<span class="number">4.5</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/kfs-<span class="number">0.2</span><span class="number">.2</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/log4j-<span class="number">1.2</span><span class="number">.15</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/mockito-all-<span class="number">1.8</span><span class="number">.5</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/oro-<span class="number">2.0</span><span class="number">.8</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/servlet-api-<span class="number">2.5</span>-<span class="number">20081211.</span>jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/slf4j-api-<span class="number">1.4</span><span class="number">.3</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/slf4j-log4j12-<span class="number">1.4</span><span class="number">.3</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/lib/xmlenc-<span class="number">0.52</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter1/build/lib/hadoop-cookbook-chapter1.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter10/C10Samples.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter2/HDFS_Java_API/HDFSJavaAPI.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter3/build/lib/hadoop-cookbook-chapter1.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter3/build/lib/hadoop-cookbook-chapter3.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter4/C4LogProcessor.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter5/build/lib/hadoop-cookbook.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter6/build/lib/hadoop-cookbook-chapter6.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter8/build/lib/hadoop-cookbook-chapter6.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter8/build/lib/hadoop-cookbook-chapter8.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/<span class="number">7287</span>OS_Code/<span class="number">7287</span>OS_Code/chapter9/C9Samples.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/datajoin/hadoop-datajoin-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/failmon/hadoop-failmon-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/gridmix/hadoop-gridmix-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/hdfsproxy/hdfsproxy-<span class="number">2.0</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/index/hadoop-index-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/streaming/hadoop-streaming-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/contrib/vaidya/hadoop-vaidya-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-ant-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-client-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-core-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-examples-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-minicluster-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-test-<span class="number">1.2</span><span class="number">.1</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/hadoop-tools-<span class="number">1.2</span><span class="number">.1</span>.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/ivy/ivy-<span class="number">2.1</span><span class="number">.0</span>.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/thriftfs/lib/hadoopthriftapi.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/contrib/thriftfs/lib/libthrift.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/ftplet-api-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/ftpserver-core-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/ftpserver-server-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar:
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/lib/mina-core-<span class="number">2.0</span><span class="number">.0</span>-M2-<span class="number">20080407.124109</span>-<span class="number">12.</span>jar:/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/src/test/org/apache/hadoop/mapred/test.jar

BUILD SUCCESSFUL
Total time: <span class="number">2</span> seconds
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> HDFS_Java_API]$ 
</code></pre><h3 id="8、copy_到_/home/grid/-bashrc_设置_CLASSPATH">8、copy 到 /home/grid/.bashrc 设置 CLASSPATH</h3>
<p>这一步主要为了编译的时候不指定jar包把刚才的classpath加进去</p>
<pre><code>export CLASSPATH<span class="subst">=</span><span class="variable">$CLASSPATH</span>:刚才ant<span class="attribute">-print</span><span class="attribute">-cp</span>出来的内容
</code></pre><h3 id="9、FileSystemCat-java">9、FileSystemCat.java</h3>
<pre><code>import java<span class="preprocessor">.io</span><span class="preprocessor">.InputStream</span><span class="comment">;</span>
import java<span class="preprocessor">.net</span><span class="preprocessor">.URI</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.conf</span><span class="preprocessor">.Configuration</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.fs</span><span class="preprocessor">.FileSystem</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.fs</span><span class="preprocessor">.Path</span><span class="comment">;</span>
import org<span class="preprocessor">.apache</span><span class="preprocessor">.hadoop</span><span class="preprocessor">.io</span><span class="preprocessor">.IOUtils</span><span class="comment">;</span>

public class FileSystemCat {
        public static void main(String[] args) throws Exception {
                String uri = args[<span class="number">0</span>]<span class="comment">;</span>
                Configuration conf = new Configuration()<span class="comment">;</span>
                FileSystem fs = FileSystem<span class="preprocessor">.get</span>(URI<span class="preprocessor">.create</span>(uri), conf)<span class="comment">;</span>
                InputStream <span class="keyword">in</span> = null<span class="comment">;</span>
                try {
                        <span class="keyword">in</span> = fs<span class="preprocessor">.open</span>(new Path(uri))<span class="comment">;</span>
                        IOUtils<span class="preprocessor">.copyBytes</span>(<span class="keyword">in</span>, System<span class="preprocessor">.out</span>, <span class="number">4096</span>, false)<span class="comment">;</span>
                } finally {
                        IOUtils<span class="preprocessor">.closeStream</span>(<span class="keyword">in</span>)<span class="comment">;</span>
                }
        }
}
</code></pre><h3 id="10、编译运行">10、编译运行</h3>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">pwd</span>
/home/<span class="keyword">grid</span>/hadoop-<span class="number">1.2</span><span class="number">.1</span>/myclass
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">ls</span>
FileSystemCat.class  FileSystemCat.java
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ rm -rf FileSystemCat.class 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ <span class="keyword">ls</span>
FileSystemCat.java
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ javac FileSystemCat.java 
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> myclass]$ hadoop FileSystemCat hdfs:<span class="comment">//hadoop01.myhadoop.com:9000/user/grid/in/test1.txt</span>
Warning: <span class="variable">$HADOOP_HOME</span> is deprecated.

hello world
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive fs常用命令]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_fs/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/18/Hadoop/hadoop_fs/</id>
    <published>2014-04-18T11:34:54.000Z</published>
    <updated>2014-04-18T11:57:41.000Z</updated>
    <content type="html"><![CDATA[<h4 id="好记性不如烂笔头">好记性不如烂笔头</h4>
<h5 id="首先记下hadoop常用的命令：（hadoop_fs_-help列出全部）">首先记下hadoop常用的命令：（hadoop fs -help列出全部）</h5>
<h6 id="1_hadoop_fs_–fs_[local_|_<file_system_URI>]：">1 hadoop fs –fs [local | <file system URI>]：</h6>
<p>声明hadoop使用的文件系统，如果不声明的话，使用当前配置文件配置的，按如下顺序查找：hadoop jar里的hadoop-default.xml-&gt;$HADOOP_CONF_DIR下的hadoop-default.xml-&gt;$HADOOP_CONF_DIR下的hadoop-site.xml。使用local代表将本地文件系统作为hadoop的DFS。如果传递uri做参数，那么就是特定的文件系统作为DFS。</p>
<h6 id="2_hadoop_fs_–ls_<path>：">2 hadoop fs –ls <path>：</h6>
<p>等同于本地系统的ls，列出在指定目录下的文件内容，支持pattern匹配。输出格式如filename(full path)   <r n>  size.其中n代表replica的个数，size代表大小（单位bytes）。</p>
<h6 id="3_hadoop_fs_–lsr_<path>：">3 hadoop fs –lsr <path>：</h6>
<p>递归列出匹配pattern的文件信息，类似ls，只不过递归列出所有子目录信息。</p>
<h6 id="4_hadoop_fs_–du_<path>：">4 hadoop fs –du <path>：</h6>
<p>列出匹配pattern的指定的文件系统空间总量（单位bytes），等价于unix下的针对目录的du –sb <path>/*和针对文件的du –b <path> ，输出格式如name(full path)  size(in bytes)。</p>
<h6 id="5_hadoop_fs_–dus_<path>：">5 hadoop fs –dus <path>：</h6>
<p>等价于-du，输出格式也相同，只不过等价于unix的du -sb。</p>
<h6 id="6_hadoop_fs_–mv_<src>_<dst>：">6 hadoop fs –mv <src> <dst>：</h6>
<p>将制定格式的文件 move到指定的目标位置。当src为多个文件时，dst必须是个目录。</p>
<h6 id="7_hadoop_fs_–cp_<src>_<dst>：">7 hadoop fs –cp <src> <dst>：</h6>
<p>拷贝文件到目标位置，当src为多个文件时，dst必须是个目录。</p>
<h6 id="8_hadoop_fs_–rm_[-skipTrash]_<src>：">8 hadoop fs –rm [-skipTrash] <src>：</h6>
<p>删除匹配pattern的指定文件，等价于unix下的rm <src>。</p>
<h6 id="9_hadoop_fs_–rmr_[skipTrash]_<src>：">9 hadoop fs –rmr [skipTrash] <src>：</h6>
<p>递归删掉所有的文件和目录，等价于unix下的rm –rf <src>。</p>
<h6 id="10_hadoop_fs_–rmi_[skipTrash]_<src>：">10 hadoop fs –rmi [skipTrash] <src>：</h6>
<p>等价于unix的rm –rfi <src>。</p>
<h6 id="11_hadoop_fs_–put_<localsrc>_…_<dst>：">11 hadoop fs –put <localsrc> … <dst>：</h6>
<p>从本地系统拷贝文件到DFS。</p>
<h6 id="12_hadoop_fs_–copyFromLocal_<localsrc>_…_<dst>：">12 hadoop fs –copyFromLocal <localsrc> … <dst>：</h6>
<p>等价于-put。</p>
<h6 id="13_hadoop_fs_–moveFromLocal_<localsrc>_…_<dst>：">13 hadoop fs –moveFromLocal <localsrc> … <dst>：</h6>
<p>等同于-put，只不过源文件在拷贝后被删除。</p>
<h6 id="14_hadoop_fs_–get_[-ignoreCrc]_[-crc]_<src>_<localdst>：">14 hadoop fs –get [-ignoreCrc] [-crc] <src> <localdst>：</h6>
<p>从DFS拷贝文件到本地文件系统，文件匹配pattern，若是多个文件，则dst必须是目录。</p>
<h6 id="15_hadoop_fs_–getmerge_<src>_<localdst>：">15 hadoop fs –getmerge <src> <localdst>：</h6>
<p>顾名思义，从DFS拷贝多个文件、合并排序为一个文件到本地文件系统。</p>
<h6 id="16_hadoop_fs_–cat_<src>：">16 hadoop fs –cat <src>：</h6>
<p>展示文件内容。</p>
<h6 id="17_hadoop_fs_–copyToLocal_[-ignoreCrc]_[-crc]_<src>_<localdst>：">17 hadoop fs –copyToLocal [-ignoreCrc] [-crc] <src> <localdst>：</h6>
<p>等价于-get。</p>
<h6 id="18_hadoop_fs_–mkdir_<path>：">18 hadoop fs –mkdir <path>：</h6>
<p>在指定位置创建目录。</p>
<h6 id="19_hadoop_fs_–setrep_[-R]_[-w]_<rep>_<path/file>：">19 hadoop fs –setrep [-R] [-w] <rep> <path/file>：</h6>
<p>设置文件的备份级别，-R标志控制是否递归设置子目录及文件。</p>
<h6 id="20_hadoop_fs_–chmod_[-R]_<MODE[,MODE]…|OCTALMODE>_PATH…：">20 hadoop fs –chmod [-R] <MODE[,MODE]…|OCTALMODE> PATH…：</h6>
<p>修改文件的权限，-R标记递归修改。MODE为a+r,g-w,+rwx等，OCTALMODE为755这样。</p>
<h6 id="21_hadoop_fs_-chown_[-R]_[OWNER][:[GROUP]]_PATH…：">21 hadoop fs -chown [-R] [OWNER][:[GROUP]] PATH…：</h6>
<p>修改文件的所有者和组。-R表示递归。</p>
<h6 id="22_hadoop_fs_-chgrp_[-R]_GROUP_PATH…：">22 hadoop fs -chgrp [-R] GROUP PATH…：</h6>
<p>等价于-chown … :GROUP …。</p>
<h6 id="23_hadoop_fs_–count[-q]_<path>：">23 hadoop fs –count[-q] <path>：</h6>
<p>计数文件个数及所占空间的详情，输出表格的列的含义依次为：DIR_COUNT,FILE_COUNT,CONTENT_SIZE,FILE_NAME或者如果加了-q的话，还会列出QUOTA,REMAINING_QUOTA,SPACE_QUOTA,REMAINING_SPACE_QUOTA。</p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hive/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用wordcount程序检验hadoop集群搭建是否成功]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/16/Hadoop/hadoop_error_2/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/16/Hadoop/hadoop_error_2/</id>
    <published>2014-04-16T09:17:35.000Z</published>
    <updated>2014-04-16T09:24:09.000Z</updated>
    <content type="html"><![CDATA[<h1 id="创建个input目录，里面有test1-txt、test2-txt">创建个input目录，里面有test1.txt、test2.txt</h1>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd input/
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ <span class="keyword">ls</span>
test1.txt  test2.txt
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ cat test1.txt 
hello world
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> input]$ cat test2.txt 
hello hadoop
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ hadoop fs -<span class="keyword">ls</span>
<span class="keyword">ls</span>: Cannot access .: No such <span class="keyword">file</span> or directory.
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> bin]$ cd ~
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ <span class="keyword">ls</span>
Desktop    Downloads     hadoop-<span class="number">1.2</span><span class="number">.1</span>.tar.gz  Music     Public  Templates
Documents  hadoop-<span class="number">1.2</span><span class="number">.1</span>  input                Pictures  share   Videos
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> ~]$ cd hadoop-<span class="number">1.2</span><span class="number">.1</span>
</code></pre><h1 id="将input的内容放到自定义目录in">将input的内容放到自定义目录in</h1>
<pre><code>[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -put ../input ./<span class="keyword">in</span>
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -ls
<span class="constant">Found</span> <span class="number">1</span> items
drwxr-xr-x   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -ls ./<span class="keyword">in</span>/*
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">12</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>/test1.txt
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">13</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span><span class="symbol">:</span><span class="number">57</span> /user/grid/<span class="keyword">in</span>/test2.txt
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>hadoop fs -cat ./<span class="keyword">in</span>/test1.txt
hello world
[grid<span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>]<span class="variable">$ </span>ls
bin          docs                          hadoop-test-<span class="number">1.2</span>.<span class="number">1</span>.jar   <span class="constant">LICENSE</span>.txt  src
build.xml    hadoop-ant-<span class="number">1.2</span>.<span class="number">1</span>.jar          hadoop-tools-<span class="number">1.2</span>.<span class="number">1</span>.jar  logs         tmp
c++          hadoop-client-<span class="number">1.2</span>.<span class="number">1</span>.jar       ivy                     <span class="constant">NOTICE</span>.txt   webapps
<span class="constant">CHANGES</span>.txt  hadoop-core-<span class="number">1.2</span>.<span class="number">1</span>.jar         ivy.xml                 <span class="constant">README</span>.txt
conf         hadoop-examples-<span class="number">1.2</span>.<span class="number">1</span>.jar     lib                     sbin
contrib      hadoop-minicluster-<span class="number">1.2</span>.<span class="number">1</span>.jar  libexec                 share
</code></pre><h1 id="运行wordcount程序，放到out目录">运行wordcount程序，放到out目录</h1>
<pre><code>[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop jar hadoop-examples-<span class="number">1.2</span><span class="number">.1</span><span class="preprocessor">.jar</span>  wordcount <span class="keyword">in</span> <span class="keyword">out</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">51</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201404161644_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">52</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">07</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">50</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">09</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">17</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">33</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">20</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201404161644_0001
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">29</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched reduce tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">20824</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Data-local map tasks=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">12323</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_READ=<span class="number">55</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">267</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">173563</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_WRITTEN=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">25</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output materialized bytes=<span class="number">61</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">2</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce shuffle bytes=<span class="number">61</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">8</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output bytes=<span class="number">41</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">336338944</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">3230</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine input records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">242</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input groups=<span class="number">3</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine output records=<span class="number">4</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">422449152</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce output records=<span class="number">3</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">2173210624</span>
<span class="number">14</span>/<span class="number">04</span>/<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">23</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">4</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -ls ./<span class="keyword">out</span>
Found <span class="number">3</span> items
-rw-r--r--   <span class="number">2</span> grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span> /user/grid/<span class="keyword">out</span>/_SUCCESS
drwxr-xr-<span class="built_in">x</span>   - grid supergroup          <span class="number">0</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">16</span>:<span class="number">59</span> /user/grid/<span class="keyword">out</span>/_logs
-rw-r--r--   <span class="number">2</span> grid supergroup         <span class="number">25</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">16</span> <span class="number">17</span>:<span class="number">00</span> /user/grid/<span class="keyword">out</span>/part-r-<span class="number">00000</span>
[grid@hadoop01 hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -cat ./<span class="keyword">out</span>/part-r-<span class="number">00000</span>
hadoop  <span class="number">1</span>
hello   <span class="number">2</span>
world   <span class="number">1</span>
</code></pre><h1 id="浏览器访问：">浏览器访问：</h1>
<pre><code><span class="label">http:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="number">50070</span>
<span class="label">http:</span>//hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="number">50030</span>
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hadoop报错：could only be replicated to 0 nodes, instead of 1 解决思路]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/15/Hadoop/hadoop_error_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/15/Hadoop/hadoop_error_1/</id>
    <published>2014-04-15T03:36:28.000Z</published>
    <updated>2014-04-15T04:07:09.000Z</updated>
    <content type="html"><![CDATA[<p>有时候做一些hadoop操作时候，例如上传文件、运行程序时<br>hadoop报错：could only be replicated to 0 nodes, instead of 1</p>
<p>其思路可以这样考虑一下</p>
<h1 id="一、优先考虑看下hadoop的日志，根据日志排错。">一、优先考虑看下hadoop的日志，根据日志排错。</h1>
<h1 id="二、如果不会看日志，注意以下几点做了没：">二、如果不会看日志，注意以下几点做了没：</h1>
<pre><code><span class="number">1</span>、看看各节点的防火墙关了没。
<span class="number">2</span>、这个问题是由于没有添加节点的原因，也就是说需要先启动namenode，
再启动datanode，然后启动jobtracker和tasktracker。
<span class="variable">$hadoop</span><span class="attribute">-daemon</span><span class="built_in">.</span>sh start namenode
<span class="variable">$hadoop</span><span class="attribute">-daemon</span><span class="built_in">.</span>sh start datanode
<span class="number">3</span>、查看配置文件   conf/hdfs<span class="attribute">-site</span><span class="built_in">.</span><span class="built_in">xml</span> 观察配置的域名和IP改了么。
<span class="number">4</span>、实验环境可以这样做，生产环境别这样做：
先把服务都停掉 stop<span class="attribute">-all</span><span class="built_in">.</span>sh
格式化namenode hadoop namenode <span class="attribute">-foramt</span>
重新启动所有服务 start<span class="attribute">-all</span><span class="built_in">.</span>sh
可以进行正常操作了
</code></pre>]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[启动eth0 网卡时报错Error: Connection activation failed: Device not managed by NetworkManager]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Linux/linux_error_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Linux/linux_error_1/</id>
    <published>2014-04-13T02:44:29.000Z</published>
    <updated>2014-04-13T12:07:14.000Z</updated>
    <content type="html"><![CDATA[<p>Centos6.x下解决办法，在虚拟机中，启动eth0 网卡时报错：Error: Connection activation failed: Device not managed by NetworkManager。<br>解决办法：<br>    chkconfig NetworkManager off<br>    chkconfig network on<br>    service NetworkManager stop<br>    service network start<br>这是因为图形界面NetworkManager管理器与network服务有冲突，关上NetworkManager管理器既可<br>附：排查网络问题<br>基本的排错步骤（从上往下）<br>ping 127.0.0.1ping的通说明tcp协议栈没有问题<br>ping 主机地址 ping的通说明网卡没有问题<br>ping 路由器默认网关 ping的通说明包可以到达路由器<br>最后 ping DNS服务器地址<br>卡在哪一步，就查哪里</p>
]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="网卡错误" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E7%BD%91%E5%8D%A1%E9%94%99%E8%AF%AF/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用NFS+DNS+Shell脚本搭建Hadoop1.2.1]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Hadoop/hadoop_install/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/13/Hadoop/hadoop_install/</id>
    <published>2014-04-13T01:24:45.000Z</published>
    <updated>2014-04-13T12:48:42.000Z</updated>
    <content type="html"><![CDATA[<p>今天按照几个帖子把hadoop1.2.1搭建了,生产环境使用hadoop区别与测试环境的主要是：<br>1.用DNS而不是hosts文件解析主机名<br>2.使用NFS共享密钥文件，而不是逐个手工拷贝添加密钥<br>3.复制Hadoop时使用批量拷贝脚本而不是逐台复制</p>
<h1 id="地址规划与安装环境">地址规划与安装环境</h1>
<h2 id="Ip地址规划">Ip地址规划</h2>
<pre><code>Ip地址             主机名                     hadoop角色                 其他用途
<span class="number">192.168</span><span class="number">.18</span><span class="number">.60</span>     hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>     namenode和jobtracker     DNS服务，NFS服务
<span class="number">192.168</span><span class="number">.18</span><span class="number">.61</span>     hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>     datanode和tasktracker
<span class="number">192.168</span><span class="number">.18</span><span class="number">.62</span>     hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>     datanode和tasktracker
</code></pre><h2 id="安装环境">安装环境</h2>
<pre><code>虚拟机软件 VMWare 
安装的操作系统 CentOS <span class="number">6.3</span> x64
java jdk <span class="number">1.7</span>_40 x64
hadoop <span class="number">1.2</span>.<span class="number">1</span>
<span class="built_in">bind</span> <span class="number">9.8</span>.<span class="number">2</span>
</code></pre><h1 id="安装系统">安装系统</h1>
<h2 id="安装CentOS_6-3">安装CentOS 6.3</h2>
<pre><code>安装后注意关闭防火墙：
service iptables <span class="keyword">stop</span>
chkconfig iptables <span class="keyword">off</span>
关闭selinux
vi /etc/selinux/config
将SELINUX=enforcing 改成SELINUX=disabled
</code></pre><h2 id="创建用户">创建用户</h2>
<pre><code>建立hadoop运行账号
在所有的节点创建hadoop运行账号
[root<span class="property">@hadoop01</span> ~]<span class="comment"># groupadd hadoop</span>
[root<span class="property">@hadoop01</span> ~]<span class="comment"># useradd -g hadoop grid --注意此处一定要指定分组，不然可能会不能建立互信</span>
[root<span class="property">@hadoop01</span> ~]<span class="comment"># id grid</span>
uid=<span class="number">501</span>(grid)gid=<span class="number">54326</span>(hadoop) groups=<span class="number">54326</span>(hadoop)
[root<span class="property">@hadoop01</span> ~]<span class="comment"># passwd grid</span>
Changingpassword <span class="keyword">for</span> user grid.
New <span class="attribute">UNIXpassword</span>:
BAD <span class="attribute">PASSWORD</span>: itis too short
Retype <span class="keyword">new</span> <span class="attribute">UNIXpassword</span>:
<span class="attribute">passwd</span>: allauthentication tokens updated successfully.
说明：在大的hadoop集群安装环境中，这步可以在批量安装linux系统之前完成，然后再进行系统的复制。（没
有试过，据说可以ghost工具软件应该可以实现）    
</code></pre><h2 id="安装JDK">安装JDK</h2>
<pre><code>[root<span class="variable">@hadoop01</span> grid]<span class="comment"># tar -zxvf jdk-7u40-linux-x64.tar.gz</span>
[root<span class="variable">@hadoop01</span> grid]<span class="comment"># mv jdk1.7.0_40/ /usr/</span>
</code></pre><h2 id="环境变量">环境变量</h2>
<pre><code>[root@hadoop01 etc]<span class="comment"># vi /etc/profile</span>
<span class="keyword">export</span> JAVA_HOME=/usr/jdk1.<span class="number">7.0</span>_40
<span class="keyword">export</span> HADOOP_FREFIX=/home/grid/hadoop-<span class="number">1.2</span>.<span class="number">1</span>
<span class="keyword">export</span> HADOOP_COMMON_HOME=<span class="variable">${HADOOP_FREFIX}</span>
<span class="keyword">export</span> HADOOP_CONF_DIR=<span class="variable">${HADOOP_FREFIX}</span>/conf
<span class="keyword">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar
<span class="keyword">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">${HADOOP_FREFIX}</span>/bin:<span class="variable">${HADOOP_FREFIX}</span>/sbin:<span class="variable">$PATH</span>
[root@hadoop01 etc]<span class="comment"># source /etc/profile</span>
</code></pre><h2 id="安装本地yum源">安装本地yum源</h2>
<p><a href="http://daminger.github.io/2014/04/09/Linux/linux_yum/" target="_blank">参考我的另外一篇博客</a></p>
<h1 id="安装DNS">安装DNS</h1>
<h2 id="从本地源安装">从本地源安装</h2>
<pre><code>[root<span class="variable">@hadoop01</span> ~]<span class="comment"># yum install bind_libs bind bind-utils</span>
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
Setting up Install Process
No <span class="keyword">package</span> bind_libs available.
Package <span class="number">32</span>:<span class="keyword">bind</span>-utils-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 already installed <span class="keyword">and</span> latest version
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package <span class="keyword">bind</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 will be installed
--&gt; Finished Dependency Resolution
Dependencies Resolved
=========================================================================
=======
Package Arch Version Repository Size
=========================================================================
=======
Installing:
<span class="keyword">bind</span> x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 Media <span class="number">4.0</span> M
Transaction Summary
=========================================================================
=======
Install <span class="number">1</span> Package(<span class="keyword">s</span>)
Total download size: <span class="number">4.0</span> M
Installed size: <span class="number">7.2</span> M
Is this ok [<span class="regexp">y/N]: y
Downloading Packages:
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
Warning: RPMDB altered outside of yum.
Installing : 32:bind-9.8.2-0.10.rc1.el6.x86_64 1/1
/sbin</span><span class="regexp">/ldconfig: /usr</span><span class="regexp">/lib64/libhdfs</span>.so.<span class="number">0</span> is <span class="keyword">not</span> a symbolic <span class="keyword">link</span>
/sbin/ldconfig: <span class="regexp">/usr/lib</span>64/libhadoop.so.<span class="number">1</span> is <span class="keyword">not</span> a symbolic <span class="keyword">link</span>
Verifying : <span class="number">32</span>:<span class="keyword">bind</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 <span class="number">1</span>/<span class="number">1</span>
Installed:
<span class="keyword">bind</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6
Complete!
[root<span class="variable">@hadoop01</span> ~]<span class="comment"># yum install bind_libs bind bind-utils bind-chroot</span>
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
Setting up Install Process
No <span class="keyword">package</span> bind_libs available.
Package <span class="number">32</span>:<span class="keyword">bind</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 already installed <span class="keyword">and</span> latest version
Package <span class="number">32</span>:<span class="keyword">bind</span>-utils-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64 already installed <span class="keyword">and</span> latest version
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package <span class="keyword">bind</span>-<span class="keyword">chroot</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 will be installed
--&gt; Finished Dependency Resolution
Dependencies Resolved
=========================================================================
=======
Package Arch Version Repository Size
=========================================================================
=======
Installing:
<span class="keyword">bind</span>-<span class="keyword">chroot</span> x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6 Media <span class="number">70</span> k
Transaction Summary
=========================================================================
=======
Install <span class="number">1</span> Package(<span class="keyword">s</span>)
Total download size: <span class="number">70</span> k
Installed size: <span class="number">0</span>
Is this ok [<span class="regexp">y/N]: y
Downloading Packages:
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
Installing : 32:bind-chroot-9.8.2-0.10.rc1.el6.x86_64 1/1
Verifying : 32:bind-chroot-9.8.2-0.10.rc1.el6.x86_64 1/</span><span class="number">1</span>
Installed:
<span class="keyword">bind</span>-<span class="keyword">chroot</span>.x86_64 <span class="number">32</span>:<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6
Complete!
安装<span class="keyword">bind</span>-<span class="keyword">chroot</span>，将<span class="keyword">bind</span>的根目录限定在某一个目录之中，增加系统安全性。
[root<span class="variable">@hadoop01</span> ~]<span class="comment"># yum install bind_libs bind bind-utils</span>
检查安装情况：
[root<span class="variable">@hadoop01</span> named]<span class="comment"># rpm -qa bind</span>
<span class="keyword">bind</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64
root<span class="variable">@hadoop01</span> named]<span class="comment"># rpm -qa bind-chroot</span>
<span class="keyword">bind</span>-<span class="keyword">chroot</span>-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64
[root<span class="variable">@hadoop01</span> named]<span class="comment"># rpm -qa bind-utils</span>
<span class="keyword">bind</span>-utils-<span class="number">9.8</span>.<span class="number">2</span>-<span class="number">0</span>.<span class="number">10</span>.rc1.el6.x86_64
将<span class="string">"<span class="variable">$AddUnixListenSocket</span> /var/named/chroot/dev/log"</span>加入/etc/rsyslog.conf文件中，不然rsyslog守护程序将无法记载
<span class="keyword">bind</span>日志。
[root<span class="variable">@hadoop01</span> etc]<span class="comment"># vi rsyslog.conf</span>
<span class="variable">$AddUnixListenSocket</span> /var/named/<span class="keyword">chroot</span>/dev/<span class="keyword">log</span>
</code></pre><h2 id="配置/etc/named-conf">配置/etc/named.conf</h2>
<pre><code>[root<span class="variable">@hadoop01</span> etc]<span class="comment"># vi named.conf</span>
options {
listen-on port <span class="number">53</span> { any; }; 把localhost改为any
listen-on-v6 port <span class="number">53</span> { <span class="symbol">:</span><span class="symbol">:</span><span class="number">1</span>; };
directory <span class="string">"/var/named"</span>;
dump-file <span class="string">"/var/named/data/cache_dump.db"</span>;
statistics-file <span class="string">"/var/named/data/named_stats.txt"</span>;
memstatistics-file <span class="string">"/var/named/data/named_mem_stats.txt"</span>;
allow-query { any; }; 把localhost改为any
recursion yes;
dnssec-enable yes;
dnssec-validation yes;
dnssec-lookaside auto;
<span class="regexp">/* Path to ISC DLV key */</span>
bindkeys-file <span class="string">"/etc/named.iscdlv.key"</span>;
managed-keys-directory <span class="string">"/var/named/dynamic"</span>;
};
logging {
channel default_debug {
file <span class="string">"data/named.run"</span>;
severity dynamic;
};
};
<span class="regexp">//zone</span> <span class="string">"."</span> <span class="constant">IN</span> {
<span class="regexp">//</span> type hint;
<span class="regexp">//</span> file <span class="string">"named.ca"</span>;
<span class="regexp">//</span> };
<span class="keyword">include</span> <span class="string">"/etc/named.rfc1912.zones"</span>;
<span class="regexp">//include</span> <span class="string">"/etc/named.root.key"</span>; 注释掉
</code></pre><h2 id="配置named-rfc1912-zones">配置named.rfc1912.zones</h2>
<pre><code>[root@hadoop01 etc]# vi named.rfc1912.zones
修改为以下内容：
zone <span class="string">"myhadoop.com"</span> IN {
<span class="class"><span class="keyword">type</span> <span class="title">master</span>;</span>
file <span class="string">"myhadoop.com.zone"</span>;
allow-update { none; };
};

zone <span class="string">"18.168.192.in-addr.arpa"</span> IN {
<span class="class"><span class="keyword">type</span> <span class="title">master</span>;</span>
file <span class="string">"18.168.192.zone"</span>;
allow-update { none; };
};
</code></pre><h2 id="创建区域文件">创建区域文件</h2>
<pre><code>需要创建正向记录文件myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>和反向记录文件<span class="number">18.168</span><span class="number">.192</span><span class="preprocessor">.in</span>-addr<span class="preprocessor">.zon</span>，文件应
在/var/named/chroot/var/named目录下
[root@hadoop01 named]<span class="preprocessor"># vi myhadoop.com.zone</span>
$TTL <span class="number">1</span>D
@ <span class="keyword">IN</span> SOA hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. root<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. (
<span class="number">20140221</span> <span class="comment">; serial</span>
<span class="number">1</span>D <span class="comment">; refresh</span>
<span class="number">1</span>H <span class="comment">; retry</span>
<span class="number">1</span>W <span class="comment">; expire</span>
<span class="number">3</span>H ) <span class="comment">; minimum</span>
@ <span class="keyword">IN</span> NS hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>.
hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. <span class="keyword">IN</span> A <span class="number">192.168</span><span class="number">.18</span><span class="number">.60</span>
hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. <span class="keyword">IN</span> A <span class="number">192.168</span><span class="number">.18</span><span class="number">.61</span>
hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. <span class="keyword">IN</span> A <span class="number">192.168</span><span class="number">.18</span><span class="number">.62</span>
[root@hadoop01 named]<span class="preprocessor"># vi 18.168.192.zone</span>
$TTL <span class="number">86400</span>
@ <span class="keyword">IN</span> SOA hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. root<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>. (
<span class="number">1997022700</span> <span class="comment">; Serial</span>
<span class="number">28800</span> <span class="comment">; Refresh</span>
<span class="number">14400</span> <span class="comment">; Retry</span>
<span class="number">3600000</span> <span class="comment">; Expire</span>
<span class="number">86400</span> ) <span class="comment">; Minimum</span>
@ <span class="keyword">IN</span> NS hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>.
<span class="number">60</span> <span class="keyword">IN</span> PTR hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">61</span> <span class="keyword">IN</span> PTR hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
<span class="number">62</span> <span class="keyword">IN</span> PTR hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>
[root@hadoop01 named]<span class="preprocessor"># chgrp named *</span>
[root@hadoop01 named]<span class="preprocessor"># ls -l</span>
total <span class="number">8</span>
-rwxr-xr-<span class="built_in">x</span> <span class="number">1</span> root named <span class="number">532</span> Feb <span class="number">21</span> <span class="number">12</span>:<span class="number">23</span> <span class="number">18.168</span><span class="number">.192</span><span class="preprocessor">.zone</span>
-rwxr-xr-<span class="built_in">x</span> <span class="number">1</span> root named <span class="number">336</span> Feb <span class="number">21</span> <span class="number">11</span>:<span class="number">38</span> myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>
</code></pre><h2 id="修改各节点/etc/resolv-conf文件">修改各节点/etc/resolv.conf文件</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># vi /etc/resolv.conf</span>
nameserver <span class="number">192.168</span>.<span class="number">18.60</span>
</code></pre><h2 id="检查">检查</h2>
<pre><code>[root@hadoop01 named]<span class="preprocessor"># named-checkzone node.myhadoop.com /var/named/chroot/var/named/myhadoop.com.zone</span>
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">9</span>: ignoring <span class="keyword">out</span>-of-zone data (hadoop01<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">10</span>: ignoring <span class="keyword">out</span>-of-zone data (hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">11</span>: ignoring <span class="keyword">out</span>-of-zone data (hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
/var/named/chroot/var/named/myhadoop<span class="preprocessor">.com</span><span class="preprocessor">.zone</span>:<span class="number">12</span>: ignoring <span class="keyword">out</span>-of-zone data (node3<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>)
zone node<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span>/<span class="keyword">IN</span>: loaded serial <span class="number">20140221</span>
OK
[root@hadoop01 named]<span class="preprocessor"># named-checkzone 192.168.18.60 /var/named/chroot/var/named/18.168.192.zone</span>
zone <span class="number">192.168</span><span class="number">.18</span><span class="number">.60</span>/<span class="keyword">IN</span>: loaded serial <span class="number">1997022700</span>
OK
</code></pre><h2 id="启动服务">启动服务</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># service named restart</span>
检查：
[root<span class="variable">@hadoop01</span> named]<span class="comment"># nslookup hadoop01.myhadoop.com</span>
</code></pre><h2 id="配置开机自启动">配置开机自启动</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig named on</span>
</code></pre><h1 id="配置/安装NFS">配置/安装NFS</h1>
<h2 id="检查-1">检查</h2>
<pre><code>[root@hadoop01 named]<span class="preprocessor"># rpm -qa |grep nfs</span>
nfs-utils-<span class="number">1.2</span><span class="number">.3</span>-<span class="number">26.</span>el6<span class="preprocessor">.x</span>86_64
nfs4-acl-tools-<span class="number">0.3</span><span class="number">.3</span>-<span class="number">6.</span>el6<span class="preprocessor">.x</span>86_64
nfs-utils-lib-<span class="number">1.1</span><span class="number">.5</span>-<span class="number">4.</span>el6<span class="preprocessor">.x</span>86_64
说明nfs已安装，如果没有安装可通过命令：
yum install nfs-utils
</code></pre><h2 id="启动服务-1">启动服务</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># service rpcbind restart</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># service nfs restart</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># service nfslock restart</span>
</code></pre><h2 id="配置开机自启动-1">配置开机自启动</h2>
<pre><code>[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig rpcbind on</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig nfs on</span>
[root<span class="variable">@hadoop01</span> named]<span class="comment"># chkconfig nfslock on</span>
</code></pre><h2 id="设置共享目录">设置共享目录</h2>
<pre><code>[root<span class="variable">@hadoop01</span> ~]<span class="comment"># vi /etc/exports</span>
增加行：
/home/grid /share *(insecure,rw,async,no_root_squash)
说明：
/home/grid/share 是<span class="constant">NFS</span>要共享的目录
* 代表所有ip地址
rw为读写，ro为只读
<span class="constant">Sync</span>为立刻写入硬盘，rsync为优先写入缓存
<span class="constant">No_root_squas</span> root用户具有根目录的完全管理访问权限（这个如果不配置会造成远程root用户只读）
重启nfs服务：
[root<span class="variable">@hadoop01</span> ~]<span class="comment"># service nfs restart</span>
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">daemon:</span> [ <span class="constant">OK</span> ]
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">mountd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">quotas:</span> [ <span class="constant">OK</span> ]
<span class="constant">Shutting</span> down <span class="constant">NFS</span> <span class="symbol">services:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">services:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">quotas:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">mountd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Stopping</span> <span class="constant">RPC</span> <span class="symbol">idmapd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">RPC</span> <span class="symbol">idmapd:</span> [ <span class="constant">OK</span> ]
<span class="constant">Starting</span> <span class="constant">NFS</span> <span class="symbol">daemon:</span> [ <span class="constant">OK</span> ]
</code></pre><h2 id="配置挂载目录">配置挂载目录</h2>
<pre><code>显示本地挂载点，即hadoop01的共享目录：
[root<span class="property">@hadoop01</span> etc]<span class="comment"># showmount -e localhost</span>
Export list <span class="keyword">for</span> <span class="attribute">localhost</span>:
/home/grid /share *
[root<span class="property">@hadoop01</span> etc]<span class="comment"># mkdir /nfs_share</span>
[root<span class="property">@hadoop01</span> etc]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
[root<span class="property">@hadoop01</span> etc]<span class="comment"># cd /nfs_share/</span>
-<span class="attribute">bash</span>: <span class="attribute">cd</span>: <span class="regexp">/nfs_share/</span>: Permission denied
[root<span class="property">@hadoop01</span> etc]<span class="comment"># su - grid</span>
[grid<span class="property">@hadoop01</span> ~]$ cd <span class="regexp">/nfs_share/</span>
用同样的方法在其它客户端（hadoop02，hadoop03）上操作
hadoop02：
[root<span class="property">@hadoop02</span> ~]<span class="comment"># mkdir /nfs_share</span>
[root<span class="property">@hadoop02</span> ~]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
hadoop03
[root<span class="property">@hadoop03</span> ~]<span class="comment"># mkdir /nfs_share</span>
[root<span class="property">@hadoop03</span> ~]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
</code></pre><h2 id="设置开机后自动挂载nfs共享目录">设置开机后自动挂载nfs共享目录</h2>
<pre><code>[root<span class="variable">@hadoop01</span> ~]<span class="comment"># vi /etc/fstab</span>
增加行：
<span class="number">192.168</span>.<span class="number">18.60</span><span class="symbol">:/home/grid/share</span> /nfs_share nfs defaults <span class="number">1</span> <span class="number">1</span>
相同方法设置hadoop02，hadoop03
</code></pre><h1 id="使用NFS做免密码登录">使用NFS做免密码登录</h1>
<p>　　我们把每个节点生成的RSA密钥对中的公钥整合到共享目录的authorized_keys文件中，这样做的好处是当我们有新的节点<br>接入时，不再需要分别向其它节点各自添加自己公钥信息，只需要把公钥信息追加到共享的authorized_keys公钥当中，其它节<br>点可直接访问最新的公钥文件。</p>
<h2 id="生成密钥对">生成密钥对</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>ssh-keygen -t rsa
在其他节点上生成密钥对
</code></pre><h2 id="整合authorized_keys">整合authorized_keys</h2>
<pre><code>[grid@hadoop01 <span class="preprocessor">.ssh</span>]$ <span class="keyword">cp</span> ~/<span class="preprocessor">.ssh</span>/id_rsa<span class="preprocessor">.pub</span> authorized_keys
[grid@hadoop01 <span class="preprocessor">.ssh</span>]$ ssh hadoop02<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span> cat ~/<span class="preprocessor">.ssh</span>/id_rsa<span class="preprocessor">.pub</span> &gt;&gt; ~/<span class="preprocessor">.ssh</span>/authorized_keys
[grid@hadoop01 <span class="preprocessor">.ssh</span>]$ ssh hadoop03<span class="preprocessor">.myhadoop</span><span class="preprocessor">.com</span> cat ~/<span class="preprocessor">.ssh</span>/id_rsa<span class="preprocessor">.pub</span> &gt;&gt; ~/<span class="preprocessor">.ssh</span>/authorized_keys
</code></pre><h2 id="创建共享目录文件authorized_keys的软连接">创建共享目录文件authorized_keys的软连接</h2>
<pre><code>在hadoop02：
[grid<span class="variable">@hadoop02</span> .ssh]<span class="variable">$ </span>ln -s /nfs_share/.ssh/authorized_keys ~<span class="regexp">/.ssh/authorized</span>_keys
在hadoop03：
[grid<span class="variable">@hadoop03</span> .ssh]<span class="variable">$ </span>ln -s /nfs_share/.ssh/authorized_keys ~<span class="regexp">/.ssh/authorized</span>_keys
</code></pre><h2 id="验证">验证</h2>
<pre><code><span class="attr_selector">[grid@hadoop01 .ssh]</span>$ <span class="tag">ssh</span> <span class="tag">hadoop02</span><span class="class">.myhadoop</span><span class="class">.com</span>
<span class="tag">Last</span> <span class="tag">login</span>: <span class="tag">Fri</span> <span class="tag">Feb</span> 21 15<span class="pseudo">:40</span><span class="pseudo">:14</span> 2014 <span class="tag">from</span> 192<span class="class">.168</span><span class="class">.18</span><span class="class">.60</span>
<span class="attr_selector">[grid@hadoop02 ~]</span>$
</code></pre><h1 id="安装hadoop系统">安装hadoop系统</h1>
<p>请先在各节点安装jdk</p>
<h2 id="解压hadoop">解压hadoop</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>tar -zxvf hadoop-<span class="number">1.2</span>.<span class="number">1</span>.tar.gz
</code></pre><h2 id="修改配置文件">修改配置文件</h2>
<pre><code>在/home/grid/hadoop-1.2.1/conf下：
[grid@hadoop01 conf]$ vi hadoop-env.sh
修改jdk路径
export JAVA_HOME=/usr/jdk1.7.0_40
[grid@hadoop01 conf]$ vi core-site.xml
<span class="tag">&lt;<span class="title">configuration</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://hadoop01.myhadoop.com:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/grid/hadoop-1.2.1/tmp<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
[grid@hadoop01 conf]$ vi hdfs-site.xml
<span class="tag">&lt;<span class="title">configuration</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>2<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;/<span class="title">configuration</span>&gt;</span>
[grid@hadoop01 conf]$ vi mapred-site.xml
<span class="tag">&lt;<span class="title">configuration</span>&gt;</span>
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
<span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
<span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop01.myhadoop.com:9001<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
<span class="tag">&lt;/<span class="title">configuration</span>&gt;</span>
[grid@hadoop01 conf]$ vi masters
hadoop01.myhadoop.com
[grid@hadoop01 conf]$ vi slaves
hadoop02.myhadoop.com
hadoop03.myhadoop.com
</code></pre><h2 id="使用awk生成分发脚本">使用awk生成分发脚本</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>cat slaves| awk <span class="string">'{print "scp -rp hadoop-1.2.1
grid@"$1":/home/grid"}'</span> &gt; cphadoop.sh
[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>cat cphadoop.sh
scp -rp hadoop-<span class="number">1.2</span>.<span class="number">1</span> grid<span class="variable">@hadoop02</span>.myhadoop.<span class="symbol">com:</span>/home/grid
scp -rp hadoop-<span class="number">1.2</span>.<span class="number">1</span> grid<span class="variable">@hadoop03</span>.myhadoop.<span class="symbol">com:</span>/home/grid
[grid<span class="variable">@hadoop01</span> conf]<span class="variable">$ </span>chmod <span class="number">755</span> cphadoop.sh
</code></pre><h2 id="分发">分发</h2>
<pre><code>[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>cd
[grid<span class="variable">@hadoop01</span> ~]<span class="variable">$ </span>hadoop-<span class="number">1.2</span>.<span class="number">1</span>/conf/cphadoop.sh
</code></pre><h2 id="格式化namenode">格式化namenode</h2>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ bin/hadoop namenode -<span class="keyword">format</span>
</code></pre><h2 id="启动hadoop集群">启动hadoop集群</h2>
<pre><code>[grid@hadoop01 hadoop-1.2.1]$ bin/start-all.sh
[grid@hadoop01 hadoop-1.2.1]$ hadoop dfsadmin -report
Configured Capacity: 79413821440 (73.96 GB)
Present Capacity: 60468486174 (56.32 GB)
DFS Remaining: 60468428800 (56.32 GB)
DFS Used: 57374 (56.03 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
<span class="header">Missing blocks: 0
-------------------------------------------------</span>
Datanodes available: 2 (2 total, 0 dead)
Name: 192.168.18.61:50010
Decommission Status : Normal
Configured Capacity: 39706910720 (36.98 GB)
DFS Used: 28687 (28.01 KB)
Non DFS Used: 9218297841 (8.59 GB)
DFS Remaining: 30488584192(28.39 GB)
DFS Used%: 0%
DFS Remaining%: 76.78%
Last contact: Fri Feb 21 17:06:28 CST 2014
Name: 192.168.18.62:50010
Decommission Status : Normal
Configured Capacity: 39706910720 (36.98 GB)
DFS Used: 28687 (28.01 KB)
Non DFS Used: 9727037425 (9.06 GB)
DFS Remaining: 29979844608(27.92 GB)
DFS Used%: 0%
DFS Remaining%: 75.5%
Last contact: Fri Feb 21 17:06:27 CST 2014
</code></pre><h2 id="验证-1">验证</h2>
<pre><code>[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ jps
<span class="number">3560</span> SecondaryNameNode
<span class="number">3634</span> JobTracker
<span class="number">3788</span> Jps
<span class="number">3392</span> NameNode
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -mkdir test
[<span class="keyword">grid</span><span class="variable">@hadoop01</span> hadoop-<span class="number">1.2</span><span class="number">.1</span>]$ hadoop fs -<span class="keyword">ls</span>
Found <span class="number">1</span> items
drwxr-xr-x - <span class="keyword">grid</span> supergroup <span class="number">0</span> <span class="number">2014</span>-<span class="number">02</span>-<span class="number">21</span> <span class="number">17</span>:<span class="number">08</span> /user/<span class="keyword">grid</span>/test
</code></pre><h1 id="问题">问题</h1>
<h2 id="挂载问题">挂载问题</h2>
<pre><code>挂载共享目录出现无法挂载点问题
[root<span class="variable">@hadoop01</span> nfs_share]<span class="comment"># mount -t nfs 192.168.18.60:/home/grid/share /nfs_share/</span>
mount.<span class="symbol">nfs:</span> access denied by server <span class="keyword">while</span> mounting <span class="number">192.168</span>.<span class="number">18.60</span><span class="symbol">:/home/grid/share</span>
解决方法：
修改挂载属性
[root<span class="variable">@hadoop01</span> nfs_share]<span class="comment"># vi /etc/exports</span>
/home/grid/share *(insecure,rw,async,no_root_squash)
~
</code></pre><h2 id="访问问题">访问问题</h2>
<pre><code>配置完免密码登录后，用ssh登录远程机器每次都要求输入密码，原来是目录访问权限问题
/home/<span class="keyword">grid</span>/share目录与/home/<span class="keyword">grid</span>/share/.ssh目录的访问权限应为<span class="number">700</span>，
authorized_keys文件的访问权限应为<span class="number">644</span>
[root<span class="variable">@hadoop01</span> <span class="keyword">grid</span>]# chmod <span class="number">700</span> share
[root<span class="variable">@hadoop01</span> <span class="keyword">grid</span>]# cd share
[root<span class="variable">@hadoop01</span> share]# chmod <span class="number">700</span> .ssh
这是另外一篇的参考 它的共享目录是/home/<span class="keyword">grid</span> 
h1免密码登录h2：发现无法免密码登录，修改了/etc/ssh/sshd_config，将StrictModes改为no就可以了
一定要注意文件权限
/home/<span class="keyword">grid</span> <span class="number">700</span>
/home/<span class="keyword">grid</span>/.ssh <span class="number">700</span>
/home/<span class="keyword">grid</span>/.ssh/authorized_keys <span class="number">644</span>
</code></pre><h2 id="格式化namenode问题">格式化namenode问题</h2>
<pre><code>[grid@hadoop01 bin]$ ./hadoop namenode -format
14/02/21 16:44:54 INFO namenode.NameNode: STARTUP_MSG:
/<span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span><span class="strong">*****</span>
STARTUP_MSG: Starting NameNode
STARTUP_MSG: host = java.net.UnknownHostException: hadoop01: hadoop01: Name or service not known
STARTUP_MSG: args = [-format]
STARTUP_MSG: version = 1.2.1
STARTUP_MSG: build = https://svn.apache.org/repos/asf ... branches/branch-1.2 -r 1503152; compiled by 'mattf' on
Mon Jul 22 15:23:09 PDT 2013
STARTUP<span class="emphasis">_MSG: java = 1.7.0_</span>40
解决方法：
hostname为hadoop01，系统没有解析，应把hostname设为hadoop01.myhadoop.com
在线修改
hostname hadoop01.myhadoop.com
永久保存：
vi /etc/sysconfig/network
HOSTNAME=hadoop01.myhadoop.com
其他节点也要修改
</code></pre><p>文章参考</p>
<p><a href="http://blog.itpub.net/26686207/viewspace-742502/" target="_blank">帖子1</a></p>
<p><a href="http://blog.itpub.net/26686207/viewspace-753510/" target="_blank">帖子2</a></p>
<p><a href="http://blog.csdn.net/lichangzai/article/details/8647242" target="_blank">帖子3</a></p>
<p><a href="http://f.dataguru.cn/thread-222047-1-1.html" target="_blank">帖子4</a></p>
<p><a href="http://f.dataguru.cn/thread-244415-1-1.html" target="_blank">帖子5</a></p>
]]></content>
    
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Centos 6.4克隆系统后网卡eth0变成eth1的解决办法]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/11/Linux/linux_ethX_problem/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/11/Linux/linux_ethX_problem/</id>
    <published>2014-04-11T14:31:24.000Z</published>
    <updated>2014-04-11T14:40:08.000Z</updated>
    <content type="html"><![CDATA[<p>vbox 或者vmware克隆系统后里面的eth0变成了eht1，开始没有发现问题，当我/etc/init.d/network restart重启的时候，发现找不到eth0，ifconfig 后只有eth1，原来linux使用udev动态管理设备文件，并根据设备的信息对其进行持久化命名。克隆时就自动变成eth1了。还好找到了解决办法，于是备份并分享之。</p>
<pre><code>vi /etc/udev/rules.d/70-persistent-net.rules
dd删除关于eth0的记录或者是用<span class="comment">#注释掉eth0的记录(谨慎的可以注释掉，不过完全没有用)</span>
记住其中eth1的mac地址 复制到文本中，备用
将eth1改成eth0
vi /etc/sysconfig/network-scripts/ifcfg-eth0
将老文件eth0的mac地址换掉，(如果细心的话会发现ifconfig eth1会跟配置文件中的mac不一样)
<span class="constant">DEVICE</span>=eth0
<span class="constant">HWADDR</span>=08:00:27:b7:bb:f7            //将备用的mac地址替换
<span class="constant">TYPE</span>=Ethernet
<span class="constant">UUID</span>=010da721-97c3-49f6-b95d-fe485634956a
<span class="constant">ONBOOT</span>=yes
<span class="constant">NM_CONTROLLED</span>=yes
<span class="constant">BOOTPROTO</span>=dhcp                       //自动IP拨号
<span class="constant">BOOTPROTO</span>=static                    //静态IP设置
<span class="constant">IPADDR</span>=192.168.1.111
<span class="constant">NETMASK</span>=255.255.255.0
<span class="constant">BROADCAST</span>=192.168.1.255
<span class="constant">GATEWAY</span>=192.168.1.1
/etc/init.d/network restart     //重启network--如果不行的话
就shutdown -r now//重启系统了
</code></pre>]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[三次握手，四次挥手]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/internet_1/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/10/计算机网络/internet_1/</id>
    <published>2014-04-10T11:47:37.000Z</published>
    <updated>2014-04-10T12:44:22.000Z</updated>
    <content type="html"><![CDATA[<h1 id="一、TCP报文格式">一、TCP报文格式</h1>
<p>下面是TCP报文格式图：<br><img src="/img/Internet/Internet_1.png" alt="图1 TCP报文格式"><br>上图中有几个字段需要重点介绍下：<br>（1）序号：Seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记。<br>（2）确认序号：Ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，Ack=Seq+1。<br>（3）标志位：共6个，即URG、ACK、PSH、RST、SYN、FIN等，具体含义如下：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（A）URG：紧急指针（urgent pointer）有效。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（B）ACK：确认序号有效。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（C）PSH：接收方应该尽快将这个报文交给应用层。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（D）RST：重置连接。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（E）SYN：发起一个新连接。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（F）FIN：释放一个连接。<br>需要注意的是：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（A）不要将确认序号Ack与标志位中的ACK搞混了。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（B）确认方Ack=发起方Req+1，两端配对。 </p>
<h1 id="二、三次握手">二、三次握手</h1>
<p>所谓三次握手（Three-Way Handshake）即建立TCP连接，就是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发，整个流程如下图所示：<br><img src="/img/Internet/Internet_2.png" alt="图2 TCP三次握手"><br>（1）第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。<br>（2）第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。<br>（3）第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。</p>
<p>SYN攻击：<br>在三次握手过程中，Server发送SYN-ACK之后，收到Client的ACK之前的TCP连接称为半连接（half-open connect），此时Server处于SYN_RCVD状态，当收到ACK后，Server转入ESTABLISHED状态。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。SYN攻击时一种典型的DDOS攻击，检测SYN攻击的方式非常简单，即当Server上有大量半连接状态且源IP地址是随机的，则可以断定遭到SYN攻击了，使用如下命令可以让之现行：</p>
<pre><code><span class="preprocessor">#netstat -nap | grep SYN_RECV</span>
</code></pre><h1 id="三、四次挥手">三、四次挥手</h1>
<p>三次握手耳熟能详，四次挥手估计就不太熟了，所谓四次挥手（Four-Way Wavehand）即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发，整个流程如下图所示<br><img src="/img/Internet/Internet_3.png" alt="图3 TCP四次挥手"></p>
<p>由于TCP连接时全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭，上图描述的即是如此。<br>（1）第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。<br>（2）第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。<br>（3）第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。<br>（4）第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。</p>
<p> 上面是一方主动关闭，另一方被动关闭的情况，实际中还会出现同时发起主动关闭的情况，具体流程如下图<br><img src="/img/Internet/Internet_4.png" alt="图4 同时挥手"></p>
<h1 id="四、附注">四、附注</h1>
<p>关于三次握手与四次挥手通常都会有典型的面试题，在此提出供有需求的同学们参考：<br>（1）三次握手是什么或者流程？四次握手呢？答案前面分析就是。<br>（2）为什么建立连接是三次握手，而关闭连接却是四次挥手呢？</p>
<p> 这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。        </p>
<p> <a href="http://blog.chinaunix.net/uid-7411781-id-3812206.html" target="_blank">   原文链接，请点击这里    </a></p>
]]></content>
    
    
      <category term="网络" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="计算机网络" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[在Linux CentOS6.3下编译安装MySQL5.6.10]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_mysql_install/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_mysql_install/</id>
    <published>2014-04-09T12:15:51.000Z</published>
    <updated>2014-04-13T12:02:26.000Z</updated>
    <content type="html"><![CDATA[<h2 id="编译安装MySQL前的准备工作">编译安装MySQL前的准备工作</h2>
<pre><code>安装编译源码所需的工具和库
yum install gcc gcc-c++ ncurses-devel perl
安装cmake，从http://www<span class="preprocessor">.cmake</span><span class="preprocessor">.org</span>下载源码并编译安装
wget http://www<span class="preprocessor">.cmake</span><span class="preprocessor">.org</span>/files/v2<span class="number">.8</span>/cmake-<span class="number">2.8</span><span class="number">.10</span><span class="number">.2</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
tar -xzvf cmake-<span class="number">2.8</span><span class="number">.10</span><span class="number">.2</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
cd cmake-<span class="number">2.8</span><span class="number">.10</span><span class="number">.2</span>
./bootstrap <span class="comment">; make ; make install</span>
cd ~
</code></pre><h2 id="设置MySQL用户和组">设置MySQL用户和组</h2>
<pre><code>新增mysql用户组
groupadd mysql
新增mysql用户
useradd <span class="attribute">-r</span> <span class="attribute">-g</span> mysql mysql
</code></pre><h2 id="新建MySQL所需要的目录">新建MySQL所需要的目录</h2>
<pre><code>新建mysql安装目录
mkdir <span class="attribute">-p</span> /usr/<span class="built_in">local</span>/mysql
新建mysql数据库数据文件目录
mkdir <span class="attribute">-p</span> /<span class="built_in">data</span>/mysqldb
</code></pre><h2 id="下载MySQL源码包并解压">下载MySQL源码包并解压</h2>
<pre><code>从http://dev<span class="preprocessor">.mysql</span><span class="preprocessor">.com</span>/downloads/mysql/直接下载源码
解压mysql-<span class="number">5.6</span><span class="number">.10</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
tar -zxv -f mysql-<span class="number">5.6</span><span class="number">.10</span><span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
cd mysql-<span class="number">5.6</span><span class="number">.10</span>
</code></pre><h2 id="编译安装MySQL">编译安装MySQL</h2>
<pre><code>从mysql5.5起，mysql源码安装开始使用cmake了，设置源码编译配置脚本。
-<span class="ruby"><span class="constant">DCMAKE_INSTALL_PREFIX</span>=dir_name    设置mysql安装目录
</span>-<span class="ruby"><span class="constant">DMYSQL_UNIX_ADDR</span>=file_name    设置监听套接字路径，这必须是一个绝对路径名。默认为/tmp/mysql.sock的中。
</span>-<span class="ruby"><span class="constant">DDEFAULT_CHARSET</span>=charset_name    设置服务器的字符集。缺省情况下，<span class="constant">MySQL</span>使用latin1的（<span class="constant">CP1252</span>西欧）字符集。
</span>cmake/character_sets.cmake文件包含允许的字符集名称列表。
-<span class="ruby"><span class="constant">DDEFAULT_COLLATION</span>=collation_name    设置服务器的排序规则。
</span>-<span class="ruby"><span class="constant">DWITH_INNOBASE_STORAGE_ENGINE</span>=<span class="number">1</span>
</span>-<span class="ruby"><span class="constant">DWITH_ARCHIVE_STORAGE_ENGINE</span>=<span class="number">1</span>
</span>-<span class="ruby"><span class="constant">DWITH_BLACKHOLE_STORAGE_ENGINE</span>=<span class="number">1</span>
</span>-<span class="ruby"><span class="constant">DWITH_PERFSCHEMA_STORAGE_ENGINE</span>=<span class="number">1</span>    存储引擎选项：
</span>MyISAM，MERGE，MEMORY，和CSV引擎是默认编译到服务器中，并不需要明确地安装。
静态编译一个存储引擎到服务器，使用-DWITH_engine_STORAGE_ENGINE= 1。
可用的存储引擎值有：ARCHIVE, BLACKHOLE, EXAMPLE, FEDERATED, INNOBASE (InnoDB),   PARTITION (partitioning support), 和 PERFSCHEMA (Performance Schema).
-<span class="ruby"><span class="constant">DMYSQL_DATADIR</span>=dir_name    设置mysql数据库文件目录
</span>-<span class="ruby"><span class="constant">DMYSQL_TCP_PORT</span>=port_num    设置mysql服务器监听端口，默认为<span class="number">3306</span>
</span>-<span class="ruby"><span class="constant">DENABLE_DOWNLOADS</span>=bool    是否要下载可选的文件。例如，启用此选项（设置为<span class="number">1</span>），cmake将下载谷歌所使用的测试套件运行单元测试。
</span>设置编译参数
cmake \
-<span class="ruby"><span class="constant">DCMAKE_INSTALL_PREFIX</span>=<span class="regexp">/usr/local</span><span class="regexp">/mysql \
</span></span>-<span class="ruby"><span class="constant">DMYSQL_UNIX_ADDR</span>=<span class="regexp">/usr/local</span><span class="regexp">/mysql/mysql</span>.sock \
</span>-<span class="ruby"><span class="constant">DDEFAULT_CHARSET</span>=utf8 \
</span>-<span class="ruby"><span class="constant">DDEFAULT_COLLATION</span>=utf8_general_ci \
</span>-<span class="ruby"><span class="constant">DWITH_INNOBASE_STORAGE_ENGINE</span>=<span class="number">1</span> \
</span>-<span class="ruby"><span class="constant">DWITH_ARCHIVE_STORAGE_ENGINE</span>=<span class="number">1</span> \
</span>-<span class="ruby"><span class="constant">DWITH_BLACKHOLE_STORAGE_ENGINE</span>=<span class="number">1</span> \
</span>-<span class="ruby"><span class="constant">DMYSQL_DATADIR</span>=<span class="regexp">/data/mysqldb</span> \
</span>-<span class="ruby"><span class="constant">DMYSQL_TCP_PORT</span>=<span class="number">3306</span> \
</span>-<span class="ruby"><span class="constant">DENABLE_DOWNLOADS</span>=<span class="number">1</span>
</span>注：重新运行配置，需要删除CMakeCache.txt文件
rm CMakeCache.txt
编译源码
make
安装
make install
</code></pre><h2 id="修改mysql目录所有者和组">修改mysql目录所有者和组</h2>
<pre><code>修改mysql安装目录
cd /usr/<span class="built_in">local</span>/mysql
chown <span class="attribute">-R</span> mysql:mysql <span class="built_in">.</span>
修改mysql数据库文件目录
cd /<span class="built_in">data</span>/mysqldb
chown <span class="attribute">-R</span> mysql:mysql <span class="built_in">.</span>
</code></pre><h2 id="初始化mysql数据库">初始化mysql数据库</h2>
<pre><code><span class="comment">cd</span> <span class="comment">/usr/local/mysql</span>
<span class="comment">scripts/mysql_install_db</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">user=mysql</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">datadir=/data/mysqldb</span>
</code></pre><h2 id="复制mysql服务启动配置文件">复制mysql服务启动配置文件</h2>
<pre><code>cp /usr/<span class="keyword">local</span>/mysql/support-files/<span class="keyword">my</span>-default.cnf /etc/<span class="keyword">my</span>.cnf
注：如果/etc/<span class="keyword">my</span>.cnf文件存在，则覆盖。
</code></pre><h2 id="复制mysql服务启动脚本及加入PATH路径">复制mysql服务启动脚本及加入PATH路径</h2>
<pre><code>cp support-files/mysql.server /etc/init.d/mysqld
vim /etc/profile
<span class="constant">PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/mysql/bin</span><span class="symbol">:/usr/local/mysql/lib</span><span class="symbol">:</span><span class="variable">$PATH</span>
export <span class="constant">PATH</span>
source /etc/profile
</code></pre><h2 id="启动mysql服务并加入开机自启动">启动mysql服务并加入开机自启动</h2>
<pre><code>service mysqld <span class="operator"><span class="keyword">start</span>
chkconfig --<span class="keyword">level</span> <span class="number">35</span> mysqld <span class="keyword">on</span></span>
</code></pre><h2 id="检查mysql服务是否启动">检查mysql服务是否启动</h2>
<pre><code>netstat <span class="attribute">-tulnp</span> <span class="subst">|</span> grep <span class="number">3306</span>
mysql <span class="attribute">-u</span> root <span class="attribute">-p</span>
密码为空，如果能登陆上，则安装成功。
</code></pre><h2 id="修改MySQL用户root的密码">修改MySQL用户root的密码</h2>
<pre><code>mysqladmin <span class="attribute">-u</span> root password <span class="string">'123456'</span>
注：也可运行安全设置脚本，修改MySQL用户root的密码，同时可禁止root远程连接，移除test数据库和匿名用户。
/usr/<span class="built_in">local</span>/mysql/bin/mysql_secure_installation
</code></pre><h2 id="可能会出现的错误">可能会出现的错误</h2>
<pre><code>问题：
Starting MySQL..The server quit <span class="keyword">without</span> updating PID <span class="type">file</span> ([FAILED]/mysql/Server03.mylinux.com.pid).
解决：
修改/etc/<span class="keyword">my</span>.cnf 中datadir,指向正确的mysql数据库文件目录

问题：
ERROR <span class="number">2002</span> (HY000): Can't connect <span class="keyword">to</span> <span class="keyword">local</span> MySQL server <span class="keyword">through</span> socket '/tmp/mysql.sock' (<span class="number">2</span>)
解决：
新建一个链接或在mysql中加入-S参数，直接指出mysql.sock位置。
ln -s /usr/<span class="keyword">local</span>/mysql/data/mysql.sock /tmp/mysql.sock
/usr/<span class="keyword">local</span>/mysql/bin/mysql -u root -S /usr/<span class="keyword">local</span>/mysql/data/mysql.sock
</code></pre>]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="Mysql" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Mysql/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CENTOS 6.5搭建本地YUM源]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_yum/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/09/Linux/linux_yum/</id>
    <published>2014-04-09T12:01:08.000Z</published>
    <updated>2014-04-13T12:04:03.000Z</updated>
    <content type="html"><![CDATA[<h1 id="CENTOS_6-5搭建本地YUM源">CENTOS 6.5搭建本地YUM源</h1>
<h2 id="方法一：用系统光盘镜像创建">方法一：用系统光盘镜像创建</h2>
<h3 id="创建yum源目录并挂载系统光盘镜像">创建yum源目录并挂载系统光盘镜像</h3>
<pre><code>mkdir /opt/<span class="built_in">local</span><span class="attribute">-mirror</span>
mount /dev/cdrom  /opt/<span class="built_in">local</span><span class="attribute">-mirror</span>
</code></pre><h3 id="新建local-mirror-repo文件">新建local-mirror.repo文件</h3>
<pre><code>cd /etc/yum.repos.d/
rm -f  *  (删掉原来的repo文件，如果以后还想用，也可以重命名或者移到其他目录，我这里删掉)
vi /etc/yum.repos.d/local-mirror.repo
[local-mirror]
name=This <span class="keyword">is</span> a local repo
baseurl=file:<span class="comment"><span class="xmlDocTag">///</span>opt/local-mirror</span>
enabled=<span class="number">1</span>
gpgcheck=<span class="number">1</span>
gpgkey=file:<span class="comment"><span class="xmlDocTag">///</span>etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6</span>
</code></pre><h3 id="设置开机自动挂载">设置开机自动挂载</h3>
<p>vi  /etc/fstab   //在最后一行加上：</p>
<pre><code>/dev/cdrom    /opt/local-mirror     iso9660        <span class="default"><span class="keyword">default</span>      0    0</span>
</code></pre><h3 id="测试">测试</h3>
<pre><code>yum clean <span class="literal">all</span>
yum  makecache
yum <span class="attribute">-y</span> install ftp(或者安装其他软件包)
</code></pre><p>如果没有提示错误，就说明本地yum源搭建成功了！</p>
<h2 id="方法二：将所有rpm包复制到指定文件夹，通过createrepo命令创建">方法二：将所有rpm包复制到指定文件夹，通过createrepo命令创建</h2>
<h3 id="创建yum源目录并且挂载系统光盘镜像之后，将所有rpm包复制到yum源目录，如果镜像文件像CentOS6-5一样是分为2个iso文件的，分2次挂载并且复制即可">创建yum源目录并且挂载系统光盘镜像之后，将所有rpm包复制到yum源目录，如果镜像文件像CentOS6.5一样是分为2个iso文件的，分2次挂载并且复制即可</h3>
<pre><code>mkdir /opt/<span class="keyword">local</span>-mirror
mount /dev/cdrom  /mnt (临时供<span class="keyword">copy</span>的目录）
cp /mnt/Packages/*   /opt/<span class="keyword">local</span>-mirror
</code></pre><h3 id="通过rpm命令手动安装createrepo命令创建yum仓库">通过rpm命令手动安装createrepo命令创建yum仓库</h3>
<pre><code>rpm <span class="attribute">-ivh</span> createrepo     (若提示需要安装依赖包，根据提示安装即可）
createrepo <span class="attribute">-v</span>  /opt/<span class="built_in">local</span><span class="attribute">-mirror</span>  (创建索引信息）
</code></pre><h3 id="同方法一的第二步">同方法一的第二步</h3>
<h3 id="测试同方法一的第四步">测试同方法一的第四步</h3>
]]></content>
    
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/Linux/"/>
    
      <category term="yum" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/yum/"/>
    
      <category term="Linux" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[第一篇Blog都是扯淡]]></title>
    <link href="https://github.com/DaMinger/DaMinger.github.io.git/2014/04/07/%E6%9D%82%E8%B0%88/first_blog/"/>
    <id>https://github.com/DaMinger/DaMinger.github.io.git/2014/04/07/杂谈/first_blog/</id>
    <published>2014-04-07T04:18:40.000Z</published>
    <updated>2014-04-09T09:05:06.000Z</updated>
    <content type="html"><![CDATA[<p>所谓的第一篇文章都是扯淡，原先学习都是用OneNote来管理个人笔记，现在整个Blog分享点学习心得,共享知识，共同进步。</p>
]]></content>
    
    
      <category term="杂谈" scheme="https://github.com/DaMinger/DaMinger.github.io.git/tags/%E6%9D%82%E8%B0%88/"/>
    
      <category term="杂谈" scheme="https://github.com/DaMinger/DaMinger.github.io.git/categories/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
</feed>
